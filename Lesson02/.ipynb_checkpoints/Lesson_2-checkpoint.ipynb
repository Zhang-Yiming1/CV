{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://img.kaikeba.com/web/hcTech/img_logo.png\" alt=\"图片替换文本\" width=\"500\" height=\"150\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Foundation of Artificial Intelligence.Lecture 2\n",
    "## 目录\n",
    "\n",
    "- Linear Regression\n",
    "- Loss Function\n",
    "- Gradient Descent\n",
    "- Activation Function\n",
    "- Logistic Regression\n",
    "- Overfitting and Underfitting\n",
    "- Bias and Variance\n",
    "- Regularlization\n",
    "- Optimizer方法、Adam等优化方法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Fit A Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Linear_Regression.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array([1., 2. ,3. ,4. ,5.])\n",
    "y = np.array([1., 3. ,2. ,3. ,5.])\n",
    "plt.scatter(x,y) # 绘制 x 与 y\n",
    "plt.grid() # 显示网格"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题1：已知两个点（1.0，1.0）与（5.0，5.0），求解二元一次方程？    \n",
    "问题2：对于上面的五个点，可以用一条直线表示吗？    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img-blog.csdnimg.cn/20200410205259492.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"500\" height=\"500\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/2020041107263822.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 损失函数(Loss Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 平均绝对误差（Mean Absolute Error，MAE）\n",
    "\n",
    "若有$m$个数据，第$i$个数据下的真实值表示为$y^i$,拟合曲线在第个值下的值为 $ \\hat{ y^i}$, MAE表示为：\n",
    "\n",
    "$$ MAE = \\frac 1m\\sum_{i=1}^m | \\hat {y^i} - y^i | $$\n",
    "\n",
    "- 均方误差（Mean Squared Error，MSE）\n",
    "\n",
    "同样，考虑到拟合直线与真实值的面积误差，表达式如下：\n",
    "\n",
    "$$ MSE = \\frac {1}{2m}\\sum_{i=1}^m ( \\hat {y^i} - y^i )^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    :param y: the real fares\n",
    "    :param y_hat: the estimated fares\n",
    "    :return: how good is the estimated fares\n",
    "    \"\"\"\n",
    "\n",
    "    return np.mean(np.abs(y_hat - y))\n",
    "    # return np.mean(np.square(y_hat - y))\n",
    "    # return np.mean(np.sqrt(y_hat - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    return sum(abs(y_hat_i - y_i) for y_hat_i, y_i in zip(list(y_hat),list(y))) / len(list(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function (x, k, b):\n",
    "    \"\"\"y = kx + b\"\"\"\n",
    "    return x*k + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 方法1，遍历求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k:[-1,1] 步长：0.1   \n",
    "b:[-1,1] 步长：0.1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_1 = np.linspace(-1,1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "-0.9\n",
      "-0.8\n",
      "-0.7000000000000001\n",
      "-0.6000000000000001\n",
      "-0.5\n",
      "-0.4\n",
      "-0.30000000000000004\n",
      "-0.2\n",
      "-0.1\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.30000000000000004\n",
      "0.4\n",
      "0.5\n",
      "0.6000000000000001\n",
      "0.7000000000000001\n",
      "0.8\n",
      "0.9\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for k in range(-10, 11):\n",
    "    k *= 0.1\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 3. 2. 3. 5.]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(-100, 101):\n",
    "    k *= 0.01\n",
    "    for b in range(-100,101):\n",
    "        b *= 0.01\n",
    "        y_hat = [function(x_i,k,b) for x_i in list(x)]\n",
    "#         print(y_hat)\n",
    "        current_loss = loss(y,y_hat)\n",
    "#         print(current_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = float(\"inf\") # 正无穷或负无穷，使用float(\"inf\")或float(\"-inf\")来表示\n",
    "best_k,best_b = None,None  # 空值\n",
    "\n",
    "for k in range(-100, 101):\n",
    "    k *= 0.01\n",
    "    for b in range(-100,101):\n",
    "        b *= 0.01\n",
    "        y_hat = [function(x_i,k,b) for x_i in list(x)]\n",
    "        #print(y_hat)\n",
    "        current_loss = loss(y,y_hat)\n",
    "        #print(current_loss)\n",
    "        if current_loss < min_loss:\n",
    "            min_loss = current_loss\n",
    "            best_k,best_b = k,b\n",
    "#             print(\"best k is {}, best b is {}, and the loss is {}\".format(best_k, best_b, current_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD6CAYAAAC8sMwIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de5xVZdn/8c8loUyMiAKNNSAeKNLARxg8IKUz/VRIzfBQVkpqLxwP5aHH8Pg89Sv1J49gnHw8ACqkPo1aNBaPSimMomnJIUItFI3TNKFYkEOjCFy/P+4NDNMc1jBrz1pr5vt+vfaL2eves/aXm801a+699rXM3RERkezaK+kAIiLSNirkIiIZp0IuIpJxKuQiIhmnQi4iknEfae8n7N27tx988MFt3s/mzZvp3r172wPFLI25lCm6NOZSpujSmCuuTIsXL97g7n0aHXT3dr2VlJR4HBYsWBDLfuKWxlzKFF0acylTdGnMFVcmYJE3UVe1tCIiknEq5CIiGadCLiKScSrkIiIZp0IuIpJxKuQiIhmnQi4iknEq5CIieVC5tJoR4+ezvHoTI8bPp3Jpdd6eK1IhN7NVZrYyd1vYYGyQmS0zs9VmNs3M9MNBRDq1yqXV3DBnOdUb6wCo3ljHDXOW562YRy667j4gd/tcg6G7gOuBQ4EjgTNizCcikjkT5q2g7sNtu22r+3AbE+atyMvzmUe4QpCZrXL3gxvZ3gdY4u79cvfLgaHufmmDx5UD5QBFRUUlFRUVbQ5eW1tLYWFhm/cTtzTmUqbo0phLmaJLS67l1Zt2fl1UAOvrdo0NLt5vj/ZZVla22N2HNTYWtWlWnZm9CbwDfN/d5+W29wXW1HvcOuC0ht/s7tOB6QDDhg3z0tLSiE/btKqqKuLYT9zSmEuZoktjLmWKLi25bho/f+eyyjWDt3LH8lBqi3sWcMV5pbE/X6SlFXc/3N0PA8YBD5tZz9zQ3sD2eg/dDmxr+P0iIp3JuJEDKejaZbdtBV27MG7kwLw8X6va2Lr7QjNbBRwM/B6oAYrrPaQvsDaucCIiWTR6SCiLYU38PYp7FjBu5MCd2+PWYiE3s+5AD3evMbMhwMeBNwDcfY2ZbTazUmAhMAa4KS9JRUQyZPSQYkYPKaaqqiovyyn1RTki/yjwrJl1ATYB5wOnmNlh7j4RuACYDfQEZrn783lLKyIi/6LFQu7u7wCfamZ8CTA4zlAiIhKdPrwjIpJxKuQiIhmnQi4iknEq5CIiGadCLiKScSrkIiIZp0IuIpJxKuQiIhmnQi4iknEq5CIiGadCLiKScSrkIiIZp0IuIpJxKuQiIhmnQi4iknGRCrmZ7W1mr5nZzAbbZ5lZtZmtzN0Oyk9MERFpStRrdt4IrGpi7Dx3r4oljYiItJq5e/MPMDscmAg8BnzW3cfWG5tFuLxbVQv7KAfKAYqKikoqKiralhqora2lsLCwzfuJWxpzKVN0acylTNGlMVdcmcrKyha7+7BGB929yRtgwK+BAcCFwMwG4zOA1cCrwDXN7WvHraSkxOOwYMGCWPYTtzTmUqbo0phLmaJLY664MgGLvIm62tIa+aVAlbuvbOKHwMXu3h8YBVxsZie19qeMiIi0TUtr5GOAfc3sy8ABQHczW+HuE+o/yN3XmtlcYBDwdH6iiohIY5ot5O5+/I6vzexCwhr5hHrbBrj7SjPrRTgqvyRfQUVEpHFRz1rZyczOBA5z94nAVDM7AvgAmObuL8QdUEREmhe5kLv7LGBWg22nxpxHRERaSZ/sFBHJOBVyEZGMUyEXEck4FXIRkYxTIRcRyTgVchGRjFMhFxHJOBVyEZGMUyEXEck4FXIRkYxTIRcRyTgVchGRjFMhFxHJOBVyEZGMUyEXEcm4SIXczPY2s9fMbGaD7YPMbJmZrTazaWamHwwiIu0sauG9EVjVyPa7gOuBQ4EjgTPiiSUiIlG1WMjN7HDgaODRBtv7AIe4+5Puvg14mHDdThERaUfm7k0PmhnwK+Ay4LOEiy+PzY0NAe509xG5+6cCl7j7lxrZTzlQDlBUVFRSUVHR5uC1tbUUFha2eT9xS2MuZYoujbmUKbo05oorU1lZ2WJ3H9booLs3eSMU8JtyX18IzKw3diywsN79UcCc5vbn7pSUlHgcFixYEMt+4pbGXMoUXRpzKVN0acwVVyZgkTdRV1u6+PIYYF8z+zJwANDdzFa4+wSgBiiu99i+wNo9+lEjIiJ7rNlC7u7H7/jazC4kLK1MyI2tMbPNZlYKLCQU/ZvyF1VERBrT6tMFzexMM/tu7u4FwDTCGS3PufvzMWYTEZEIWlpa2cndZwGzGmxbAgyON5KIiLSGPsAjIpJxKuQiIhmnQi4iknEq5CIiGadCLiKScSrkIiIZp0IuIpIPdXUwfTpFTz2V96eKfB65iIhE8Je/wH//N9x7L7z7Ln2OP77l72kjFXIRkTgsWQKTJsEjj8DWrfClL8F3vsMr27ZRmuen1tKKiMie2rYNKivhxBOhpCR8fdll8MYb8POfwwkngFneY+iIXESktd57Dx54AKZOhTffhIMOgokTYexY2G+/do+jQi4iEtXq1TBtGsycCZs2wfDhcNttcOaZ8JHkyqkKuYhIS158Max/z5kT7n/5y3D11XDsscnmylEhFxFpzNat8LOfhQL+299Cz55wzTXw7W9Dv35Jp9uNCrmISH0bN8KMGWEJZe1aGDAA7rwTLrgAUnY90B1aLORmthcwD+gPOHClu8+rNz4LOBmoy236vLuviT+qiEgevfEGTJkCs2bB5s1QVhbOBz/tNNgr3Sf4RTkid+Ab7l5jZqOAWwmFvb7z3L0q7nAiInnlDlVVYflk7tzwhuXXvx7Wv486Kul0kbVYyHNXb67J3e0PLMtrIhGRfPvgA6iogMmT4fe/h9694T/+Ay6/HA48MOl0rWahTrfwILNrgeuAd4CR7r663tgM4BSgFrjf3e9o5PvLgXKAoqKikoqKijYHr62tpTCF61VpzKVM0aUxlzJF11Kurhs38olf/IJPPP44+/ztb2w++GDWnXMO6086ie377JNIpqjKysoWu/uwRgfdPfINOAv4E7kfAA3G+uXGTmpuHyUlJR6HBQsWxLKfuKUxlzJFl8ZcyhRdk7leecV97Fj3bt3cwX3UKPd589y3b08uUysBi7yJutqqFXx3nwMUAr0aGVsLzAUGtWafIiJ54Q5PPQUjR8KgQfDQQ/CNb8Brr8GTT8Ipp7TLx+fbQ4uF3MwONbMDc18PB9539w31xgfk/uwFjAJezlNWEZGW5drH8pnPwBe+AMuXwy23hFMJ770XDj886YSxi3LWSk/gKTPrArwNnGtmZwKHuftEYKqZHQF8AExz9xfyF1dEpAk1NRxy331wzjnw7rswdCg8+CB85Suw995Jp8urKGetLAE+1WDz4nrjp8YdSkQksqVLw+mDFRUcVK99LJ/7XIdZOmmJPtkpItmzbVs473vSJHj22fCJy8su47fHHMNx552XdLp2l+6PK4mI1Pfee6F17MCBMHo0/PnPoX3s2rUwZQrvFxcnnTAROiIXkfRLafvYtNAMiEh6NWwfe845Yf07Je1j00KFXETSpWH72P32g3//99A+9qCDkk6XSirkIpIOjbWPnTYNLrwwte1j00KFXESStXJlaB/7wAOhfWxpaej/ffrpqW8fmxYq5CLS/tzDaYOTJsEvfxnesPza18L6d4bax6aFCrmItJ8tW0L72EmTdm8fe9ll8PGPJ50us1TIRST/NmyAe+4JV9z561/hiCPCevh550FBQdLpMk+FXETy57XXwsUbHnwQ3n8fRo0Kyycnn9xpPj7fHlTIRSRe7jBvXlg++dWvoFu30D72qqvCkbjEToVcROJRVxeOvCdPhj/+Max533ILXHJJWAuXvFEhF5G2qakJa9/33BPaxw4ZAj/+MZx7bodvH5sWKuQismfqtY9l61Y444yw/n3CCVr/bmdRrhC0l5n92sxeN7MVZjaywfggM1tmZqvNbJqZ6Qz+lKhcWs2I8fNZXr2JEePnU7m0OulIknGVi9Zw3TduZsBl34ahQ9n605/BpZfC669DZSWceKKKeAKiHJE78A13rzGzUcCtwLx643cB1wO/AuYDZwCVcQeV1qlcWs0Nc5ZT9+E26AfVG+u4Yc5yAEYP6ZytPqUNamv5w82TGDLjbkb/vYZ/9OrDraXf5PFhX+DGrw9n9AC9ppLU4tFz7gLONbm7/YFlO8bMrA9wiLs/6e7bgIcJ1+2UhE2YtyIU8XrqPtzGhHkrEkokmbRmDYwbB337cuTt3+Pdgh5864zr+PGP7mHGsWfxdpcCvaZSwNy95QeZXQtcB7wDjHT31bntQ4A73X1E7v6pwCXu/qUG318OlAMUFRWVVFRUtDl4bW0thSlspJOWXMurN+38uqgA1tftGhtcvF8CiXaXlnlqKI25ksjU47XX6PvYY/R57jkA3jnxRKpOHMX6AQOBdL6moGP/+5WVlS1292GNjUUq5DsfbHYW8P+Aw93dzexYYKK7fy43Pgood/ezmtrHsGHDfNGiRa36CzSmqqqK0tLSNu8nbmnJNWL8fKo3hv9p1wzeyh3Lwypacc8CXrj+80lGA9IzTw2lMVe7Zdq6NfT9njQJXnoptI8tL9/ZPjbtryno2P9+ZtZkIW/VG5PuPgcoBHrlNtUA9RfH+gJr9ySkxGvcyIEUdO2y27aCrl0YN3JgQokktTZuDJdLO+ywcMrghg2hfey6dXD77Tt7gOs1lV4tvtlpZocC/3T3v5rZcOB9d98A4O5rzGyzmZUCC4ExwE35DCzR7HhDM6xfvkdxzwLGjRyoNzpll5Urw/Uv779/V/vYadPgtNOgS5d/ebheU+kV5ayVnsBTZtYFeBs418zOBA5z94nABcDs3ONmufvzeUsrrTJ6SDGjhxRTVVXFFeeVJh1H0qCp9rFXXx0+yNMCvabSqcVC7u5LgE812Ly4wfjgmHOJSJx2tI+dPDl8kKd3b7jpJrj8crWP7QD0yU6Rjqyx9rHTp8P556t9bAeiQi7SETVsHztyJMyaBaecok9edkAq5CIdhXtoGztpUmgj260bjBkT1r/VPrZDUyEXybq6OnjooXAE/tprcOCBcPPNoQeK2sd2CirkIllVUwN33RXWwDdsCBctnj07nAu+zz5Jp5N2pEIukjW//31YPvnJT9Q+VgAVcpFs2L4d5s4NBbyqCrp3D0snV14JAwYknU4SpkIukma1tRT//Odw8cXhk5gHHQQTJsDYsdCzZ9LpJCVUyEXSaM0auPNOmDGDT27cCMcdB7feCmedFT6NKVKPXhEiafLSS2H55Gc/C/fPPpslJ57I0MsvTzaXpJouyyaStK1b4dFHYfjwcJs3L7x5+dZb8Mgj/EPngEsLdEQukpSNG2HmzNBxcM2a0EZ26lS46CJI2cURJN1UyEXa25tvwpQp8MADUFsbLlg8dSqcfnqj7WNFWqJCLtIe3OG558L69y9+Ed6w/OpXwxJKhPaxIs1RIRfJpy1b4JFHQgFfuhR69YIbb4RvfUvtYyU2KuQi+bBhA9x7b2gfW1MDhx8e7o8Zo/axErsol3rrBkwFTgS6AZPdfVK98VnAycCOa2p/3t3XxB9VJAP++MfQvOrHP97VPvaBB9Q+VvIqyhF5d2AecAnhosuvmtlP3b3+RZbPc/eqPOQTST93+PWvw/LJU0+pfay0O3P31n2D2SLgIndfnrs/i3CtzqpmvqccKAcoKioqqaio2NO8O9XW1lKYwlO00phLmaJrTa69PviAoqefpu9Pf0r3Vav44IAD+Mvo0fzli1/kwxg/Pp/GuUpjJkhnrrgylZWVLXb3YY0OunvkGzAIeIXcD4DcthnAauBV4JqW9lFSUuJxWLBgQSz7iVsacylTdJFy1dS4/+d/uvfu7Q7uRx3lPnu2+/vvJ5epnaUxk3s6c8WVCVjkTdTVyG92mllv4EHC0fjOw3h3vzg33g/4tZktc/en9+QnjkiqNWwf+8UvhtMHTzxR69+SqEiF3Mz2B34J3OjuLzf2GHdfa2ZzCUftKuTSMTTWPvaSS0L72E9+Mul0IkC0s1Z6AL8AbnX3JxsZH+DuK82sFzCK8KaoSLbV1oaLFU+ZEtrH9usHt98e2smqfaykTJQj8iuBocBkM5uc23Y3YZ18IjDVzI4APgCmufsL+Ykqkn/7vP02XHstzJgReqEceyzccgucfbbax0pqtfjKdPdbgFuaGT811kQiSfjtb2HSJI577LGw3n322WH9+7jjkk4m0iIdYkjntXUr/PznYf37xRehRw/WnXMO/W6/Hfr3TzqdSGTqRy6dz6ZNcMcd4VqXX/kKvP126D64bh1vXnaZirhkjo7IpfN4881QsO+/f1f72ClT1D5WMk+FXDo2d1i4MCyfPP74rvaxV18NQ4cmnU4kFirk0jHtaB87eTIsWbKrfezll8MnPpF0OpFYqZBLx9JU+9jzz4ePfjTpdCJ5oUIuHUPD9rGnnBLWwk85BfbSe/rSsamQS3Y1bB+7zz672sd+5jNJpxNpNyrkkj11dfDww+EI/NVX4cAD4eabQw+UPn2STifS7lTIJTv++le46y64++6wFn7UUTB7Npx7bjgaF+mkVMgl/ZYt29U+9sMP1T5WpAEVckmn7dvhf/83FPAFC0L72PJytY8VaYQKuaRLbW1YLpkyBd54Y1f72LFjYf/9k04nkkoq5JIOa9fCnXfC9Om72sdWVMBZZ0HXrkmnE0k1nWAryfrd7+BrX4NDDoGJE+Hkk+E3v4GXXgpvYrZTEa9cWs2I8fNZXr2JEePnU7m0ul2eVyQOLRZyM+tmZtPNbIWZrTaz7zQYH2Rmy3Jj08xMPxykeVu30qeqCkaMCEfeTzwRzv1+6y149FEYPrxd41QureaGOcup3lgHQPXGOm6Ys1zFXDIjStHtDswDPg2UANfnLrS8w13A9cChwJHAGXGHlA6iXvvYz/zgB+F0wilTYN26cDSeUPvYCfNWUPfhtt221X24jQnzViSSR6S1zN1b9w1mi4CL3H25mfUBlrh7v9xYOTDU3S9t8D3lQDlAUVFRSUVFRZuD19bWUlhY2Ob9xC2NuZLO1K26mr5z5nDgk0/ykbo6Nh55JG+cfjqbP//5VLSPXV69aefXRQWwvm7X2ODi/RJItLuk//0ak8ZMkM5ccWUqKytb7O7DGh1098g3YBDwCrt+AAwBXqg3firweHP7KCkp8TgsWLAglv3ELY25Esm0fbv7s8+6jx7tbubetav7mDHuixcnl6kJx9/2jPe/bq73v26uT32ocufXx9/2TNLR3D1dc7VDGjO5pzNXXJmARd5EXY28nm1mvYEHCUfjOw7j9wa213vYdmBbw++VTmTLFnjoIRg2LHxgZ+HC0D521arQ0CqFPcDHjRxIQdfdfzMo6NqFcSMHJpRIpHUinX5oZvsDvwRudPeX6w3VAMX17vcF1sYXTzLj3Xd3tY/9y18y1T529JDwEg5r4u9R3LOAcSMH7twuknYtFnIz6wH8ArjV3Z+sP+bua8xss5mVAguBMcBN+QgqKfWnP+1qH1tXF9rG3ndf5trHjh5SzOghxVRVVXHFeaVJxxFplShH5FcCQ4HJZjY5t+1uwjr5ROACYDbQE5jl7s/nJamkhzs8/XT4+PyTT6p9rEjCWizk7n4LcEsz40uAwXGGkpR6//1d7WNfeQWKiuCHP4RLL1X7WJEE6SP60rL163e1j33nHfi3f4NZs8JFjNU+ViRxKuTStGXLwtH3//xPaB97+umhfWxpqdrHiqSICrnsbkf72MmTYf78cMbJxRfDVVepfaxISqmQS7B5c1gu2dE+tm9f+K//CkVc7WNFUk2FvLNr2D72mGPClXjOPlvtY0UyQoW8s/rd78Lpg489Fk4nPPvscPrg8OFa/xbJGBXyzmTrVqisDAX8N7+BHj1C8b7iisQ6D4pI26mQdwJdamvhRz+CqVNh9Wo49NCwFn7RRbDvvknHE5E2UiHvyN56C6ZOZfiMGfDPf8IJJ4SzUb74xVS0jxWReKiQdzTu8PzzYfmkshK6dGFDWRkH3nYblJQknU5E8iA7XY2keVu2hI/PH310OPJ+9lm44QZYvZo/3XijirhIB6Yj8qx7991w6uCdd4b2sZ/+NNxzT2hitaN97OuvJ5tRRPJKhTyrGraPPflkmDkTRo7MVPtYEWk7FfIsaax97Pnnh1MIBw1KOp2IJCRyITezAqCfu+v39PbWWPvYH/wgtI/92MeSTiciCWvxd3Az62FmlcB64NpGxmeZWbWZrczdDspH0E5p/Xr4/vfhoINg7NiwZPLAA+Fc8O99T0VcRIBoR+TbgWnAXOC4Jh5znrtXxRWq0/vDH8LyidrHikgEUa4QVAs8Y2YX5j9OJ7Z9OzzxRCjgah8rIq1g7h7tgaGQf9bdxzbYPgM4BagF7nf3Oxr53nKgHKCoqKikoqKijbGhtraWwsLCNu8nbq3NtVddHQfOm0ffOXP46Nq1vN+nD9VnnknN6aezNaaPz6dxrtKYCdKZS5miS2OuuDKVlZUtdvdhjQ66e6QbcCEws5nxfsCfgJOa209JSYnHYcGCBbHsJ26Rc61d637dde777+8O7scc4/6Tn7hv2ZJcpnaUxkzu6cylTNGlMVdcmYBF3kRdje30Q3dfa2ZzgUHA03Htt8Np2D72rLPC+rfax4rIHmpzITezAe6+0sx6AaOAS9oeq4NprH3sVVeF9rEHH5x0OhHJuBYLuZntCywF9gW6mVkpMA44zN0nAlPN7AjgA2Cau7+Qx7zZsmkT3HcfTJsGq1bBIYeEc8G/+U21jxWR2EQ5a+U9YEAz46fGmqgjyLWP5f774b334HOfC/3AzzhD7WNFJHb6iH5ccu1jP/O978ELL4QP75x7blj/VudBEckjFfK22rIlvHE5aRIsXkzPHj3g+uvh8suhuDjpdCLSCaiQ76m//Q3uvfdf2se+2L8/J4walXQ6EelEVMhba8WK8Ibl7NmNto/dXlWVdEIR6WRUyKNwh2eeCcsnTzyh9rEikioq5M1p2D72Yx9T+1gRSR0V8sasXw933QV33w3vvANHHhnax37ta+FoXEQkRVTI66vfPnbLll3tY8vK9PF5EUktFfLG2seOHRs+Qv+pTyWdTkSkRZ23kG/eHM48mTIlXGW+uBjGjw89wA84IOl0IiKRdb5Cvm5dOPd7+nT4+9/h6KPDUso550DXrkmnExFptc5TyF9+eVf72O3bQ/vYq6+G44/X+reIZFrHLuTbtu1qH/vCC6F97JVXqn2siHQoHbOQ/+MfoX3s1Km7t4+96KJQzEVEOpDIhdzMCoB+7v56HvO0zZ//HIr3ffepfayIdBp7tfQAM+thZpXAeuDaRsYHmdkyM1ttZtPMrMV9tkXl0mpGjJ/P8upNjBg/n8ol62DhwrDmPWBAeCPzjDNg0SJ47jk480wVcRHp0KIckW8HpgFzgeMaGb8LuB74FTAfOAOojCtgfZVLq7lhznLqPtzGXh/fyrAXnuCwqZdCzRvhlMHrroNvfUvtY0WkU4lyhaBa4Bkzu7DhmJn1AQ5x9ydz9x8mXLczL4V8wrwV1H24jWPXLOeC6RMo/PvfePOAvkwYfTXjHr41fJhHRKSTMXeP9sBQyD/r7mPrbRsC3OnuI3L3TwUucfcvNfjecqAcoKioqKSiomKPwi6v3gRA97+9yxfuv5OXTzqN1UcOhb32YnDxfnu0z7jV1tZSWFiYdIzdKFN0acylTNGlMVdcmcrKyha7+7DGxtp61srehKWXHbYD2xo+yN2nA9MBhg0b5qWlpXv0ZDeNn0/1xjqgiM3f/T53LP8IvArFPQu44rw922fcqqqq2NO/X74oU3RpzKVM0aUxV3tkausbkzVA/QXpvsDaNu6zSeNGDqSg6+5vXBZ07cK4kQPz9ZQiIqnXpkLu7muAzWZWamZdgDHAY7Eka8ToIcXcdtZginsWAOFI/LazBjN6iN7cFJHOq8WlFTPbF1gK7At0M7NSYBxwmLtPBC4AZgM9gVnu/nz+4oZiPnpIMVVVValZThERSVKUs1beAwY0M74EGBxnKBERiS6vH94REZH8UyEXEck4FXIRkYxTIRcRyTgVchGRjFMhFxHJuMi9VmJ7QrN3gNUx7Ko3sCGG/cQtjbmUKbo05lKm6NKYK65M/d29T2MD7V7I42Jmi5pqIJOkNOZSpujSmEuZoktjrvbIpKUVEZGMUyEXEcm4LBfy6UkHaEIacylTdGnMpUzRpTFX3jNldo1cRESCLB+Ri4gIKuQiIpmXmUJuZgVm9qmkczSU1lwi0nmkvpCbWQ8zqwTWA9c2Mj7IzJaZ2Wozm2Zm7fJ3ipBrlplVm9nK3O2gdsjUzcymm9mK3Hx8p8F4u89VhExJzNNeZvZrM3s9l2tkg/GkXlMt5Wr3uco9795m9pqZzWywPZF5ipArkXnKPfeqes+7sMFY/ubL3VN9AwqB/wOMBWY2Mv4c8AWgC/AsMDoluWYBpe08V72AswEjfJpsPdAvybmKkCmJeTLg47mvRwGLUvKaailXu89V7nn/L/BEw9d5UvMUIVci85R77lXNjOVtvlJ/RO7ute7+DLC14ZiZ9QEOcfcn3X0b8DDhP0CiuZLi7u+6+8882EC4EHZPSG6umsuUlFyWmtzd/sCyHWMJv6aazJUUMzscOBp4tMH2xOapuVxple/5Sn0hb0FfYE29++uAjyeUpaEPgdlm9qqZXdPeT25mg4BuwCu5TYnPVSOZIKF5MrNrzexd4DvAD+sNJTpPzeSCdp4rMzNgKnBVI8OJzVMLuSDZ/3t1Zvammb3UYGksr/OV9UK+N7C93v3twLaEsuzG3S929/6En7oXm9lJ7fXcZtYbeBC4yHO/05HwXDWRKbF5cvfb3b0XcCMwL1ccIOF5aiZXEnN1KVDl7isbGUtynprLlej/PXc/3N0PI1yg/mEz2/HbZ17nK+uFvAYorne/L+FX99Rw97XAXGBQezyfme0P/BK40d1frjeU2Fw1k2mn9p6nes87h/B+R6/cplS8phrJVX+sveZqDPBVM/s94beDM9WNKi8AAAESSURBVM1sXG4syXlqLtdOSb2mcs+9EFgFHJzblN/5SuINgT25ARfS+JuKy4FSdr2B8NmU5BqQ+7MXYSlhRDtk6QEsBE5vYrzd5ypCpiTm6VDgwNzXw4GVSc9TxFztPlf1nvtfXudJ/99rJlci8wR0Z9eb1UOAaqB7e8xXu076Hk7OvsBKwtkOm3Jfnwl8Nzc+NDdBa4GbU5TrCcJP5BXAt9sp038Am3NZdtyuSXKuImRKYp6GAq8DbwIvAiUpeU21lKvd56petguBmWmYpwi5EpknoE+9f78lQFl7zZd6rYiIZFzW18hFRDo9FXIRkYxTIRcRyTgVchGRjFMhFxHJOBVyEZGMUyEXEck4FXIRkYz7/0UqiSwuGU+mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# best k is 0.68, best b is 0.32, and the loss is 0.5999999999999999\n",
    "k,b = 0.68,0.32\n",
    "y_hat = k*x + b\n",
    "plt.plot(x,y_hat,color='red') # 绘制 x 与 y，color='red'设置颜色\n",
    "plt.scatter(x,y)\n",
    "plt.grid() # 显示网格"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 方法2，如何找到改变的方向呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "direction  = [\n",
    "    (+1, -1), # 第一个是k的方向，第二个b的方向，+1表示增大，-1表示减小\n",
    "    (+1, +1),\n",
    "    (-1, -1),\n",
    "    (-1, +1),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200412140525549.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"800\" height=\"800\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 梯度下降(Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis (拟合的直线) ： $ \\hat y = k\\cdot x + b $    \n",
    "\n",
    "Parameters (参数) ： $k,b$   \n",
    "\n",
    "Cost Func (损失函数，loss选择MSE) ：  $ Cost(k,b) =  \\frac {1}{2m}\\sum_{i=1}^m ( \\hat {y^i} - y^i )^2 $   \n",
    "\n",
    "Goal ：$\\min_{k,b}Cost(k,b)$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 首先对 $k$、$b$ 赋值，这个值可以是随机的，也可以是一个零向量；  \n",
    "- 改变 $k$、$b$ 的值，使得 $ Cost(k,b) $ 按梯度下降的方向进行减少；  \n",
    "- 当 $ Cost(k,b) $  下降到无法下降时为止.\n",
    "\n",
    "$$ temp0 = k - \\lambda \\frac{\\partial }{\\partial k} Cost(k,b) $$\n",
    "\n",
    "$$ temp1 = b - \\lambda \\frac{\\partial }{\\partial b} Cost(k,b) $$\n",
    "\n",
    "$$ k := temp0 $$\n",
    "\n",
    "$$ b := temp1 $$\n",
    "\n",
    "$ \\lambda $ 是步长，也被成为学习率，凭经验值。  \n",
    "\n",
    "$ \\frac{\\partial }{\\partial k} Cost(k,b) ,\\frac{\\partial }{\\partial b} Cost(k,b)$ 是多少呢？\n",
    "\n",
    "$$ \\frac{\\partial }{\\partial k} Cost(k,b) = \\frac {1}{m}\\sum_{i=1}^m (\\hat {y^i} - y^i ) x $$ \n",
    "\n",
    "$$ \\frac{\\partial }{\\partial b} Cost(k,b) = \\frac {1}{m}\\sum_{i=1}^m (\\hat {y^i} - y^i) $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    return sum((y_hat_i - y_i)**2 for y_hat_i, y_i in zip(list(y_hat),list(y))) / len(list(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_k(x,y,y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for x_i,y_i,y_hat_i in zip(list(x),list(y),list(y_hat)):\n",
    "        gradient += (y_hat_i - y_i) * x_i\n",
    "    return gradient / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_b(y,y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for y_i, y_hat_i in zip(list(y),list(y_hat)):\n",
    "        gradient += (y_hat_i - y_i) \n",
    "    return gradient / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 演示\n",
    "```python\n",
    "current_k = current_k - learn_rate * k_gradient\n",
    "current_b = current_b - learn_rate * b_gradient\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best k is 0.7996398909330126, best b is 0.40130010799381305, and the loss is 0.4800003183324043\n"
     ]
    }
   ],
   "source": [
    "min_loss = float(\"inf\") # 正无穷或负无穷，使用float(\"inf\")或float(\"-inf\")来表示\n",
    "best_k,best_b = None,None  # 空值\n",
    "current_k, current_b = 10, 10\n",
    "try_times = 1000\n",
    "learn_rate = 0.1\n",
    "for i in range(try_times):\n",
    "    \n",
    "    y_hat = [function(x_i,current_k,current_b) for x_i in list(x)]\n",
    "    current_loss = loss(y,y_hat)\n",
    "    if current_loss < min_loss:\n",
    "        min_loss = current_loss\n",
    "        best_k,best_b = current_k, current_b\n",
    "    else:\n",
    "        k_gradient = gradient_k(x,y,y_hat)\n",
    "        b_gradient = gradient_b(y,y_hat)\n",
    "        current_k = current_k - learn_rate * k_gradient\n",
    "        current_b = current_b - learn_rate * b_gradient\n",
    "        best_k,best_b = current_k, current_b\n",
    "print(\"best k is {}, best b is {}, and the loss is {}\".format(best_k, best_b, current_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# best k is 0.80, best b is 0.40, and the loss is 0.48\n",
    "k,b = 0.80,0.40\n",
    "y_hat = k*x + b\n",
    "plt.plot(x,y_hat,color='red') # 绘制 x 与 y，color='red'设置颜色\n",
    "plt.scatter(x,y)\n",
    "plt.grid() # 显示网格"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 3 线性回归(Linear Regression) \n",
    " ## 3.1 Extension-Multiple Variables\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200413143328240.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_left\" alt=\"图片替换文本\" width=\"500\" height=\"500\" align=\"bottom\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200413143817359.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_left\" alt=\"图片替换文本\" width=\"700\" height=\"600\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 过拟合(Overfitting) 和 欠拟合(Underfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/20200412172241826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "## 4.1 why？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Bias and Variance\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200413123053887.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"500\" height=\"500\" align=\"bottom\" />\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200413131243813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"500\" height=\"500\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 How to detect Overfitting?\n",
    "> ### 4.3.1 数据集划分\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200412182252554.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"600\" height=\"400\" align=\"bottom\" />\n",
    "\n",
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/20200413132215734.jpeg#pic_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> ### 4.3.2 正则化(Regularlization)\n",
    "\n",
    " >> $ R(k) $ 刻画模型的复杂程度\n",
    "  $$ Cost(k,b) +  \\lambda R(k) $$\n",
    "\n",
    "\n",
    " >> L1 正则化： \n",
    "  $$ R(k) = \\left \\|  k \\right \\|_1 = \\sum_i |k_i|$$\n",
    "\n",
    " >> L2 正则化： \n",
    "$$ R(k) = \\left \\|  k \\right \\|_2^2 = \\sum_i |k_i^2|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 如何评价？\n",
    "\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200413103828575.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"700\" height=\"600\" align=\"bottom\" />\n",
    "\n",
    "$$ Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}  $$   \n",
    "\n",
    "$$ Precision = \\frac{TP}{(TP+FP)}  $$\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    " \n",
    "$$ F1_{score} = \\frac{2\\times Recall\\times Precision}{Recall + Precision} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Classes\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200413152008202.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"600\" height=\"600\" align=\"bottom\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5 Logistic Function and Sigmoid Function\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200413152402793.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"600\" height=\"600\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(X, theta):\n",
    "    z = X.dot(theta)\n",
    "    g = 1/(1+np.e**-z)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 逻辑回归(Logistic Regression)\n",
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/2020041315514144.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代价函数和梯度\n",
    "def costFunction(theta, X, y):\n",
    "    h = hypothesis(X,theta)\n",
    "    J = 1.0/m*(-y.T.dot(log(h)) - (1-y).T.dot(log(1-h)))\n",
    "    grad = 1.0/m*X.T.dot(h-y)\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 信用卡反欺诈检测实战----判断客户交易记录是否正常    \n",
    "\n",
    "> 银行提供了一份个人交易记录，考虑到数据比较私密，不清楚大部分数据是什么，仅知道Amount列表示贷款的金额，Class列表示分类结果，若Class为0代表该条交易记录正常，若Class为1代表交易异常。    \n",
    "  数据已经做了处理，分出了训练集和测试集，且平衡了两个类别下的数据量。   \n",
    "  该实战的目的是希望学生掌握读取数据、逻辑回归建模、模型参数调优、模型评价。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6870</th>\n",
       "      <td>-1.863756</td>\n",
       "      <td>3.442644</td>\n",
       "      <td>-4.468260</td>\n",
       "      <td>2.805336</td>\n",
       "      <td>-2.118412</td>\n",
       "      <td>-2.332285</td>\n",
       "      <td>-4.261237</td>\n",
       "      <td>1.701682</td>\n",
       "      <td>-1.439396</td>\n",
       "      <td>-6.999907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360924</td>\n",
       "      <td>0.667927</td>\n",
       "      <td>-0.516242</td>\n",
       "      <td>-0.012218</td>\n",
       "      <td>0.070614</td>\n",
       "      <td>0.058504</td>\n",
       "      <td>0.304883</td>\n",
       "      <td>0.418012</td>\n",
       "      <td>0.208858</td>\n",
       "      <td>-0.349231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152015</th>\n",
       "      <td>-0.425454</td>\n",
       "      <td>1.226245</td>\n",
       "      <td>-1.314904</td>\n",
       "      <td>0.775567</td>\n",
       "      <td>2.264370</td>\n",
       "      <td>-0.768401</td>\n",
       "      <td>1.371214</td>\n",
       "      <td>-0.326371</td>\n",
       "      <td>0.816325</td>\n",
       "      <td>-1.715394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.229286</td>\n",
       "      <td>-0.210073</td>\n",
       "      <td>-0.280529</td>\n",
       "      <td>-0.351881</td>\n",
       "      <td>-0.098883</td>\n",
       "      <td>0.411762</td>\n",
       "      <td>-0.588556</td>\n",
       "      <td>-0.046577</td>\n",
       "      <td>0.089299</td>\n",
       "      <td>-0.133335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261556</th>\n",
       "      <td>2.030930</td>\n",
       "      <td>-0.866155</td>\n",
       "      <td>-0.228221</td>\n",
       "      <td>-0.428373</td>\n",
       "      <td>-1.155582</td>\n",
       "      <td>-0.474754</td>\n",
       "      <td>-0.873382</td>\n",
       "      <td>-0.039503</td>\n",
       "      <td>-0.415414</td>\n",
       "      <td>0.946528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.500968</td>\n",
       "      <td>-0.546578</td>\n",
       "      <td>-1.040354</td>\n",
       "      <td>0.497881</td>\n",
       "      <td>0.044952</td>\n",
       "      <td>-0.780473</td>\n",
       "      <td>0.204253</td>\n",
       "      <td>-0.015302</td>\n",
       "      <td>-0.043013</td>\n",
       "      <td>-0.245321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214775</th>\n",
       "      <td>-0.395582</td>\n",
       "      <td>-0.751792</td>\n",
       "      <td>-1.984666</td>\n",
       "      <td>-0.203459</td>\n",
       "      <td>1.903967</td>\n",
       "      <td>-1.430289</td>\n",
       "      <td>-0.076548</td>\n",
       "      <td>-0.992260</td>\n",
       "      <td>0.756307</td>\n",
       "      <td>0.217630</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.027716</td>\n",
       "      <td>1.377515</td>\n",
       "      <td>2.151787</td>\n",
       "      <td>0.189225</td>\n",
       "      <td>0.772943</td>\n",
       "      <td>-0.872443</td>\n",
       "      <td>-0.200612</td>\n",
       "      <td>0.356856</td>\n",
       "      <td>0.032113</td>\n",
       "      <td>-0.350471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149145</th>\n",
       "      <td>-2.405580</td>\n",
       "      <td>3.738235</td>\n",
       "      <td>-2.317843</td>\n",
       "      <td>1.367442</td>\n",
       "      <td>0.394001</td>\n",
       "      <td>1.919938</td>\n",
       "      <td>-3.106942</td>\n",
       "      <td>-10.764403</td>\n",
       "      <td>3.353525</td>\n",
       "      <td>0.369936</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.140874</td>\n",
       "      <td>10.005998</td>\n",
       "      <td>-2.454964</td>\n",
       "      <td>1.684957</td>\n",
       "      <td>0.118263</td>\n",
       "      <td>-1.531380</td>\n",
       "      <td>-0.695308</td>\n",
       "      <td>-0.152502</td>\n",
       "      <td>-0.138866</td>\n",
       "      <td>-0.325283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "6870   -1.863756  3.442644 -4.468260  2.805336 -2.118412 -2.332285 -4.261237   \n",
       "152015 -0.425454  1.226245 -1.314904  0.775567  2.264370 -0.768401  1.371214   \n",
       "261556  2.030930 -0.866155 -0.228221 -0.428373 -1.155582 -0.474754 -0.873382   \n",
       "214775 -0.395582 -0.751792 -1.984666 -0.203459  1.903967 -1.430289 -0.076548   \n",
       "149145 -2.405580  3.738235 -2.317843  1.367442  0.394001  1.919938 -3.106942   \n",
       "\n",
       "               V8        V9       V10  ...       V20        V21       V22  \\\n",
       "6870     1.701682 -1.439396 -6.999907  ...  0.360924   0.667927 -0.516242   \n",
       "152015  -0.326371  0.816325 -1.715394  ... -0.229286  -0.210073 -0.280529   \n",
       "261556  -0.039503 -0.415414  0.946528  ... -0.500968  -0.546578 -1.040354   \n",
       "214775  -0.992260  0.756307  0.217630  ... -1.027716   1.377515  2.151787   \n",
       "149145 -10.764403  3.353525  0.369936  ... -2.140874  10.005998 -2.454964   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  normAmount  \n",
       "6870   -0.012218  0.070614  0.058504  0.304883  0.418012  0.208858   -0.349231  \n",
       "152015 -0.351881 -0.098883  0.411762 -0.588556 -0.046577  0.089299   -0.133335  \n",
       "261556  0.497881  0.044952 -0.780473  0.204253 -0.015302 -0.043013   -0.245321  \n",
       "214775  0.189225  0.772943 -0.872443 -0.200612  0.356856  0.032113   -0.350471  \n",
       "149145  1.684957  0.118263 -1.531380 -0.695308 -0.152502 -0.138866   -0.325283  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train = pd.read_csv('X_train.csv', index_col=0)\n",
    "y_train = pd.read_csv('y_train.csv', index_col=0)\n",
    "\n",
    "X_test = pd.read_csv('X_test.csv', index_col=0)\n",
    "y_test = pd.read_csv('y_test.csv', index_col=0)\n",
    "\n",
    "X_train.head() # 前五行数据展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > 原始数据为个人交易记录，该数据集总共有29列，其中数据特征有30列，Time列暂时不考虑，Amount列表示贷款的金额."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6870</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152015</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261556</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214775</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149145</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Class\n",
       "6870        1\n",
       "152015      0\n",
       "261556      0\n",
       "214775      1\n",
       "149145      1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Class为0代表该条交易记录正常，Class为1代表交易异常。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score # Recall = TP/(TP+FN)\n",
    "from sklearn.model_selection import KFold, cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KFold in module sklearn.model_selection._split:\n",
      "\n",
      "class KFold(_BaseKFold)\n",
      " |  KFold(n_splits=5, shuffle=False, random_state=None)\n",
      " |  \n",
      " |  K-Folds cross-validator\n",
      " |  \n",
      " |  Provides train/test indices to split data in train/test sets. Split\n",
      " |  dataset into k consecutive folds (without shuffling by default).\n",
      " |  \n",
      " |  Each fold is then used once as a validation while the k - 1 remaining\n",
      " |  folds form the training set.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=5\n",
      " |      Number of folds. Must be at least 2.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |          ``n_splits`` default value changed from 3 to 5.\n",
      " |  \n",
      " |  shuffle : boolean, optional\n",
      " |      Whether to shuffle the data before splitting into batches.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default=None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`. Only used when ``shuffle`` is True. This should be left\n",
      " |      to None if ``shuffle`` is False.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.model_selection import KFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([1, 2, 3, 4])\n",
      " |  >>> kf = KFold(n_splits=2)\n",
      " |  >>> kf.get_n_splits(X)\n",
      " |  2\n",
      " |  >>> print(kf)\n",
      " |  KFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for train_index, test_index in kf.split(X):\n",
      " |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      " |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      " |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      " |  TRAIN: [2 3] TEST: [0 1]\n",
      " |  TRAIN: [0 1] TEST: [2 3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The first ``n_samples % n_splits`` folds have size\n",
      " |  ``n_samples // n_splits + 1``, other folds have size\n",
      " |  ``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n",
      " |  \n",
      " |  Randomized CV splitters may return different results for each call of\n",
      " |  split. You can make the results identical by setting ``random_state``\n",
      " |  to an integer.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  StratifiedKFold\n",
      " |      Takes group information into account to avoid building folds with\n",
      " |      imbalanced class distributions (for binary or multiclass\n",
      " |      classification tasks).\n",
      " |  \n",
      " |  GroupKFold: K-fold iterator variant with non-overlapping groups.\n",
      " |  \n",
      " |  RepeatedKFold: Repeats K-Fold n times.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KFold\n",
      " |      _BaseKFold\n",
      " |      BaseCrossValidator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_splits=5, shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |  \n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |  \n",
      " |  split(self, X, y=None, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target variable for supervised learning problems.\n",
      " |      \n",
      " |      groups : array-like, with shape (n_samples,), optional\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |      \n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(KFold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_get_best_c(x_train_data,y_train_data):\n",
    "\n",
    "    fold = KFold(5,shuffle=False)  # k-fold 表示K折的交叉验证\n",
    "    # 这里会得到两个索引集合: 训练集 = indices[0], 验证集 = indices[1]\n",
    "    \n",
    "    c_param_range = [0.01,0.1,1,10,100]  # 定义不同的正则化惩罚力度\n",
    "    \n",
    "    results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\n",
    "    results_table['C_parameter'] = c_param_range\n",
    "\n",
    "    j = 0\n",
    "    # 循环遍历不同的参数\n",
    "    for c_param in c_param_range:\n",
    "        print('-------------------------------------------')\n",
    "        print('正则化惩罚力度: ', c_param)\n",
    "        print('-------------------------------------------')\n",
    "        print('')\n",
    "\n",
    "        recall_accs = []\n",
    "        \n",
    "        \n",
    "        for iteration, indices in enumerate(fold.split(x_train_data)):\n",
    "\n",
    "            # 逻辑回归，并且给定参数,L1正则\n",
    "            lr = LogisticRegression(C = c_param, penalty = 'l1',solver='liblinear')\n",
    "\n",
    "            # 训练模型，注意索引不要给错了，训练的时候一定传入的是训练集，所以X和Y的索引都是0\n",
    "            lr.fit(x_train_data.iloc[indices[0],:],y_train_data.iloc[indices[0],:].values.ravel())\n",
    "   \n",
    "            # 建立好模型后，预测模型结果，这里用的就是验证集，索引为1\n",
    "            y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values)\n",
    "\n",
    "            # 有了预测结果之后就可以来进行评估了，这里recall_score需要传入预测值和真实值。\n",
    "            recall_acc = recall_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample)\n",
    "            # 一会还要算平均，先把每一步的结果都先保存起来。\n",
    "            recall_accs.append(recall_acc)\n",
    "            print('Iteration ', iteration,': 召回率 = ', recall_acc)\n",
    "\n",
    "        \n",
    "        # 当执行完所有的交叉验证后，计算平均结果\n",
    "        results_table.loc[j,'Mean recall score'] = np.mean(recall_accs)\n",
    "        j += 1\n",
    "        print('')\n",
    "        print('平均召回率 ', np.mean(recall_accs))\n",
    "        print('')\n",
    "        \n",
    "    #找到最好的参数，哪一个Recall高，自然就是最好的了。\n",
    "    best_c = results_table.loc[results_table['Mean recall score'].astype('float32').idxmax()]['C_parameter']\n",
    "    \n",
    "    # 打印最好的结果\n",
    "    print('*********************************************************************************')\n",
    "    print('效果最好的模型所选参数 = ', best_c)\n",
    "    print('*********************************************************************************')\n",
    "    \n",
    "    return best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "正则化惩罚力度:  0.01\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  0 : 召回率 =  0.9315068493150684\n",
      "Iteration  1 : 召回率 =  0.9178082191780822\n",
      "Iteration  2 : 召回率 =  1.0\n",
      "Iteration  3 : 召回率 =  0.9594594594594594\n",
      "Iteration  4 : 召回率 =  0.9696969696969697\n",
      "\n",
      "平均召回率  0.955694299529916\n",
      "\n",
      "-------------------------------------------\n",
      "正则化惩罚力度:  0.1\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  0 : 召回率 =  0.8356164383561644\n",
      "Iteration  1 : 召回率 =  0.863013698630137\n",
      "Iteration  2 : 召回率 =  0.9152542372881356\n",
      "Iteration  3 : 召回率 =  0.9459459459459459\n",
      "Iteration  4 : 召回率 =  0.8787878787878788\n",
      "\n",
      "平均召回率  0.8877236398016523\n",
      "\n",
      "-------------------------------------------\n",
      "正则化惩罚力度:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  0 : 召回率 =  0.863013698630137\n",
      "Iteration  1 : 召回率 =  0.863013698630137\n",
      "Iteration  2 : 召回率 =  0.9491525423728814\n",
      "Iteration  3 : 召回率 =  0.9459459459459459\n",
      "Iteration  4 : 召回率 =  0.8939393939393939\n",
      "\n",
      "平均召回率  0.9030130559036991\n",
      "\n",
      "-------------------------------------------\n",
      "正则化惩罚力度:  10\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  0 : 召回率 =  0.863013698630137\n",
      "Iteration  1 : 召回率 =  0.863013698630137\n",
      "Iteration  2 : 召回率 =  0.9491525423728814\n",
      "Iteration  3 : 召回率 =  0.9459459459459459\n",
      "Iteration  4 : 召回率 =  0.9090909090909091\n",
      "\n",
      "平均召回率  0.906043358934002\n",
      "\n",
      "-------------------------------------------\n",
      "正则化惩罚力度:  100\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  0 : 召回率 =  0.863013698630137\n",
      "Iteration  1 : 召回率 =  0.863013698630137\n",
      "Iteration  2 : 召回率 =  0.9491525423728814\n",
      "Iteration  3 : 召回率 =  0.9459459459459459\n",
      "Iteration  4 : 召回率 =  0.8939393939393939\n",
      "\n",
      "平均召回率  0.9030130559036991\n",
      "\n",
      "*********************************************************************************\n",
      "效果最好的模型所选参数 =  0.01\n",
      "*********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# 交叉验证得到不同参数结果\n",
    "best_c = Kfold_get_best_c(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "召回率： 0.9319727891156463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edz/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(C = 0.01, penalty = 'l1', solver='liblinear')\n",
    "LR.fit(X_train, y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "LR_recall = recall_score(y_test,y_pred)\n",
    "print('召回率：',LR_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "召回率： 0.9115646258503401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edz/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# 没有加L1正则化\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "recall = recall_score(y_test,y_pred)\n",
    "print('召回率：',recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 神经网络(Neural Network)\n",
    "## 7.1  Why Need NN?\n",
    ">  Non Linear\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200414104406949.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"600\" height=\"600\" align=\"bottom\" />\n",
    "\n",
    ">  Mimic Brain\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200414105454130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"600\" height=\"600\" align=\"bottom\" />\n",
    "\n",
    "## 7.2 Logistic Unit And Neural Network\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200414112153267.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"600\" height=\"600\" align=\"bottom\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.3 Activation Function\n",
    "![](./激活函数.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Back Propagation \n",
    "<img src=\"https://img-blog.csdnimg.cn/20200415162406262.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"600\" height=\"600\" align=\"bottom\" />\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200415162638161.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"600\" height=\"500\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, inputs=[]):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "\n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)\n",
    "            # set 'self' node as inbound_nodes's outbound_nodes\n",
    "\n",
    "        self.value = None\n",
    "\n",
    "        self.gradients = {}\n",
    "        # keys are the inputs to this node, and their\n",
    "        # values are the partials of this node with \n",
    "        # respect to that input.\n",
    "        # \\partial{node}{input_i}\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        '''\n",
    "        Forward propagation. \n",
    "        Compute the output value vased on 'inbound_nodes' and store the \n",
    "        result in self.value\n",
    "        '''\n",
    "\n",
    "        raise NotImplemented\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        raise NotImplemented\n",
    "        \n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        An Input node has no inbound nodes.\n",
    "        So no need to pass anything to the Node instantiator.\n",
    "        '''\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self, value=None):\n",
    "        '''\n",
    "        Only input node is the node where the value may be passed\n",
    "        as an argument to forward().\n",
    "        All other node implementations should get the value of the \n",
    "        previous node from self.inbound_nodes\n",
    "        \n",
    "        Example: \n",
    "        val0: self.inbound_nodes[0].value\n",
    "        '''\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "            ## It's is input node, when need to forward, this node initiate self's value.\n",
    "\n",
    "        # Input subclass just holds a value, such as a data feature or a model parameter(weight/bias)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.gradients = {self:0}\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost * 1\n",
    "            \n",
    "        \n",
    "        # input N --> N1, N2\n",
    "        # \\partial L / \\partial N \n",
    "        # ==> \\partial L / \\partial N1 * \\ partial N1 / \\partial N\n",
    "\n",
    "\n",
    "class Add(Node):\n",
    "    def __init__(self, *nodes):\n",
    "        Node.__init__(self, nodes)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = sum(map(lambda n: n.value, self.inputs))\n",
    "        ## when execute forward, this node caculate value as defined.\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(self, [nodes, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        inputs = self.inputs[0].value\n",
    "        weights = self.inputs[1].value\n",
    "        bias = self.inputs[2].value\n",
    "\n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "        \n",
    "    def backward(self):\n",
    "\n",
    "        # initial a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            # Get the partial of the cost w.r.t this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "\n",
    "            self.gradients[self.inputs[0]] = np.dot(grad_cost, self.inputs[1].value.T)\n",
    "            self.gradients[self.inputs[1]] = np.dot(self.inputs[0].value.T, grad_cost)\n",
    "            self.gradients[self.inputs[2]] = np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "        # WX + B / W ==> X\n",
    "        # WX + B / X ==> W\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1 + np.exp(-1 * x))\n",
    "\n",
    "    def forward(self):\n",
    "        self.x = self.inputs[0].value\n",
    "        self.value = self._sigmoid(self.x)\n",
    "\n",
    "    def backward(self):\n",
    "        self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x))\n",
    "        \n",
    "        # y = 1 / (1 + e^-x)\n",
    "        # y' = 1 / (1 + e^-x) (1 - 1 / (1 + e^-x))\n",
    "        \n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]  # Get the partial of the cost with respect to this node.\n",
    "\n",
    "            self.gradients[self.inputs[0]] = grad_cost * self.partial\n",
    "            # use * to keep all the dimension same!.\n",
    "\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        y = self.inputs[0].value.reshape(-1, 1)\n",
    "        a = self.inputs[1].value.reshape(-1, 1)\n",
    "        assert(y.shape == a.shape)\n",
    "\n",
    "        self.m = self.inputs[0].value.shape[0]\n",
    "        self.diff = y - a\n",
    "\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        self.gradients[self.inputs[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inputs[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def forward_and_backward(outputnode, graph):\n",
    "    # execute all the forward method of sorted_nodes.\n",
    "\n",
    "    ## In practice, it's common to feed in mutiple data example in each forward pass rather than just 1. \n",
    "    ## Because the examples can be processed in parallel. The number of examples is called batch size.\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "        ## each node execute forward, get self.value based on the topological sort result.\n",
    "\n",
    "    for n in  graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "    #return outputnode.value\n",
    "\n",
    "###   v -->  a -->  C\n",
    "##    b --> C\n",
    "##    b --> v -- a --> C\n",
    "##    v --> v ---> a -- > C\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "            ## if n is Input Node, set n'value as \n",
    "            ## feed_dict[n]\n",
    "            ## else, n's value is caculate as its\n",
    "            ## inbounds\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    # there are so many other update / optimization methods\n",
    "    # such as Adam, Mom, \n",
    "    for t in trainables:\n",
    "        t.value += -1 * learning_rate * t.gradients[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 170.474\n",
      "Epoch: 101, Loss: 6.181\n",
      "Epoch: 201, Loss: 5.384\n",
      "Epoch: 301, Loss: 5.605\n",
      "Epoch: 401, Loss: 5.069\n",
      "Epoch: 501, Loss: 4.489\n",
      "Epoch: 601, Loss: 3.337\n",
      "Epoch: 701, Loss: 4.082\n",
      "Epoch: 801, Loss: 3.838\n",
      "Epoch: 901, Loss: 3.012\n",
      "Epoch: 1001, Loss: 3.951\n",
      "Epoch: 1101, Loss: 2.961\n",
      "Epoch: 1201, Loss: 2.855\n",
      "Epoch: 1301, Loss: 3.913\n",
      "Epoch: 1401, Loss: 3.797\n",
      "Epoch: 1501, Loss: 3.387\n",
      "Epoch: 1601, Loss: 3.078\n",
      "Epoch: 1701, Loss: 3.845\n",
      "Epoch: 1801, Loss: 3.174\n",
      "Epoch: 1901, Loss: 2.797\n",
      "Epoch: 2001, Loss: 3.403\n",
      "Epoch: 2101, Loss: 3.517\n",
      "Epoch: 2201, Loss: 3.167\n",
      "Epoch: 2301, Loss: 3.393\n",
      "Epoch: 2401, Loss: 2.931\n",
      "Epoch: 2501, Loss: 3.079\n",
      "Epoch: 2601, Loss: 3.308\n",
      "Epoch: 2701, Loss: 2.888\n",
      "Epoch: 2801, Loss: 3.538\n",
      "Epoch: 2901, Loss: 3.323\n",
      "Epoch: 3001, Loss: 3.145\n",
      "Epoch: 3101, Loss: 2.786\n",
      "Epoch: 3201, Loss: 3.296\n",
      "Epoch: 3301, Loss: 3.394\n",
      "Epoch: 3401, Loss: 2.945\n",
      "Epoch: 3501, Loss: 3.096\n",
      "Epoch: 3601, Loss: 3.405\n",
      "Epoch: 3701, Loss: 2.821\n",
      "Epoch: 3801, Loss: 3.027\n",
      "Epoch: 3901, Loss: 2.848\n",
      "Epoch: 4001, Loss: 3.202\n",
      "Epoch: 4101, Loss: 3.116\n",
      "Epoch: 4201, Loss: 2.783\n",
      "Epoch: 4301, Loss: 3.007\n",
      "Epoch: 4401, Loss: 2.566\n",
      "Epoch: 4501, Loss: 2.927\n",
      "Epoch: 4601, Loss: 2.992\n",
      "Epoch: 4701, Loss: 2.713\n",
      "Epoch: 4801, Loss: 3.263\n",
      "Epoch: 4901, Loss: 2.854\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "losses = []\n",
    "\n",
    "\"\"\"\n",
    "Check out the new network architecture and dataset!\n",
    "Notice that the weights and biases are\n",
    "generated randomly.\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size, etc!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 5000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 16\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        _ = None\n",
    "        forward_and_backward(_, graph) # set output node not important.\n",
    "\n",
    "        # Step 3\n",
    "        rate = 1e-2\n",
    "    \n",
    "        sgd_update(trainables, rate)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a257ec450>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Optimizer Adam\n",
    "## 8.1 SGD + Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./SGD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./AdaGrad.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](RMSProp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Adam.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试机器学习问题比较好的默认参数设定为：\n",
    "\n",
    "alpha=0.001、beta1=0.9、beta2=0.999 和 epsilon=10E−8。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用框架默认参数设定\n",
    "TensorFlow：learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08.\n",
    "\n",
    "Keras：lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0.\n",
    "\n",
    "Blocks：learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-08, decay_factor=1.\n",
    "\n",
    "Lasagne：learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08\n",
    "\n",
    "Caffe：learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08\n",
    "\n",
    "MxNet：learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8\n",
    "\n",
    "Torch：learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.6 (default, Jan  8 2020, 13:42:34) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "start to optimize:\n",
      "weights:[ 1.85660145 -0.36412288  2.44143684] loss:0.02741367962460835\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, loss, weights, lr=0.001, beta1=0.9, beta2=0.999, epislon=1e-8):\n",
    "        self.loss = loss\n",
    "        self.theta = weights\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epislon = epislon\n",
    "        self.get_gradient = grad(loss)\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        self.t = 0\n",
    "    def minimize_raw(self):\n",
    "        self.t += 1\n",
    "        g = self.get_gradient(self.theta)\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * g\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (g * g)\n",
    "        self.m_cat = self.m / (1 - self.beta1 ** self.t)\n",
    "        self.v_cat = self.v / (1 - self.beta2 ** self.t)\n",
    "        self.theta -= self.lr * self.m_cat / (self.v_cat ** 0.5 + self.epislon)\n",
    "\n",
    "    def minimize(self):\n",
    "        self.t += 1\n",
    "        g = self.get_gradient(self.theta)\n",
    "        lr = self.lr * (1 - self.beta2 ** self.t) ** 0.5 / (1 - self.beta1 ** self.t)\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * g\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (g * g)\n",
    "        self.theta -= lr * self.m / (self.v ** 0.5 + self.epislon)\n",
    "        \n",
    "        \n",
    "    def minimize_show(self, epochs=5000):\n",
    "        for _ in range(epochs):\n",
    "            self.t += 1\n",
    "            g = self.get_gradient(self.theta)\n",
    "            lr = self.lr * (1 - self.beta2 ** self.t) ** 0.5 / (1 - self.beta1 ** self.t)\n",
    "            self.m = self.beta1 * self.m + (1 - self.beta1) * g\n",
    "            self.v = self.beta2 * self.v + (1 - self.beta2) * (g * g)\n",
    "            self.theta -= lr * self.m / (self.v ** 0.5 + self.epislon)\n",
    "            print(\"step{: 4d} g:{} lr:{} m:{} v:{} theta:{}\".format(self.t, g, lr, self.m, self.v, self.theta))\n",
    "            final_loss = self.loss(self.theta)\n",
    "            print(\"final loss:{} weights:{}\".format(final_loss, self.theta))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5*(np.tanh(x) + 1)\n",
    "\n",
    "def logistic_predictions(weights, inputs):\n",
    "    # Outputs probability of a label being true according to logistic model.\n",
    "    return sigmoid(np.dot(inputs, weights))\n",
    "\n",
    "def training_loss(weights):\n",
    "    # Training loss is the negative log-likelihood of the training labels.\n",
    "    preds = logistic_predictions(weights, inputs)\n",
    "    label_probabilities = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -np.sum(np.log(label_probabilities))\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = np.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "targets = np.array([True, True, False, True])\n",
    "weights = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "def sgd(epochs=1000):\n",
    "    training_gradient_fun = grad(training_loss)\n",
    "    \n",
    "    # Optimize weights using gradient descent.\n",
    "    weights = np.array([0.0, 0.0, 0.0])\n",
    "    print(\"Initial loss:{}\".format(training_loss(weights)))\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        weights -= training_gradient_fun(weights) * 0.01\n",
    "\n",
    "    print(\"Trained loss:{}\".format(training_loss(weights)))\n",
    "    print(\"weights:{}\".format(weights))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    adam = Adam(training_loss, weights, lr=0.01)\n",
    "    print(\"start to optimize:\")\n",
    "    # adam.minimize_show(epochs=EPOCHS)\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        adam.minimize_raw()\n",
    "    print(\"weights:{} loss:{}\".format(adam.theta, adam.loss(adam.theta)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 深度学习框架\n",
    "## 9.1 Tensorflow\n",
    "请学生自行安装tensorflow 2.0版本，没有安装的同学，可以在cell中运行【! pip install tensorflow==2.0.0b1】。  \n",
    "同学，你可以自行阅读[tensorflow 2.0 中文文档](http://www.tensorfly.cn/tfdoc/get_started/basic_usage.html)    \n",
    "**更建议同学直接阅读**[英文文档](https://tensorflow.google.cn/tutorials/quickstart/beginner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-de672dc32709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_floatx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcast_to_floatx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/backend/load_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# Try and load external backend.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfdev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, activation='sigmoid', input_dim=13))\n",
    "model.add(Dense(units=30, activation='sigmoid', input_dim=64))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-161d380db306>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(X_, y_, epochs=1000, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
