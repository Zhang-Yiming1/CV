{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://img.kaikeba.com/web/hcTech/img_logo.png\" alt=\"图片替换文本\" width=\"500\" height=\"150\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Foundation of Artificial Intelligence.Lecture 2\n",
    "## 目录\n",
    "\n",
    "- Linear Regression\n",
    "- Loss Function\n",
    "- Gradient Descent\n",
    "- Activation Function\n",
    "- Logistic Regression\n",
    "- Overfitting and Underfitting\n",
    "- Bias and Variance\n",
    "- Regularlization\n",
    "- Optimizer方法、Adam等优化方法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Fit A Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.loli.net/2020/04/16/qebkFl2mC6XKEac.png\" width=\"800\" height=\"800\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array([1., 2. ,3. ,4. ,5.])\n",
    "y = np.array([1., 3. ,2. ,3. ,5.])\n",
    "plt.scatter(x,y) # 绘制 x 与 y\n",
    "plt.grid() # 显示网格"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题1：已知两个点（1.0，1.0）与（5.0，5.0），求解二元一次方程？    \n",
    "问题2：对于上面的五个点，可以用一条直线表示吗？    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img-blog.csdnimg.cn/20200410205259492.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"500\" height=\"500\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直线的值记为 $\\hat y$。那 $|\\hat y - y|$ 表示拟合直线与真实值的误差(Error)，即是红色的线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/2020041107263822.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 损失函数(Loss Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 平均绝对误差（Mean Absolute Error，MAE）\n",
    "\n",
    "若有$m$个数据，第$i$个数据下的真实值表示为$y^i$,拟合曲线在第个值下的值为 $ \\hat{ y^i}$, MAE表示为：\n",
    "\n",
    "$$ MAE = \\frac 1m\\sum_{i=1}^m | \\hat {y^i} - y^i | $$\n",
    "\n",
    "- 均方误差（Mean Squared Error，MSE）\n",
    "\n",
    "同样，考虑到拟合直线与真实值的面积误差，表达式如下：\n",
    "\n",
    "$$ MSE = \\frac {1}{2m}\\sum_{i=1}^m ( \\hat {y^i} - y^i )^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    :param y: the real fares\n",
    "    :param y_hat: the estimated fares\n",
    "    :return: how good is the estimated fares\n",
    "    \"\"\"\n",
    "\n",
    "    return np.mean(np.abs(y_hat - y))\n",
    "    # return np.mean(np.square(y_hat - y))\n",
    "    # return np.mean(np.sqrt(y_hat - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    return sum(abs(y_hat_i - y_i) for y_hat_i, y_i in zip(list(y_hat),list(y))) / len(list(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3333333333333335"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([1.,1.,1.])\n",
    "y_hat = np.array([-2.,-1.,-1.])\n",
    "loss(y,y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function (x, k, b):\n",
    "    \"\"\"y = kx + b\"\"\"\n",
    "    return x*k + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 方法1，遍历求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k:[-1,1] 步长：0.1   \n",
    "b:[-1,1] 步长：0.1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in range(-10, 11):\n",
    "    k *= 0.1\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in range(-100, 101):\n",
    "    k *= 0.01\n",
    "    for b in range(-100,101):\n",
    "        b *= 0.01\n",
    "        y_hat = [function(x_i,k,b) for x_i in list(x)]\n",
    "        #print(y_hat)\n",
    "        current_loss = loss(y,y_hat)\n",
    "        print(current_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best k is -1.0, best b is -1.0, and the loss is 6.8\n",
      "best k is -1.0, best b is -0.99, and the loss is 6.790000000000001\n",
      "best k is -1.0, best b is -0.98, and the loss is 6.780000000000001\n",
      "best k is -1.0, best b is -0.97, and the loss is 6.769999999999999\n",
      "best k is -1.0, best b is -0.96, and the loss is 6.76\n",
      "best k is -1.0, best b is -0.9500000000000001, and the loss is 6.75\n",
      "best k is -1.0, best b is -0.9400000000000001, and the loss is 6.74\n",
      "best k is -1.0, best b is -0.93, and the loss is 6.7299999999999995\n",
      "best k is -1.0, best b is -0.92, and the loss is 6.720000000000001\n",
      "best k is -1.0, best b is -0.91, and the loss is 6.709999999999999\n",
      "best k is -1.0, best b is -0.9, and the loss is 6.7\n",
      "best k is -1.0, best b is -0.89, and the loss is 6.69\n",
      "best k is -1.0, best b is -0.88, and the loss is 6.68\n",
      "best k is -1.0, best b is -0.87, and the loss is 6.67\n",
      "best k is -1.0, best b is -0.86, and the loss is 6.659999999999999\n",
      "best k is -1.0, best b is -0.85, and the loss is 6.65\n",
      "best k is -1.0, best b is -0.84, and the loss is 6.640000000000001\n",
      "best k is -1.0, best b is -0.8300000000000001, and the loss is 6.63\n",
      "best k is -1.0, best b is -0.8200000000000001, and the loss is 6.62\n",
      "best k is -1.0, best b is -0.81, and the loss is 6.610000000000001\n",
      "best k is -1.0, best b is -0.8, and the loss is 6.6\n",
      "best k is -1.0, best b is -0.79, and the loss is 6.590000000000001\n",
      "best k is -1.0, best b is -0.78, and the loss is 6.580000000000001\n",
      "best k is -1.0, best b is -0.77, and the loss is 6.5699999999999985\n",
      "best k is -1.0, best b is -0.76, and the loss is 6.56\n",
      "best k is -1.0, best b is -0.75, and the loss is 6.55\n",
      "best k is -1.0, best b is -0.74, and the loss is 6.540000000000001\n",
      "best k is -1.0, best b is -0.73, and the loss is 6.530000000000001\n",
      "best k is -1.0, best b is -0.72, and the loss is 6.519999999999999\n",
      "best k is -1.0, best b is -0.71, and the loss is 6.51\n",
      "best k is -1.0, best b is -0.7000000000000001, and the loss is 6.5\n",
      "best k is -1.0, best b is -0.6900000000000001, and the loss is 6.49\n",
      "best k is -1.0, best b is -0.68, and the loss is 6.4799999999999995\n",
      "best k is -1.0, best b is -0.67, and the loss is 6.470000000000001\n",
      "best k is -1.0, best b is -0.66, and the loss is 6.459999999999999\n",
      "best k is -1.0, best b is -0.65, and the loss is 6.45\n",
      "best k is -1.0, best b is -0.64, and the loss is 6.44\n",
      "best k is -1.0, best b is -0.63, and the loss is 6.43\n",
      "best k is -1.0, best b is -0.62, and the loss is 6.42\n",
      "best k is -1.0, best b is -0.61, and the loss is 6.409999999999999\n",
      "best k is -1.0, best b is -0.6, and the loss is 6.4\n",
      "best k is -1.0, best b is -0.59, and the loss is 6.39\n",
      "best k is -1.0, best b is -0.58, and the loss is 6.38\n",
      "best k is -1.0, best b is -0.5700000000000001, and the loss is 6.37\n",
      "best k is -1.0, best b is -0.56, and the loss is 6.360000000000001\n",
      "best k is -1.0, best b is -0.55, and the loss is 6.35\n",
      "best k is -1.0, best b is -0.54, and the loss is 6.34\n",
      "best k is -1.0, best b is -0.53, and the loss is 6.33\n",
      "best k is -1.0, best b is -0.52, and the loss is 6.319999999999999\n",
      "best k is -1.0, best b is -0.51, and the loss is 6.31\n",
      "best k is -1.0, best b is -0.5, and the loss is 6.3\n",
      "best k is -1.0, best b is -0.49, and the loss is 6.290000000000001\n",
      "best k is -1.0, best b is -0.48, and the loss is 6.28\n",
      "best k is -1.0, best b is -0.47000000000000003, and the loss is 6.27\n",
      "best k is -1.0, best b is -0.46, and the loss is 6.26\n",
      "best k is -1.0, best b is -0.45, and the loss is 6.25\n",
      "best k is -1.0, best b is -0.44, and the loss is 6.24\n",
      "best k is -1.0, best b is -0.43, and the loss is 6.2299999999999995\n",
      "best k is -1.0, best b is -0.42, and the loss is 6.220000000000001\n",
      "best k is -1.0, best b is -0.41000000000000003, and the loss is 6.21\n",
      "best k is -1.0, best b is -0.4, and the loss is 6.2\n",
      "best k is -1.0, best b is -0.39, and the loss is 6.19\n",
      "best k is -1.0, best b is -0.38, and the loss is 6.18\n",
      "best k is -1.0, best b is -0.37, and the loss is 6.17\n",
      "best k is -1.0, best b is -0.36, and the loss is 6.159999999999999\n",
      "best k is -1.0, best b is -0.35000000000000003, and the loss is 6.15\n",
      "best k is -1.0, best b is -0.34, and the loss is 6.14\n",
      "best k is -1.0, best b is -0.33, and the loss is 6.13\n",
      "best k is -1.0, best b is -0.32, and the loss is 6.12\n",
      "best k is -1.0, best b is -0.31, and the loss is 6.11\n",
      "best k is -1.0, best b is -0.3, and the loss is 6.1\n",
      "best k is -1.0, best b is -0.29, and the loss is 6.09\n",
      "best k is -1.0, best b is -0.28, and the loss is 6.08\n",
      "best k is -1.0, best b is -0.27, and the loss is 6.069999999999999\n",
      "best k is -1.0, best b is -0.26, and the loss is 6.06\n",
      "best k is -1.0, best b is -0.25, and the loss is 6.05\n",
      "best k is -1.0, best b is -0.24, and the loss is 6.040000000000001\n",
      "best k is -1.0, best b is -0.23, and the loss is 6.03\n",
      "best k is -1.0, best b is -0.22, and the loss is 6.02\n",
      "best k is -1.0, best b is -0.21, and the loss is 6.01\n",
      "best k is -1.0, best b is -0.2, and the loss is 6.0\n",
      "best k is -1.0, best b is -0.19, and the loss is 5.99\n",
      "best k is -1.0, best b is -0.18, and the loss is 5.9799999999999995\n",
      "best k is -1.0, best b is -0.17, and the loss is 5.970000000000001\n",
      "best k is -1.0, best b is -0.16, and the loss is 5.96\n",
      "best k is -1.0, best b is -0.15, and the loss is 5.95\n",
      "best k is -1.0, best b is -0.14, and the loss is 5.94\n",
      "best k is -1.0, best b is -0.13, and the loss is 5.93\n",
      "best k is -1.0, best b is -0.12, and the loss is 5.92\n",
      "best k is -1.0, best b is -0.11, and the loss is 5.909999999999999\n",
      "best k is -1.0, best b is -0.1, and the loss is 5.9\n",
      "best k is -1.0, best b is -0.09, and the loss is 5.89\n",
      "best k is -1.0, best b is -0.08, and the loss is 5.88\n",
      "best k is -1.0, best b is -0.07, and the loss is 5.87\n",
      "best k is -1.0, best b is -0.06, and the loss is 5.86\n",
      "best k is -1.0, best b is -0.05, and the loss is 5.85\n",
      "best k is -1.0, best b is -0.04, and the loss is 5.84\n",
      "best k is -1.0, best b is -0.03, and the loss is 5.83\n",
      "best k is -1.0, best b is -0.02, and the loss is 5.819999999999999\n",
      "best k is -1.0, best b is -0.01, and the loss is 5.81\n",
      "best k is -1.0, best b is 0.0, and the loss is 5.8\n",
      "best k is -1.0, best b is 0.01, and the loss is 5.790000000000001\n",
      "best k is -1.0, best b is 0.02, and the loss is 5.78\n",
      "best k is -1.0, best b is 0.03, and the loss is 5.7700000000000005\n",
      "best k is -1.0, best b is 0.04, and the loss is 5.76\n",
      "best k is -1.0, best b is 0.05, and the loss is 5.75\n",
      "best k is -1.0, best b is 0.06, and the loss is 5.74\n",
      "best k is -1.0, best b is 0.07, and the loss is 5.7299999999999995\n",
      "best k is -1.0, best b is 0.08, and the loss is 5.720000000000001\n",
      "best k is -1.0, best b is 0.09, and the loss is 5.71\n",
      "best k is -1.0, best b is 0.1, and the loss is 5.7\n",
      "best k is -1.0, best b is 0.11, and the loss is 5.69\n",
      "best k is -1.0, best b is 0.12, and the loss is 5.68\n",
      "best k is -1.0, best b is 0.13, and the loss is 5.67\n",
      "best k is -1.0, best b is 0.14, and the loss is 5.659999999999999\n",
      "best k is -1.0, best b is 0.15, and the loss is 5.65\n",
      "best k is -1.0, best b is 0.16, and the loss is 5.64\n",
      "best k is -1.0, best b is 0.17, and the loss is 5.63\n",
      "best k is -1.0, best b is 0.18, and the loss is 5.62\n",
      "best k is -1.0, best b is 0.19, and the loss is 5.61\n",
      "best k is -1.0, best b is 0.2, and the loss is 5.6\n",
      "best k is -1.0, best b is 0.21, and the loss is 5.59\n",
      "best k is -1.0, best b is 0.22, and the loss is 5.58\n",
      "best k is -1.0, best b is 0.23, and the loss is 5.569999999999999\n",
      "best k is -1.0, best b is 0.24, and the loss is 5.56\n",
      "best k is -1.0, best b is 0.25, and the loss is 5.55\n",
      "best k is -1.0, best b is 0.26, and the loss is 5.540000000000001\n",
      "best k is -1.0, best b is 0.27, and the loss is 5.53\n",
      "best k is -1.0, best b is 0.28, and the loss is 5.52\n",
      "best k is -1.0, best b is 0.29, and the loss is 5.51\n",
      "best k is -1.0, best b is 0.3, and the loss is 5.5\n",
      "best k is -1.0, best b is 0.31, and the loss is 5.49\n",
      "best k is -1.0, best b is 0.32, and the loss is 5.4799999999999995\n",
      "best k is -1.0, best b is 0.33, and the loss is 5.470000000000001\n",
      "best k is -1.0, best b is 0.34, and the loss is 5.46\n",
      "best k is -1.0, best b is 0.35000000000000003, and the loss is 5.45\n",
      "best k is -1.0, best b is 0.36, and the loss is 5.44\n",
      "best k is -1.0, best b is 0.37, and the loss is 5.43\n",
      "best k is -1.0, best b is 0.38, and the loss is 5.42\n",
      "best k is -1.0, best b is 0.39, and the loss is 5.409999999999999\n",
      "best k is -1.0, best b is 0.4, and the loss is 5.4\n",
      "best k is -1.0, best b is 0.41000000000000003, and the loss is 5.39\n",
      "best k is -1.0, best b is 0.42, and the loss is 5.38\n",
      "best k is -1.0, best b is 0.43, and the loss is 5.37\n",
      "best k is -1.0, best b is 0.44, and the loss is 5.36\n",
      "best k is -1.0, best b is 0.45, and the loss is 5.35\n",
      "best k is -1.0, best b is 0.46, and the loss is 5.34\n",
      "best k is -1.0, best b is 0.47000000000000003, and the loss is 5.33\n",
      "best k is -1.0, best b is 0.48, and the loss is 5.319999999999999\n",
      "best k is -1.0, best b is 0.49, and the loss is 5.31\n",
      "best k is -1.0, best b is 0.5, and the loss is 5.3\n",
      "best k is -1.0, best b is 0.51, and the loss is 5.290000000000001\n",
      "best k is -1.0, best b is 0.52, and the loss is 5.28\n",
      "best k is -1.0, best b is 0.53, and the loss is 5.27\n",
      "best k is -1.0, best b is 0.54, and the loss is 5.26\n",
      "best k is -1.0, best b is 0.55, and the loss is 5.25\n",
      "best k is -1.0, best b is 0.56, and the loss is 5.239999999999999\n",
      "best k is -1.0, best b is 0.5700000000000001, and the loss is 5.2299999999999995\n",
      "best k is -1.0, best b is 0.58, and the loss is 5.220000000000001\n",
      "best k is -1.0, best b is 0.59, and the loss is 5.21\n",
      "best k is -1.0, best b is 0.6, and the loss is 5.2\n",
      "best k is -1.0, best b is 0.61, and the loss is 5.19\n",
      "best k is -1.0, best b is 0.62, and the loss is 5.18\n",
      "best k is -1.0, best b is 0.63, and the loss is 5.17\n",
      "best k is -1.0, best b is 0.64, and the loss is 5.159999999999999\n",
      "best k is -1.0, best b is 0.65, and the loss is 5.15\n",
      "best k is -1.0, best b is 0.66, and the loss is 5.14\n",
      "best k is -1.0, best b is 0.67, and the loss is 5.13\n",
      "best k is -1.0, best b is 0.68, and the loss is 5.12\n",
      "best k is -1.0, best b is 0.6900000000000001, and the loss is 5.11\n",
      "best k is -1.0, best b is 0.7000000000000001, and the loss is 5.1\n",
      "best k is -1.0, best b is 0.71, and the loss is 5.09\n",
      "best k is -1.0, best b is 0.72, and the loss is 5.08\n",
      "best k is -1.0, best b is 0.73, and the loss is 5.069999999999999\n",
      "best k is -1.0, best b is 0.74, and the loss is 5.06\n",
      "best k is -1.0, best b is 0.75, and the loss is 5.05\n",
      "best k is -1.0, best b is 0.76, and the loss is 5.040000000000001\n",
      "best k is -1.0, best b is 0.77, and the loss is 5.03\n",
      "best k is -1.0, best b is 0.78, and the loss is 5.02\n",
      "best k is -1.0, best b is 0.79, and the loss is 5.01\n",
      "best k is -1.0, best b is 0.8, and the loss is 5.0\n",
      "best k is -1.0, best b is 0.81, and the loss is 4.989999999999999\n",
      "best k is -1.0, best b is 0.8200000000000001, and the loss is 4.9799999999999995\n",
      "best k is -1.0, best b is 0.8300000000000001, and the loss is 4.970000000000001\n",
      "best k is -1.0, best b is 0.84, and the loss is 4.96\n",
      "best k is -1.0, best b is 0.85, and the loss is 4.95\n",
      "best k is -1.0, best b is 0.86, and the loss is 4.94\n",
      "best k is -1.0, best b is 0.87, and the loss is 4.93\n",
      "best k is -1.0, best b is 0.88, and the loss is 4.92\n",
      "best k is -1.0, best b is 0.89, and the loss is 4.909999999999999\n",
      "best k is -1.0, best b is 0.9, and the loss is 4.9\n",
      "best k is -1.0, best b is 0.91, and the loss is 4.89\n",
      "best k is -1.0, best b is 0.92, and the loss is 4.88\n",
      "best k is -1.0, best b is 0.93, and the loss is 4.87\n",
      "best k is -1.0, best b is 0.9400000000000001, and the loss is 4.86\n",
      "best k is -1.0, best b is 0.9500000000000001, and the loss is 4.85\n",
      "best k is -1.0, best b is 0.96, and the loss is 4.84\n",
      "best k is -1.0, best b is 0.97, and the loss is 4.83\n",
      "best k is -1.0, best b is 0.98, and the loss is 4.819999999999999\n",
      "best k is -1.0, best b is 0.99, and the loss is 4.81\n",
      "best k is -1.0, best b is 1.0, and the loss is 4.8\n",
      "best k is -0.99, best b is 0.98, and the loss is 4.790000000000001\n",
      "best k is -0.99, best b is 0.99, and the loss is 4.779999999999999\n",
      "best k is -0.99, best b is 1.0, and the loss is 4.77\n",
      "best k is -0.98, best b is 0.98, and the loss is 4.76\n",
      "best k is -0.98, best b is 0.99, and the loss is 4.75\n",
      "best k is -0.98, best b is 1.0, and the loss is 4.74\n",
      "best k is -0.97, best b is 0.98, and the loss is 4.7299999999999995\n",
      "best k is -0.97, best b is 0.99, and the loss is 4.72\n",
      "best k is -0.97, best b is 1.0, and the loss is 4.709999999999999\n",
      "best k is -0.96, best b is 0.98, and the loss is 4.7\n",
      "best k is -0.96, best b is 0.99, and the loss is 4.6899999999999995\n",
      "best k is -0.96, best b is 1.0, and the loss is 4.68\n",
      "best k is -0.9500000000000001, best b is 0.98, and the loss is 4.67\n",
      "best k is -0.9500000000000001, best b is 0.99, and the loss is 4.66\n",
      "best k is -0.9500000000000001, best b is 1.0, and the loss is 4.65\n",
      "best k is -0.9400000000000001, best b is 0.98, and the loss is 4.640000000000001\n",
      "best k is -0.9400000000000001, best b is 0.99, and the loss is 4.630000000000001\n",
      "best k is -0.9400000000000001, best b is 1.0, and the loss is 4.62\n",
      "best k is -0.93, best b is 0.98, and the loss is 4.61\n",
      "best k is -0.93, best b is 0.99, and the loss is 4.6\n",
      "best k is -0.93, best b is 1.0, and the loss is 4.590000000000001\n",
      "best k is -0.92, best b is 0.98, and the loss is 4.58\n",
      "best k is -0.92, best b is 0.99, and the loss is 4.57\n",
      "best k is -0.92, best b is 1.0, and the loss is 4.5600000000000005\n",
      "best k is -0.91, best b is 0.98, and the loss is 4.55\n",
      "best k is -0.91, best b is 0.99, and the loss is 4.54\n",
      "best k is -0.91, best b is 1.0, and the loss is 4.53\n",
      "best k is -0.9, best b is 0.98, and the loss is 4.5200000000000005\n",
      "best k is -0.9, best b is 0.99, and the loss is 4.51\n",
      "best k is -0.9, best b is 1.0, and the loss is 4.5\n",
      "best k is -0.89, best b is 0.98, and the loss is 4.49\n",
      "best k is -0.89, best b is 0.99, and the loss is 4.48\n",
      "best k is -0.89, best b is 1.0, and the loss is 4.470000000000001\n",
      "best k is -0.88, best b is 0.98, and the loss is 4.459999999999999\n",
      "best k is -0.88, best b is 0.99, and the loss is 4.45\n",
      "best k is -0.88, best b is 1.0, and the loss is 4.4399999999999995\n",
      "best k is -0.87, best b is 0.98, and the loss is 4.43\n",
      "best k is -0.87, best b is 0.99, and the loss is 4.42\n",
      "best k is -0.87, best b is 1.0, and the loss is 4.41\n",
      "best k is -0.86, best b is 0.97, and the loss is 4.409999999999999\n",
      "best k is -0.86, best b is 0.98, and the loss is 4.4\n",
      "best k is -0.86, best b is 0.99, and the loss is 4.39\n",
      "best k is -0.86, best b is 1.0, and the loss is 4.38\n",
      "best k is -0.85, best b is 0.98, and the loss is 4.37\n",
      "best k is -0.85, best b is 0.99, and the loss is 4.359999999999999\n",
      "best k is -0.85, best b is 1.0, and the loss is 4.35\n",
      "best k is -0.84, best b is 0.98, and the loss is 4.340000000000001\n",
      "best k is -0.84, best b is 0.99, and the loss is 4.33\n",
      "best k is -0.84, best b is 1.0, and the loss is 4.319999999999999\n",
      "best k is -0.8300000000000001, best b is 0.98, and the loss is 4.3100000000000005\n",
      "best k is -0.8300000000000001, best b is 0.99, and the loss is 4.3\n",
      "best k is -0.8300000000000001, best b is 1.0, and the loss is 4.290000000000001\n",
      "best k is -0.8200000000000001, best b is 0.98, and the loss is 4.28\n",
      "best k is -0.8200000000000001, best b is 0.99, and the loss is 4.2700000000000005\n",
      "best k is -0.8200000000000001, best b is 1.0, and the loss is 4.26\n",
      "best k is -0.81, best b is 0.98, and the loss is 4.25\n",
      "best k is -0.81, best b is 0.99, and the loss is 4.24\n",
      "best k is -0.81, best b is 1.0, and the loss is 4.2299999999999995\n",
      "best k is -0.8, best b is 0.98, and the loss is 4.220000000000001\n",
      "best k is -0.8, best b is 0.99, and the loss is 4.209999999999999\n",
      "best k is -0.8, best b is 1.0, and the loss is 4.2\n",
      "best k is -0.79, best b is 0.98, and the loss is 4.19\n",
      "best k is -0.79, best b is 0.99, and the loss is 4.18\n",
      "best k is -0.79, best b is 1.0, and the loss is 4.17\n",
      "best k is -0.78, best b is 0.98, and the loss is 4.16\n",
      "best k is -0.78, best b is 0.99, and the loss is 4.15\n",
      "best k is -0.78, best b is 1.0, and the loss is 4.140000000000001\n",
      "best k is -0.77, best b is 0.97, and the loss is 4.14\n",
      "best k is -0.77, best b is 0.98, and the loss is 4.13\n",
      "best k is -0.77, best b is 0.99, and the loss is 4.12\n",
      "best k is -0.77, best b is 1.0, and the loss is 4.11\n",
      "best k is -0.76, best b is 0.98, and the loss is 4.1\n",
      "best k is -0.76, best b is 0.99, and the loss is 4.09\n",
      "best k is -0.76, best b is 1.0, and the loss is 4.08\n",
      "best k is -0.75, best b is 0.98, and the loss is 4.07\n",
      "best k is -0.75, best b is 0.99, and the loss is 4.06\n",
      "best k is -0.75, best b is 1.0, and the loss is 4.05\n",
      "best k is -0.74, best b is 0.98, and the loss is 4.040000000000001\n",
      "best k is -0.74, best b is 0.99, and the loss is 4.029999999999999\n",
      "best k is -0.74, best b is 1.0, and the loss is 4.02\n",
      "best k is -0.73, best b is 0.98, and the loss is 4.01\n",
      "best k is -0.73, best b is 0.99, and the loss is 4.0\n",
      "best k is -0.73, best b is 1.0, and the loss is 3.9899999999999998\n",
      "best k is -0.72, best b is 0.98, and the loss is 3.9799999999999995\n",
      "best k is -0.72, best b is 0.99, and the loss is 3.9699999999999998\n",
      "best k is -0.72, best b is 1.0, and the loss is 3.9599999999999995\n",
      "best k is -0.71, best b is 0.98, and the loss is 3.95\n",
      "best k is -0.71, best b is 0.99, and the loss is 3.94\n",
      "best k is -0.71, best b is 1.0, and the loss is 3.9299999999999997\n",
      "best k is -0.7000000000000001, best b is 0.98, and the loss is 3.9200000000000004\n",
      "best k is -0.7000000000000001, best b is 0.99, and the loss is 3.91\n",
      "best k is -0.7000000000000001, best b is 1.0, and the loss is 3.9\n",
      "best k is -0.6900000000000001, best b is 0.98, and the loss is 3.8900000000000006\n",
      "best k is -0.6900000000000001, best b is 0.99, and the loss is 3.8800000000000003\n",
      "best k is -0.6900000000000001, best b is 1.0, and the loss is 3.87\n",
      "best k is -0.68, best b is 0.98, and the loss is 3.8600000000000003\n",
      "best k is -0.68, best b is 0.99, and the loss is 3.85\n",
      "best k is -0.68, best b is 1.0, and the loss is 3.8400000000000007\n",
      "best k is -0.67, best b is 0.97, and the loss is 3.84\n",
      "best k is -0.67, best b is 0.98, and the loss is 3.8300000000000005\n",
      "best k is -0.67, best b is 0.99, and the loss is 3.8200000000000003\n",
      "best k is -0.67, best b is 1.0, and the loss is 3.8099999999999996\n",
      "best k is -0.66, best b is 0.98, and the loss is 3.8\n",
      "best k is -0.66, best b is 0.99, and the loss is 3.7900000000000005\n",
      "best k is -0.66, best b is 1.0, and the loss is 3.7800000000000002\n",
      "best k is -0.65, best b is 0.98, and the loss is 3.7700000000000005\n",
      "best k is -0.65, best b is 0.99, and the loss is 3.7599999999999993\n",
      "best k is -0.65, best b is 1.0, and the loss is 3.75\n",
      "best k is -0.64, best b is 0.98, and the loss is 3.7400000000000007\n",
      "best k is -0.64, best b is 0.99, and the loss is 3.7299999999999995\n",
      "best k is -0.64, best b is 1.0, and the loss is 3.72\n",
      "best k is -0.63, best b is 0.98, and the loss is 3.7099999999999995\n",
      "best k is -0.63, best b is 0.99, and the loss is 3.7\n",
      "best k is -0.63, best b is 1.0, and the loss is 3.69\n",
      "best k is -0.62, best b is 0.98, and the loss is 3.6799999999999997\n",
      "best k is -0.62, best b is 0.99, and the loss is 3.6700000000000004\n",
      "best k is -0.62, best b is 1.0, and the loss is 3.66\n",
      "best k is -0.61, best b is 0.97, and the loss is 3.6599999999999993\n",
      "best k is -0.61, best b is 0.98, and the loss is 3.65\n",
      "best k is -0.61, best b is 0.99, and the loss is 3.6399999999999997\n",
      "best k is -0.61, best b is 1.0, and the loss is 3.63\n",
      "best k is -0.6, best b is 0.98, and the loss is 3.62\n",
      "best k is -0.6, best b is 0.99, and the loss is 3.6099999999999994\n",
      "best k is -0.6, best b is 1.0, and the loss is 3.6\n",
      "best k is -0.59, best b is 0.98, and the loss is 3.59\n",
      "best k is -0.59, best b is 0.99, and the loss is 3.5800000000000005\n",
      "best k is -0.59, best b is 1.0, and the loss is 3.5699999999999994\n",
      "best k is -0.58, best b is 0.98, and the loss is 3.5599999999999996\n",
      "best k is -0.58, best b is 0.99, and the loss is 3.55\n",
      "best k is -0.58, best b is 1.0, and the loss is 3.5400000000000005\n",
      "best k is -0.5700000000000001, best b is 0.98, and the loss is 3.5300000000000002\n",
      "best k is -0.5700000000000001, best b is 0.99, and the loss is 3.5200000000000005\n",
      "best k is -0.5700000000000001, best b is 1.0, and the loss is 3.5100000000000002\n",
      "best k is -0.56, best b is 0.98, and the loss is 3.5\n",
      "best k is -0.56, best b is 0.99, and the loss is 3.4900000000000007\n",
      "best k is -0.56, best b is 1.0, and the loss is 3.4800000000000004\n",
      "best k is -0.55, best b is 0.98, and the loss is 3.47\n",
      "best k is -0.55, best b is 0.99, and the loss is 3.4599999999999995\n",
      "best k is -0.55, best b is 1.0, and the loss is 3.45\n",
      "best k is -0.54, best b is 0.98, and the loss is 3.4400000000000004\n",
      "best k is -0.54, best b is 0.99, and the loss is 3.4299999999999997\n",
      "best k is -0.54, best b is 1.0, and the loss is 3.4200000000000004\n",
      "best k is -0.53, best b is 0.98, and the loss is 3.41\n",
      "best k is -0.53, best b is 0.99, and the loss is 3.4\n",
      "best k is -0.53, best b is 1.0, and the loss is 3.3900000000000006\n",
      "best k is -0.52, best b is 0.97, and the loss is 3.3899999999999997\n",
      "best k is -0.52, best b is 0.98, and the loss is 3.38\n",
      "best k is -0.52, best b is 0.99, and the loss is 3.37\n",
      "best k is -0.52, best b is 1.0, and the loss is 3.3599999999999994\n",
      "best k is -0.51, best b is 0.98, and the loss is 3.35\n",
      "best k is -0.51, best b is 0.99, and the loss is 3.34\n",
      "best k is -0.51, best b is 1.0, and the loss is 3.3300000000000005\n",
      "best k is -0.5, best b is 0.98, and the loss is 3.3200000000000003\n",
      "best k is -0.5, best b is 0.99, and the loss is 3.3099999999999996\n",
      "best k is -0.5, best b is 1.0, and the loss is 3.3\n",
      "best k is -0.49, best b is 0.98, and the loss is 3.2900000000000005\n",
      "best k is -0.49, best b is 0.99, and the loss is 3.2800000000000002\n",
      "best k is -0.49, best b is 1.0, and the loss is 3.2699999999999996\n",
      "best k is -0.48, best b is 0.98, and the loss is 3.2599999999999993\n",
      "best k is -0.48, best b is 0.99, and the loss is 3.25\n",
      "best k is -0.48, best b is 1.0, and the loss is 3.2400000000000007\n",
      "best k is -0.47000000000000003, best b is 0.97, and the loss is 3.2399999999999998\n",
      "best k is -0.47000000000000003, best b is 0.98, and the loss is 3.2300000000000004\n",
      "best k is -0.47000000000000003, best b is 0.99, and the loss is 3.22\n",
      "best k is -0.47000000000000003, best b is 1.0, and the loss is 3.2099999999999995\n",
      "best k is -0.46, best b is 0.98, and the loss is 3.2\n",
      "best k is -0.46, best b is 0.99, and the loss is 3.1900000000000004\n",
      "best k is -0.46, best b is 1.0, and the loss is 3.18\n",
      "best k is -0.45, best b is 0.98, and the loss is 3.17\n",
      "best k is -0.45, best b is 0.99, and the loss is 3.16\n",
      "best k is -0.45, best b is 1.0, and the loss is 3.15\n",
      "best k is -0.44, best b is 0.98, and the loss is 3.14\n",
      "best k is -0.44, best b is 0.99, and the loss is 3.13\n",
      "best k is -0.44, best b is 1.0, and the loss is 3.12\n",
      "best k is -0.43, best b is 0.98, and the loss is 3.1100000000000003\n",
      "best k is -0.43, best b is 0.99, and the loss is 3.1\n",
      "best k is -0.43, best b is 1.0, and the loss is 3.0900000000000003\n",
      "best k is -0.42, best b is 0.97, and the loss is 3.09\n",
      "best k is -0.42, best b is 0.98, and the loss is 3.0800000000000005\n",
      "best k is -0.42, best b is 0.99, and the loss is 3.0700000000000003\n",
      "best k is -0.42, best b is 1.0, and the loss is 3.0599999999999996\n",
      "best k is -0.41000000000000003, best b is 0.98, and the loss is 3.05\n",
      "best k is -0.41000000000000003, best b is 0.99, and the loss is 3.04\n",
      "best k is -0.41000000000000003, best b is 1.0, and the loss is 3.0300000000000002\n",
      "best k is -0.4, best b is 0.98, and the loss is 3.0200000000000005\n",
      "best k is -0.4, best b is 0.99, and the loss is 3.01\n",
      "best k is -0.4, best b is 1.0, and the loss is 3.0\n",
      "best k is -0.39, best b is 0.98, and the loss is 2.99\n",
      "best k is -0.39, best b is 0.99, and the loss is 2.9799999999999995\n",
      "best k is -0.39, best b is 1.0, and the loss is 2.97\n",
      "best k is -0.38, best b is 0.97, and the loss is 2.9699999999999998\n",
      "best k is -0.38, best b is 0.98, and the loss is 2.96\n",
      "best k is -0.38, best b is 0.99, and the loss is 2.95\n",
      "best k is -0.38, best b is 1.0, and the loss is 2.94\n",
      "best k is -0.37, best b is 0.98, and the loss is 2.9299999999999997\n",
      "best k is -0.37, best b is 0.99, and the loss is 2.9200000000000004\n",
      "best k is -0.37, best b is 1.0, and the loss is 2.91\n",
      "best k is -0.36, best b is 0.97, and the loss is 2.9099999999999997\n",
      "best k is -0.36, best b is 0.98, and the loss is 2.9\n",
      "best k is -0.36, best b is 0.99, and the loss is 2.8899999999999997\n",
      "best k is -0.36, best b is 1.0, and the loss is 2.88\n",
      "best k is -0.35000000000000003, best b is 0.98, and the loss is 2.87\n",
      "best k is -0.35000000000000003, best b is 0.99, and the loss is 2.8600000000000003\n",
      "best k is -0.35000000000000003, best b is 1.0, and the loss is 2.85\n",
      "best k is -0.34, best b is 0.98, and the loss is 2.8400000000000003\n",
      "best k is -0.34, best b is 0.99, and the loss is 2.8300000000000005\n",
      "best k is -0.34, best b is 1.0, and the loss is 2.8200000000000003\n",
      "best k is -0.33, best b is 0.97, and the loss is 2.82\n",
      "best k is -0.33, best b is 0.98, and the loss is 2.8099999999999996\n",
      "best k is -0.33, best b is 0.99, and the loss is 2.8\n",
      "best k is -0.33, best b is 1.0, and the loss is 2.79\n",
      "best k is -0.32, best b is 0.98, and the loss is 2.7800000000000002\n",
      "best k is -0.32, best b is 0.99, and the loss is 2.7700000000000005\n",
      "best k is -0.32, best b is 1.0, and the loss is 2.76\n",
      "best k is -0.31, best b is 0.98, and the loss is 2.75\n",
      "best k is -0.31, best b is 0.99, and the loss is 2.74\n",
      "best k is -0.31, best b is 1.0, and the loss is 2.7300000000000004\n",
      "best k is -0.3, best b is 0.97, and the loss is 2.7299999999999995\n",
      "best k is -0.3, best b is 0.98, and the loss is 2.7199999999999998\n",
      "best k is -0.3, best b is 0.99, and the loss is 2.71\n",
      "best k is -0.3, best b is 1.0, and the loss is 2.7\n",
      "best k is -0.29, best b is 0.98, and the loss is 2.69\n",
      "best k is -0.29, best b is 0.99, and the loss is 2.6799999999999997\n",
      "best k is -0.29, best b is 1.0, and the loss is 2.6700000000000004\n",
      "best k is -0.28, best b is 0.97, and the loss is 2.67\n",
      "best k is -0.28, best b is 0.98, and the loss is 2.66\n",
      "best k is -0.28, best b is 0.99, and the loss is 2.65\n",
      "best k is -0.28, best b is 1.0, and the loss is 2.6399999999999997\n",
      "best k is -0.27, best b is 0.98, and the loss is 2.63\n",
      "best k is -0.27, best b is 0.99, and the loss is 2.62\n",
      "best k is -0.27, best b is 1.0, and the loss is 2.6100000000000003\n",
      "best k is -0.26, best b is 0.98, and the loss is 2.6\n",
      "best k is -0.26, best b is 0.99, and the loss is 2.59\n",
      "best k is -0.26, best b is 1.0, and the loss is 2.58\n",
      "best k is -0.25, best b is 0.98, and the loss is 2.57\n",
      "best k is -0.25, best b is 0.99, and the loss is 2.5599999999999996\n",
      "best k is -0.25, best b is 1.0, and the loss is 2.55\n",
      "best k is -0.24, best b is 0.98, and the loss is 2.54\n",
      "best k is -0.24, best b is 0.99, and the loss is 2.5300000000000002\n",
      "best k is -0.24, best b is 1.0, and the loss is 2.52\n",
      "best k is -0.23, best b is 0.98, and the loss is 2.51\n",
      "best k is -0.23, best b is 0.99, and the loss is 2.5\n",
      "best k is -0.23, best b is 1.0, and the loss is 2.4899999999999998\n",
      "best k is -0.22, best b is 0.98, and the loss is 2.4800000000000004\n",
      "best k is -0.22, best b is 0.99, and the loss is 2.47\n",
      "best k is -0.22, best b is 1.0, and the loss is 2.46\n",
      "best k is -0.21, best b is 0.98, and the loss is 2.45\n",
      "best k is -0.21, best b is 0.99, and the loss is 2.44\n",
      "best k is -0.21, best b is 1.0, and the loss is 2.4299999999999997\n",
      "best k is -0.2, best b is 0.98, and the loss is 2.42\n",
      "best k is -0.2, best b is 0.99, and the loss is 2.41\n",
      "best k is -0.2, best b is 1.0, and the loss is 2.4\n",
      "best k is -0.19, best b is 0.98, and the loss is 2.3899999999999997\n",
      "best k is -0.19, best b is 0.99, and the loss is 2.38\n",
      "best k is -0.19, best b is 1.0, and the loss is 2.37\n",
      "best k is -0.18, best b is 0.98, and the loss is 2.3600000000000003\n",
      "best k is -0.18, best b is 0.99, and the loss is 2.35\n",
      "best k is -0.18, best b is 1.0, and the loss is 2.34\n",
      "best k is -0.17, best b is 0.98, and the loss is 2.33\n",
      "best k is -0.17, best b is 0.99, and the loss is 2.3200000000000003\n",
      "best k is -0.17, best b is 1.0, and the loss is 2.3099999999999996\n",
      "best k is -0.16, best b is 0.98, and the loss is 2.3\n",
      "best k is -0.16, best b is 0.99, and the loss is 2.29\n",
      "best k is -0.16, best b is 1.0, and the loss is 2.2800000000000002\n",
      "best k is -0.15, best b is 0.98, and the loss is 2.27\n",
      "best k is -0.15, best b is 0.99, and the loss is 2.2600000000000002\n",
      "best k is -0.15, best b is 1.0, and the loss is 2.25\n",
      "best k is -0.14, best b is 0.98, and the loss is 2.2399999999999998\n",
      "best k is -0.14, best b is 0.99, and the loss is 2.23\n",
      "best k is -0.14, best b is 1.0, and the loss is 2.22\n",
      "best k is -0.13, best b is 0.97, and the loss is 2.2199999999999998\n",
      "best k is -0.13, best b is 0.98, and the loss is 2.21\n",
      "best k is -0.13, best b is 0.99, and the loss is 2.2\n",
      "best k is -0.13, best b is 1.0, and the loss is 2.19\n",
      "best k is -0.12, best b is 0.98, and the loss is 2.1799999999999997\n",
      "best k is -0.12, best b is 0.99, and the loss is 2.1700000000000004\n",
      "best k is -0.12, best b is 1.0, and the loss is 2.16\n",
      "best k is -0.11, best b is 0.98, and the loss is 2.15\n",
      "best k is -0.11, best b is 0.99, and the loss is 2.14\n",
      "best k is -0.11, best b is 1.0, and the loss is 2.13\n",
      "best k is -0.1, best b is 0.98, and the loss is 2.12\n",
      "best k is -0.1, best b is 0.99, and the loss is 2.1100000000000003\n",
      "best k is -0.1, best b is 1.0, and the loss is 2.1\n",
      "best k is -0.09, best b is 0.98, and the loss is 2.09\n",
      "best k is -0.09, best b is 0.99, and the loss is 2.08\n",
      "best k is -0.09, best b is 1.0, and the loss is 2.07\n",
      "best k is -0.08, best b is 0.98, and the loss is 2.06\n",
      "best k is -0.08, best b is 0.99, and the loss is 2.05\n",
      "best k is -0.08, best b is 1.0, and the loss is 2.04\n",
      "best k is -0.07, best b is 0.98, and the loss is 2.03\n",
      "best k is -0.07, best b is 0.99, and the loss is 2.0200000000000005\n",
      "best k is -0.07, best b is 1.0, and the loss is 2.0100000000000002\n",
      "best k is -0.06, best b is 0.98, and the loss is 2.0\n",
      "best k is -0.06, best b is 0.99, and the loss is 1.9900000000000002\n",
      "best k is -0.06, best b is 1.0, and the loss is 1.98\n",
      "best k is -0.05, best b is 0.98, and the loss is 1.97\n",
      "best k is -0.05, best b is 0.99, and the loss is 1.9600000000000002\n",
      "best k is -0.05, best b is 1.0, and the loss is 1.95\n",
      "best k is -0.04, best b is 0.98, and the loss is 1.94\n",
      "best k is -0.04, best b is 0.99, and the loss is 1.9299999999999997\n",
      "best k is -0.04, best b is 1.0, and the loss is 1.9200000000000004\n",
      "best k is -0.03, best b is 0.97, and the loss is 1.92\n",
      "best k is -0.03, best b is 0.98, and the loss is 1.9100000000000001\n",
      "best k is -0.03, best b is 0.99, and the loss is 1.9\n",
      "best k is -0.03, best b is 1.0, and the loss is 1.89\n",
      "best k is -0.02, best b is 0.98, and the loss is 1.8800000000000001\n",
      "best k is -0.02, best b is 0.99, and the loss is 1.8700000000000003\n",
      "best k is -0.02, best b is 1.0, and the loss is 1.86\n",
      "best k is -0.01, best b is 0.98, and the loss is 1.85\n",
      "best k is -0.01, best b is 0.99, and the loss is 1.8400000000000003\n",
      "best k is -0.01, best b is 1.0, and the loss is 1.83\n",
      "best k is 0.0, best b is 0.98, and the loss is 1.8199999999999998\n",
      "best k is 0.0, best b is 0.99, and the loss is 1.8099999999999998\n",
      "best k is 0.0, best b is 1.0, and the loss is 1.8\n",
      "best k is 0.01, best b is 0.98, and the loss is 1.7899999999999998\n",
      "best k is 0.01, best b is 0.99, and the loss is 1.7799999999999998\n",
      "best k is 0.01, best b is 1.0, and the loss is 1.7740000000000002\n",
      "best k is 0.02, best b is 0.97, and the loss is 1.77\n",
      "best k is 0.02, best b is 0.98, and the loss is 1.7600000000000002\n",
      "best k is 0.02, best b is 0.99, and the loss is 1.754\n",
      "best k is 0.02, best b is 1.0, and the loss is 1.748\n",
      "best k is 0.03, best b is 0.97, and the loss is 1.7399999999999998\n",
      "best k is 0.03, best b is 0.98, and the loss is 1.734\n",
      "best k is 0.03, best b is 0.99, and the loss is 1.7280000000000002\n",
      "best k is 0.03, best b is 1.0, and the loss is 1.722\n",
      "best k is 0.04, best b is 0.96, and the loss is 1.72\n",
      "best k is 0.04, best b is 0.97, and the loss is 1.714\n",
      "best k is 0.04, best b is 0.98, and the loss is 1.7079999999999997\n",
      "best k is 0.04, best b is 0.99, and the loss is 1.702\n",
      "best k is 0.04, best b is 1.0, and the loss is 1.6960000000000002\n",
      "best k is 0.05, best b is 0.96, and the loss is 1.6939999999999997\n",
      "best k is 0.05, best b is 0.97, and the loss is 1.6880000000000002\n",
      "best k is 0.05, best b is 0.98, and the loss is 1.682\n",
      "best k is 0.05, best b is 0.99, and the loss is 1.6759999999999997\n",
      "best k is 0.05, best b is 1.0, and the loss is 1.67\n",
      "best k is 0.06, best b is 0.96, and the loss is 1.668\n",
      "best k is 0.06, best b is 0.97, and the loss is 1.6620000000000001\n",
      "best k is 0.06, best b is 0.98, and the loss is 1.6560000000000001\n",
      "best k is 0.06, best b is 0.99, and the loss is 1.65\n",
      "best k is 0.06, best b is 1.0, and the loss is 1.6439999999999997\n",
      "best k is 0.07, best b is 0.96, and the loss is 1.642\n",
      "best k is 0.07, best b is 0.97, and the loss is 1.636\n",
      "best k is 0.07, best b is 0.98, and the loss is 1.6300000000000001\n",
      "best k is 0.07, best b is 0.99, and the loss is 1.624\n",
      "best k is 0.07, best b is 1.0, and the loss is 1.6179999999999999\n",
      "best k is 0.08, best b is 0.96, and the loss is 1.616\n",
      "best k is 0.08, best b is 0.97, and the loss is 1.61\n",
      "best k is 0.08, best b is 0.98, and the loss is 1.6039999999999999\n",
      "best k is 0.08, best b is 0.99, and the loss is 1.598\n",
      "best k is 0.08, best b is 1.0, and the loss is 1.592\n",
      "best k is 0.09, best b is 0.96, and the loss is 1.59\n",
      "best k is 0.09, best b is 0.97, and the loss is 1.584\n",
      "best k is 0.09, best b is 0.98, and the loss is 1.578\n",
      "best k is 0.09, best b is 0.99, and the loss is 1.5720000000000003\n",
      "best k is 0.09, best b is 1.0, and the loss is 1.566\n",
      "best k is 0.1, best b is 0.96, and the loss is 1.564\n",
      "best k is 0.1, best b is 0.97, and the loss is 1.558\n",
      "best k is 0.1, best b is 0.98, and the loss is 1.552\n",
      "best k is 0.1, best b is 0.99, and the loss is 1.546\n",
      "best k is 0.1, best b is 1.0, and the loss is 1.54\n",
      "best k is 0.11, best b is 0.96, and the loss is 1.538\n",
      "best k is 0.11, best b is 0.97, and the loss is 1.532\n",
      "best k is 0.11, best b is 0.98, and the loss is 1.526\n",
      "best k is 0.11, best b is 0.99, and the loss is 1.52\n",
      "best k is 0.11, best b is 1.0, and the loss is 1.514\n",
      "best k is 0.12, best b is 0.96, and the loss is 1.512\n",
      "best k is 0.12, best b is 0.97, and the loss is 1.5059999999999998\n",
      "best k is 0.12, best b is 0.98, and the loss is 1.5\n",
      "best k is 0.12, best b is 0.99, and the loss is 1.494\n",
      "best k is 0.12, best b is 1.0, and the loss is 1.4880000000000002\n",
      "best k is 0.13, best b is 0.96, and the loss is 1.486\n",
      "best k is 0.13, best b is 0.97, and the loss is 1.48\n",
      "best k is 0.13, best b is 0.98, and the loss is 1.474\n",
      "best k is 0.13, best b is 0.99, and the loss is 1.468\n",
      "best k is 0.13, best b is 1.0, and the loss is 1.462\n",
      "best k is 0.14, best b is 0.96, and the loss is 1.4600000000000002\n",
      "best k is 0.14, best b is 0.97, and the loss is 1.454\n",
      "best k is 0.14, best b is 0.98, and the loss is 1.448\n",
      "best k is 0.14, best b is 0.99, and the loss is 1.4419999999999997\n",
      "best k is 0.14, best b is 1.0, and the loss is 1.436\n",
      "best k is 0.15, best b is 0.96, and the loss is 1.434\n",
      "best k is 0.15, best b is 0.97, and the loss is 1.4280000000000002\n",
      "best k is 0.15, best b is 0.98, and the loss is 1.422\n",
      "best k is 0.15, best b is 0.99, and the loss is 1.416\n",
      "best k is 0.15, best b is 1.0, and the loss is 1.41\n",
      "best k is 0.16, best b is 0.96, and the loss is 1.408\n",
      "best k is 0.16, best b is 0.97, and the loss is 1.402\n",
      "best k is 0.16, best b is 0.98, and the loss is 1.396\n",
      "best k is 0.16, best b is 0.99, and the loss is 1.3900000000000001\n",
      "best k is 0.16, best b is 1.0, and the loss is 1.384\n",
      "best k is 0.17, best b is 0.96, and the loss is 1.3820000000000001\n",
      "best k is 0.17, best b is 0.97, and the loss is 1.376\n",
      "best k is 0.17, best b is 0.98, and the loss is 1.3699999999999999\n",
      "best k is 0.17, best b is 0.99, and the loss is 1.364\n",
      "best k is 0.17, best b is 1.0, and the loss is 1.3579999999999999\n",
      "best k is 0.18, best b is 0.96, and the loss is 1.3560000000000003\n",
      "best k is 0.18, best b is 0.97, and the loss is 1.35\n",
      "best k is 0.18, best b is 0.98, and the loss is 1.3439999999999999\n",
      "best k is 0.18, best b is 0.99, and the loss is 1.338\n",
      "best k is 0.18, best b is 1.0, and the loss is 1.332\n",
      "best k is 0.19, best b is 0.96, and the loss is 1.33\n",
      "best k is 0.19, best b is 0.97, and the loss is 1.3239999999999998\n",
      "best k is 0.19, best b is 0.98, and the loss is 1.318\n",
      "best k is 0.19, best b is 0.99, and the loss is 1.312\n",
      "best k is 0.19, best b is 1.0, and the loss is 1.306\n",
      "best k is 0.2, best b is 0.96, and the loss is 1.304\n",
      "best k is 0.2, best b is 0.97, and the loss is 1.298\n",
      "best k is 0.2, best b is 0.98, and the loss is 1.2919999999999998\n",
      "best k is 0.2, best b is 0.99, and the loss is 1.286\n",
      "best k is 0.2, best b is 1.0, and the loss is 1.28\n",
      "best k is 0.21, best b is 0.96, and the loss is 1.278\n",
      "best k is 0.21, best b is 0.97, and the loss is 1.2719999999999998\n",
      "best k is 0.21, best b is 0.98, and the loss is 1.266\n",
      "best k is 0.21, best b is 0.99, and the loss is 1.26\n",
      "best k is 0.21, best b is 1.0, and the loss is 1.254\n",
      "best k is 0.22, best b is 0.96, and the loss is 1.252\n",
      "best k is 0.22, best b is 0.97, and the loss is 1.246\n",
      "best k is 0.22, best b is 0.98, and the loss is 1.2399999999999998\n",
      "best k is 0.22, best b is 0.99, and the loss is 1.234\n",
      "best k is 0.22, best b is 1.0, and the loss is 1.2280000000000002\n",
      "best k is 0.23, best b is 0.96, and the loss is 1.226\n",
      "best k is 0.23, best b is 0.97, and the loss is 1.22\n",
      "best k is 0.23, best b is 0.98, and the loss is 1.214\n",
      "best k is 0.23, best b is 0.99, and the loss is 1.2079999999999997\n",
      "best k is 0.23, best b is 1.0, and the loss is 1.202\n",
      "best k is 0.24, best b is 0.96, and the loss is 1.2\n",
      "best k is 0.24, best b is 0.97, and the loss is 1.1940000000000002\n",
      "best k is 0.24, best b is 0.98, and the loss is 1.1880000000000002\n",
      "best k is 0.24, best b is 0.99, and the loss is 1.182\n",
      "best k is 0.24, best b is 1.0, and the loss is 1.176\n",
      "best k is 0.25, best b is 0.96, and the loss is 1.174\n",
      "best k is 0.25, best b is 0.97, and the loss is 1.1680000000000001\n",
      "best k is 0.25, best b is 0.98, and the loss is 1.1620000000000001\n",
      "best k is 0.25, best b is 0.99, and the loss is 1.156\n",
      "best k is 0.25, best b is 1.0, and the loss is 1.15\n",
      "best k is 0.26, best b is 0.96, and the loss is 1.1480000000000001\n",
      "best k is 0.26, best b is 0.97, and the loss is 1.1420000000000001\n",
      "best k is 0.26, best b is 0.98, and the loss is 1.136\n",
      "best k is 0.26, best b is 0.99, and the loss is 1.13\n",
      "best k is 0.26, best b is 1.0, and the loss is 1.124\n",
      "best k is 0.27, best b is 0.96, and the loss is 1.1219999999999999\n",
      "best k is 0.27, best b is 0.97, and the loss is 1.116\n",
      "best k is 0.27, best b is 0.98, and the loss is 1.1099999999999999\n",
      "best k is 0.27, best b is 0.99, and the loss is 1.1039999999999999\n",
      "best k is 0.27, best b is 1.0, and the loss is 1.098\n",
      "best k is 0.28, best b is 0.96, and the loss is 1.0959999999999999\n",
      "best k is 0.28, best b is 0.97, and the loss is 1.09\n",
      "best k is 0.28, best b is 0.98, and the loss is 1.084\n",
      "best k is 0.28, best b is 0.99, and the loss is 1.0779999999999998\n",
      "best k is 0.28, best b is 1.0, and the loss is 1.0719999999999998\n",
      "best k is 0.29, best b is 0.96, and the loss is 1.0699999999999998\n",
      "best k is 0.29, best b is 0.97, and the loss is 1.064\n",
      "best k is 0.29, best b is 0.98, and the loss is 1.0580000000000003\n",
      "best k is 0.29, best b is 0.99, and the loss is 1.052\n",
      "best k is 0.29, best b is 1.0, and the loss is 1.0459999999999998\n",
      "best k is 0.3, best b is 0.96, and the loss is 1.044\n",
      "best k is 0.3, best b is 0.97, and the loss is 1.038\n",
      "best k is 0.3, best b is 0.98, and the loss is 1.032\n",
      "best k is 0.3, best b is 0.99, and the loss is 1.026\n",
      "best k is 0.3, best b is 1.0, and the loss is 1.02\n",
      "best k is 0.31, best b is 0.96, and the loss is 1.018\n",
      "best k is 0.31, best b is 0.97, and the loss is 1.012\n",
      "best k is 0.31, best b is 0.98, and the loss is 1.006\n",
      "best k is 0.31, best b is 0.99, and the loss is 1.0\n",
      "best k is 0.31, best b is 1.0, and the loss is 0.994\n",
      "best k is 0.32, best b is 0.96, and the loss is 0.9919999999999998\n",
      "best k is 0.32, best b is 0.97, and the loss is 0.986\n",
      "best k is 0.32, best b is 0.98, and the loss is 0.9800000000000001\n",
      "best k is 0.32, best b is 0.99, and the loss is 0.974\n",
      "best k is 0.32, best b is 1.0, and the loss is 0.968\n",
      "best k is 0.33, best b is 0.96, and the loss is 0.9659999999999999\n",
      "best k is 0.33, best b is 0.97, and the loss is 0.96\n",
      "best k is 0.33, best b is 0.98, and the loss is 0.9540000000000001\n",
      "best k is 0.33, best b is 0.99, and the loss is 0.9480000000000001\n",
      "best k is 0.33, best b is 1.0, and the loss is 0.9419999999999998\n",
      "best k is 0.34, best b is 0.96, and the loss is 0.9399999999999998\n",
      "best k is 0.34, best b is 0.97, and the loss is 0.9339999999999999\n",
      "best k is 0.34, best b is 0.98, and the loss is 0.9280000000000002\n",
      "best k is 0.34, best b is 0.99, and the loss is 0.9259999999999998\n",
      "best k is 0.34, best b is 1.0, and the loss is 0.9239999999999998\n",
      "best k is 0.35000000000000003, best b is 0.9500000000000001, and the loss is 0.9199999999999999\n",
      "best k is 0.35000000000000003, best b is 0.96, and the loss is 0.9179999999999999\n",
      "best k is 0.35000000000000003, best b is 0.97, and the loss is 0.916\n",
      "best k is 0.35000000000000003, best b is 0.98, and the loss is 0.914\n",
      "best k is 0.35000000000000003, best b is 0.99, and the loss is 0.9119999999999999\n",
      "best k is 0.35000000000000003, best b is 1.0, and the loss is 0.9099999999999998\n",
      "best k is 0.36, best b is 0.9400000000000001, and the loss is 0.9080000000000001\n",
      "best k is 0.36, best b is 0.9500000000000001, and the loss is 0.906\n",
      "best k is 0.36, best b is 0.96, and the loss is 0.9040000000000001\n",
      "best k is 0.36, best b is 0.97, and the loss is 0.9019999999999999\n",
      "best k is 0.36, best b is 0.98, and the loss is 0.9\n",
      "best k is 0.36, best b is 0.99, and the loss is 0.898\n",
      "best k is 0.36, best b is 1.0, and the loss is 0.8960000000000001\n",
      "best k is 0.37, best b is 0.93, and the loss is 0.8959999999999999\n",
      "best k is 0.37, best b is 0.9400000000000001, and the loss is 0.8939999999999999\n",
      "best k is 0.37, best b is 0.9500000000000001, and the loss is 0.8919999999999998\n",
      "best k is 0.37, best b is 0.96, and the loss is 0.8899999999999999\n",
      "best k is 0.37, best b is 0.97, and the loss is 0.8879999999999999\n",
      "best k is 0.37, best b is 0.98, and the loss is 0.8859999999999999\n",
      "best k is 0.37, best b is 0.99, and the loss is 0.884\n",
      "best k is 0.37, best b is 1.0, and the loss is 0.882\n",
      "best k is 0.38, best b is 0.9400000000000001, and the loss is 0.8800000000000001\n",
      "best k is 0.38, best b is 0.9500000000000001, and the loss is 0.8780000000000001\n",
      "best k is 0.38, best b is 0.96, and the loss is 0.8760000000000001\n",
      "best k is 0.38, best b is 0.97, and the loss is 0.874\n",
      "best k is 0.38, best b is 0.98, and the loss is 0.8720000000000001\n",
      "best k is 0.38, best b is 0.99, and the loss is 0.8700000000000001\n",
      "best k is 0.38, best b is 1.0, and the loss is 0.868\n",
      "best k is 0.39, best b is 0.9400000000000001, and the loss is 0.866\n",
      "best k is 0.39, best b is 0.9500000000000001, and the loss is 0.8639999999999999\n",
      "best k is 0.39, best b is 0.96, and the loss is 0.8619999999999999\n",
      "best k is 0.39, best b is 0.97, and the loss is 0.8599999999999998\n",
      "best k is 0.39, best b is 0.98, and the loss is 0.8579999999999999\n",
      "best k is 0.39, best b is 0.99, and the loss is 0.8559999999999999\n",
      "best k is 0.39, best b is 1.0, and the loss is 0.8539999999999999\n",
      "best k is 0.4, best b is 0.9400000000000001, and the loss is 0.852\n",
      "best k is 0.4, best b is 0.9500000000000001, and the loss is 0.85\n",
      "best k is 0.4, best b is 0.96, and the loss is 0.8480000000000001\n",
      "best k is 0.4, best b is 0.97, and the loss is 0.8460000000000001\n",
      "best k is 0.4, best b is 0.98, and the loss is 0.8440000000000001\n",
      "best k is 0.4, best b is 0.99, and the loss is 0.8420000000000002\n",
      "best k is 0.4, best b is 1.0, and the loss is 0.8400000000000001\n",
      "best k is 0.41000000000000003, best b is 0.93, and the loss is 0.8399999999999999\n",
      "best k is 0.41000000000000003, best b is 0.9400000000000001, and the loss is 0.8379999999999999\n",
      "best k is 0.41000000000000003, best b is 0.9500000000000001, and the loss is 0.836\n",
      "best k is 0.41000000000000003, best b is 0.96, and the loss is 0.834\n",
      "best k is 0.41000000000000003, best b is 0.97, and the loss is 0.8319999999999999\n",
      "best k is 0.41000000000000003, best b is 0.98, and the loss is 0.8299999999999998\n",
      "best k is 0.41000000000000003, best b is 0.99, and the loss is 0.828\n",
      "best k is 0.41000000000000003, best b is 1.0, and the loss is 0.8259999999999998\n",
      "best k is 0.42, best b is 0.9400000000000001, and the loss is 0.8240000000000001\n",
      "best k is 0.42, best b is 0.9500000000000001, and the loss is 0.8219999999999998\n",
      "best k is 0.42, best b is 0.96, and the loss is 0.82\n",
      "best k is 0.42, best b is 0.97, and the loss is 0.818\n",
      "best k is 0.42, best b is 0.98, and the loss is 0.8160000000000001\n",
      "best k is 0.42, best b is 0.99, and the loss is 0.8140000000000001\n",
      "best k is 0.42, best b is 1.0, and the loss is 0.812\n",
      "best k is 0.43, best b is 0.9400000000000001, and the loss is 0.8099999999999999\n",
      "best k is 0.43, best b is 0.9500000000000001, and the loss is 0.8080000000000002\n",
      "best k is 0.43, best b is 0.96, and the loss is 0.8060000000000003\n",
      "best k is 0.43, best b is 0.97, and the loss is 0.8039999999999999\n",
      "best k is 0.43, best b is 0.98, and the loss is 0.8019999999999999\n",
      "best k is 0.43, best b is 0.99, and the loss is 0.8\n",
      "best k is 0.43, best b is 1.0, and the loss is 0.7980000000000002\n",
      "best k is 0.44, best b is 0.93, and the loss is 0.7979999999999999\n",
      "best k is 0.44, best b is 0.9400000000000001, and the loss is 0.796\n",
      "best k is 0.44, best b is 0.9500000000000001, and the loss is 0.7939999999999999\n",
      "best k is 0.44, best b is 0.96, and the loss is 0.792\n",
      "best k is 0.44, best b is 0.97, and the loss is 0.79\n",
      "best k is 0.44, best b is 0.98, and the loss is 0.7879999999999999\n",
      "best k is 0.44, best b is 0.99, and the loss is 0.7859999999999999\n",
      "best k is 0.44, best b is 1.0, and the loss is 0.784\n",
      "best k is 0.45, best b is 0.9400000000000001, and the loss is 0.7819999999999999\n",
      "best k is 0.45, best b is 0.9500000000000001, and the loss is 0.78\n",
      "best k is 0.45, best b is 0.96, and the loss is 0.7780000000000001\n",
      "best k is 0.45, best b is 0.97, and the loss is 0.776\n",
      "best k is 0.45, best b is 0.98, and the loss is 0.7739999999999999\n",
      "best k is 0.45, best b is 0.99, and the loss is 0.7719999999999999\n",
      "best k is 0.45, best b is 1.0, and the loss is 0.7700000000000001\n",
      "best k is 0.46, best b is 0.93, and the loss is 0.7699999999999999\n",
      "best k is 0.46, best b is 0.9400000000000001, and the loss is 0.768\n",
      "best k is 0.46, best b is 0.9500000000000001, and the loss is 0.7659999999999999\n",
      "best k is 0.46, best b is 0.96, and the loss is 0.764\n",
      "best k is 0.46, best b is 0.97, and the loss is 0.7619999999999999\n",
      "best k is 0.46, best b is 0.98, and the loss is 0.76\n",
      "best k is 0.46, best b is 0.99, and the loss is 0.758\n",
      "best k is 0.46, best b is 1.0, and the loss is 0.756\n",
      "best k is 0.47000000000000003, best b is 0.9400000000000001, and the loss is 0.7539999999999999\n",
      "best k is 0.47000000000000003, best b is 0.9500000000000001, and the loss is 0.752\n",
      "best k is 0.47000000000000003, best b is 0.96, and the loss is 0.7500000000000001\n",
      "best k is 0.47000000000000003, best b is 0.97, and the loss is 0.7479999999999999\n",
      "best k is 0.47000000000000003, best b is 0.98, and the loss is 0.7459999999999999\n",
      "best k is 0.47000000000000003, best b is 0.99, and the loss is 0.744\n",
      "best k is 0.47000000000000003, best b is 1.0, and the loss is 0.7420000000000001\n",
      "best k is 0.48, best b is 0.93, and the loss is 0.742\n",
      "best k is 0.48, best b is 0.9400000000000001, and the loss is 0.74\n",
      "best k is 0.48, best b is 0.9500000000000001, and the loss is 0.738\n",
      "best k is 0.48, best b is 0.96, and the loss is 0.736\n",
      "best k is 0.48, best b is 0.97, and the loss is 0.7340000000000001\n",
      "best k is 0.48, best b is 0.98, and the loss is 0.732\n",
      "best k is 0.48, best b is 0.99, and the loss is 0.73\n",
      "best k is 0.48, best b is 1.0, and the loss is 0.728\n",
      "best k is 0.49, best b is 0.93, and the loss is 0.7279999999999999\n",
      "best k is 0.49, best b is 0.9400000000000001, and the loss is 0.7260000000000001\n",
      "best k is 0.49, best b is 0.9500000000000001, and the loss is 0.7239999999999999\n",
      "best k is 0.49, best b is 0.96, and the loss is 0.7219999999999999\n",
      "best k is 0.49, best b is 0.97, and the loss is 0.7200000000000001\n",
      "best k is 0.49, best b is 0.98, and the loss is 0.7180000000000001\n",
      "best k is 0.49, best b is 0.99, and the loss is 0.7159999999999999\n",
      "best k is 0.49, best b is 1.0, and the loss is 0.7139999999999999\n",
      "best k is 0.5, best b is 0.9400000000000001, and the loss is 0.712\n",
      "best k is 0.5, best b is 0.9500000000000001, and the loss is 0.71\n",
      "best k is 0.5, best b is 0.96, and the loss is 0.708\n",
      "best k is 0.5, best b is 0.97, and the loss is 0.7060000000000001\n",
      "best k is 0.5, best b is 0.98, and the loss is 0.704\n",
      "best k is 0.5, best b is 0.99, and the loss is 0.702\n",
      "best k is 0.5, best b is 1.0, and the loss is 0.7\n",
      "best k is 0.51, best b is 0.9400000000000001, and the loss is 0.6980000000000002\n",
      "best k is 0.51, best b is 0.9500000000000001, and the loss is 0.696\n",
      "best k is 0.51, best b is 0.96, and the loss is 0.6940000000000002\n",
      "best k is 0.52, best b is 0.89, and the loss is 0.694\n",
      "best k is 0.52, best b is 0.9, and the loss is 0.692\n",
      "best k is 0.52, best b is 0.91, and the loss is 0.69\n",
      "best k is 0.52, best b is 0.92, and the loss is 0.688\n",
      "best k is 0.53, best b is 0.85, and the loss is 0.6879999999999998\n",
      "best k is 0.53, best b is 0.86, and the loss is 0.686\n",
      "best k is 0.53, best b is 0.87, and the loss is 0.6839999999999998\n",
      "best k is 0.53, best b is 0.88, and the loss is 0.682\n",
      "best k is 0.54, best b is 0.81, and the loss is 0.6819999999999999\n",
      "best k is 0.54, best b is 0.8200000000000001, and the loss is 0.6799999999999999\n",
      "best k is 0.54, best b is 0.8300000000000001, and the loss is 0.6779999999999999\n",
      "best k is 0.54, best b is 0.84, and the loss is 0.6759999999999999\n",
      "best k is 0.55, best b is 0.78, and the loss is 0.6739999999999998\n",
      "best k is 0.55, best b is 0.79, and the loss is 0.672\n",
      "best k is 0.55, best b is 0.8, and the loss is 0.6700000000000002\n",
      "best k is 0.56, best b is 0.73, and the loss is 0.6699999999999999\n",
      "best k is 0.56, best b is 0.74, and the loss is 0.6679999999999999\n",
      "best k is 0.56, best b is 0.75, and the loss is 0.6659999999999999\n",
      "best k is 0.56, best b is 0.76, and the loss is 0.6639999999999999\n",
      "best k is 0.5700000000000001, best b is 0.7000000000000001, and the loss is 0.6619999999999997\n",
      "best k is 0.5700000000000001, best b is 0.71, and the loss is 0.6599999999999998\n",
      "best k is 0.5700000000000001, best b is 0.72, and the loss is 0.658\n",
      "best k is 0.58, best b is 0.66, and the loss is 0.656\n",
      "best k is 0.58, best b is 0.67, and the loss is 0.654\n",
      "best k is 0.58, best b is 0.68, and the loss is 0.6519999999999999\n",
      "best k is 0.59, best b is 0.62, and the loss is 0.6500000000000001\n",
      "best k is 0.59, best b is 0.63, and the loss is 0.648\n",
      "best k is 0.59, best b is 0.64, and the loss is 0.6460000000000001\n",
      "best k is 0.6, best b is 0.5700000000000001, and the loss is 0.646\n",
      "best k is 0.6, best b is 0.58, and the loss is 0.6439999999999999\n",
      "best k is 0.6, best b is 0.59, and the loss is 0.642\n",
      "best k is 0.6, best b is 0.6, and the loss is 0.64\n",
      "best k is 0.61, best b is 0.54, and the loss is 0.6380000000000001\n",
      "best k is 0.61, best b is 0.55, and the loss is 0.636\n",
      "best k is 0.61, best b is 0.56, and the loss is 0.634\n",
      "best k is 0.62, best b is 0.5, and the loss is 0.632\n",
      "best k is 0.62, best b is 0.51, and the loss is 0.6299999999999999\n",
      "best k is 0.62, best b is 0.52, and the loss is 0.6279999999999999\n",
      "best k is 0.63, best b is 0.46, and the loss is 0.6260000000000001\n",
      "best k is 0.63, best b is 0.47000000000000003, and the loss is 0.624\n",
      "best k is 0.63, best b is 0.48, and the loss is 0.6220000000000001\n",
      "best k is 0.64, best b is 0.41000000000000003, and the loss is 0.6219999999999999\n",
      "best k is 0.64, best b is 0.42, and the loss is 0.6199999999999999\n",
      "best k is 0.64, best b is 0.43, and the loss is 0.618\n",
      "best k is 0.64, best b is 0.44, and the loss is 0.616\n",
      "best k is 0.65, best b is 0.38, and the loss is 0.6140000000000001\n",
      "best k is 0.65, best b is 0.39, and the loss is 0.612\n",
      "best k is 0.65, best b is 0.4, and the loss is 0.61\n",
      "best k is 0.66, best b is 0.34, and the loss is 0.6079999999999999\n",
      "best k is 0.66, best b is 0.35000000000000003, and the loss is 0.6059999999999999\n",
      "best k is 0.66, best b is 0.36, and the loss is 0.6039999999999999\n",
      "best k is 0.67, best b is 0.32, and the loss is 0.602\n",
      "best k is 0.67, best b is 0.33, and the loss is 0.6\n",
      "best k is 0.68, best b is 0.32, and the loss is 0.5999999999999999\n"
     ]
    }
   ],
   "source": [
    "min_loss = float(\"inf\") # 正无穷或负无穷，使用float(\"inf\")或float(\"-inf\")来表示\n",
    "best_k,best_b = None,None  # 空值\n",
    "\n",
    "for k in range(-100, 101):\n",
    "    k *= 0.01\n",
    "    for b in range(-100,101):\n",
    "        b *= 0.01\n",
    "        y_hat = [function(x_i,k,b) for x_i in list(x)]\n",
    "        #print(y_hat)\n",
    "        current_loss = loss(y,y_hat)\n",
    "        #print(current_loss)\n",
    "        if current_loss < min_loss:\n",
    "            min_loss = current_loss\n",
    "            best_k,best_b = k,b\n",
    "            print(\"best k is {}, best b is {}, and the loss is {}\".format(best_k, best_b, current_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# best k is 0.68, best b is 0.32, and the loss is 0.5999999999999999\n",
    "k,b = 0.68,0.32\n",
    "y_hat = k*x + b\n",
    "plt.plot(x,y_hat,color='red') # 绘制 x 与 y，color='red'设置颜色\n",
    "plt.scatter(x,y)\n",
    "plt.grid() # 显示网格"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 方法2，如何找到改变的方向呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "direction  = [\n",
    "    (+1, -1), # 第一个是k的方向，第二个b的方向，+1表示增大，-1表示减小\n",
    "    (+1, +1),\n",
    "    (-1, -1),\n",
    "    (-1, +1),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200412140525549.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"800\" height=\"800\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 梯度下降(Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis (拟合的直线) ： $ \\hat y = k\\cdot x + b $    \n",
    "\n",
    "Parameters (参数) ： $k,b$   \n",
    "\n",
    "Cost Func (损失函数，loss选择MSE) ：  $ Cost(k,b) =  \\frac {1}{2m}\\sum_{i=1}^m ( \\hat {y^i} - y^i )^2 $   \n",
    "\n",
    "Goal ：$\\min_{k,b}Cost(k,b)$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 首先对 $k$、$b$ 赋值，这个值可以是随机的，也可以是一个零向量；  \n",
    "- 改变 $k$、$b$ 的值，使得 $ Cost(k,b) $ 按梯度下降的方向进行减少；  \n",
    "- 当 $ Cost(k,b) $  下降到无法下降时为止.\n",
    "\n",
    "$$ temp0 = k - \\lambda \\frac{\\partial }{\\partial k} Cost(k,b) $$\n",
    "\n",
    "$$ temp1 = b - \\lambda \\frac{\\partial }{\\partial b} Cost(k,b) $$\n",
    "\n",
    "$$ k := temp0 $$\n",
    "\n",
    "$$ b := temp1 $$\n",
    "\n",
    "$ \\lambda $ 是步长，也被成为学习率，凭经验值。  \n",
    "\n",
    "$ \\frac{\\partial }{\\partial k} Cost(k,b) ,\\frac{\\partial }{\\partial b} Cost(k,b)$ 是多少呢？\n",
    "\n",
    "$$ \\frac{\\partial }{\\partial k} Cost(k,b) = \\frac {1}{m}\\sum_{i=1}^m (\\hat {y^i} - y^i ) x $$ \n",
    "\n",
    "$$ \\frac{\\partial }{\\partial b} Cost(k,b) = \\frac {1}{m}\\sum_{i=1}^m (\\hat {y^i} - y^i) $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    return sum((y_hat_i - y_i)**2 for y_hat_i, y_i in zip(list(y_hat),list(y))) / len(list(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_k(x,y,y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for x_i,y_i,y_hat_i in zip(list(x),list(y),list(y_hat)):\n",
    "        gradient += (y_hat_i - y_i) * x_i\n",
    "    return gradient / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_b(y,y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for y_i, y_hat_i in zip(list(y),list(y_hat)):\n",
    "        gradient += (y_hat_i - y_i) \n",
    "    return gradient / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_k = current_k - learn_rate * k_gradient\n",
    "current_b = current_b - learn_rate * b_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best k is 10, best b is 10, and the loss is 1553.6\n",
      "best k is -3.0, best b is 6.279999999999999, and the loss is 1553.6\n",
      "best k is -3.0, best b is 6.279999999999999, and the loss is 59.83040000000001\n",
      "best k is -0.5839999999999996, best b is 6.831999999999999, and the loss is 59.83040000000001\n",
      "best k is -0.5839999999999996, best b is 6.831999999999999, and the loss is 9.509311999999998\n",
      "best k is -0.9911999999999997, best b is 6.603999999999999, and the loss is 9.509311999999998\n",
      "best k is -0.9911999999999997, best b is 6.603999999999999, and the loss is 7.586359039999998\n",
      "best k is -0.8820799999999998, best b is 6.520959999999999, and the loss is 7.586359039999998\n",
      "best k is -0.8820799999999998, best b is 6.520959999999999, and the loss is 7.293809331199997\n",
      "best k is -0.8680799999999996, best b is 6.413487999999999, and the loss is 7.293809331199997\n",
      "best k is -0.8680799999999996, best b is 6.413487999999999, and the loss is 7.063563298303999\n",
      "best k is -0.8372383999999998, best b is 6.3125632, and the loss is 7.063563298303999\n",
      "best k is -0.8372383999999998, best b is 6.3125632, and the loss is 6.842795875973121\n",
      "best k is -0.8100451199999998, best b is 6.212478399999999, and the loss is 6.842795875973121\n",
      "best k is -0.8100451199999998, best b is 6.212478399999999, and the loss is 6.629488425108069\n",
      "best k is -0.7827390079999998, best b is 6.114244095999999, and the loss is 6.629488425108069\n",
      "best k is -0.7827390079999998, best b is 6.114244095999999, and the loss is 6.42333383872654\n",
      "best k is -0.7559993279999998, best b is 6.0176413887999995, and the loss is 6.42333383872654\n",
      "best k is -0.7559993279999998, best b is 6.0176413887999995, and the loss is 6.224090413753039\n",
      "best k is -0.7296924838399999, best b is 5.92267704832, and the loss is 6.224090413753039\n",
      "best k is -0.7296924838399999, best b is 5.92267704832, and the loss is 6.0315263973782995\n",
      "best k is -0.7038338661119999, best b is 5.82931708864, and the loss is 6.0315263973782995\n",
      "best k is -0.7038338661119999, best b is 5.82931708864, and the loss is 5.845417867972701\n",
      "best k is -0.6784117399808, best b is 5.7375355396096, and the loss is 5.845417867972701\n",
      "best k is -0.6784117399808, best b is 5.7375355396096, and the loss is 5.665548412697644\n",
      "best k is -0.6534194878848, best b is 5.64730550764288, and the loss is 5.665548412697644\n",
      "best k is -0.6534194878848, best b is 5.64730550764288, and the loss is 5.4917088737753295\n",
      "best k is -0.628849703504384, best b is 5.558600803244032, and the loss is 5.4917088737753295\n",
      "best k is -0.628849703504384, best b is 5.558600803244032, and the loss is 5.323697105203925\n",
      "best k is -0.6046952706227714, best b is 5.471395633970944, and the loss is 5.323697105203925\n",
      "best k is -0.6046952706227714, best b is 5.471395633970944, and the loss is 5.161317737693605\n",
      "best k is -0.5809491631290061, best b is 5.385664651760681, and the loss is 5.161317737693605\n",
      "best k is -0.5809491631290061, best b is 5.385664651760681, and the loss is 5.004381951484998\n",
      "best k is -0.5576044792153038, best b is 5.301382935523315, and the loss is 5.004381951484998\n",
      "best k is -0.5576044792153038, best b is 5.301382935523315, and the loss is 4.8527072567837255\n",
      "best k is -0.5346544327354641, best b is 5.218525985735575, and the loss is 4.8527072567837255\n",
      "best k is -0.5346544327354641, best b is 5.218525985735575, and the loss is 4.706117281555612\n",
      "best k is -0.5120923524471259, best b is 5.137069716982657, and the loss is 4.706117281555612\n",
      "best k is -0.5120923524471259, best b is 5.137069716982657, and the loss is 4.5644415664358196\n",
      "best k is -0.48991167985008444, best b is 5.056990451018529, and the loss is 4.5644415664358196\n",
      "best k is -0.48991167985008444, best b is 5.056990451018529, and the loss is 4.427515366513418\n",
      "best k is -0.4681059673205503, best b is 4.978264909871702, and the loss is 4.427515366513418\n",
      "best k is -0.4681059673205503, best b is 4.978264909871702, and the loss is 4.295179459760897\n",
      "best k is -0.4466688762294555, best b is 4.900870209080697, and the loss is 4.295179459760897\n",
      "best k is -0.4466688762294555, best b is 4.900870209080697, and the loss is 4.167279961885862\n",
      "best k is -0.4255941751012635, best b is 4.824783851041464, and the loss is 4.167279961885862\n",
      "best k is -0.4255941751012635, best b is 4.824783851041464, and the loss is 4.04366814738961\n",
      "best k is -0.40487573780231284, best b is 4.749983718467696, and the loss is 4.04366814738961\n",
      "best k is -0.40487573780231284, best b is 4.749983718467696, and the loss is 3.924200276624507\n",
      "best k is -0.38450754176007756, best b is 4.676448067961621, and the loss is 3.924200276624507\n",
      "best k is -0.38450754176007756, best b is 4.676448067961621, and the loss is 3.808737428649085\n",
      "best k is -0.3644836662124785, best b is 4.604155523693482, and the loss is 3.808737428649085\n",
      "best k is -0.3644836662124785, best b is 4.604155523693482, and the loss is 3.6971453396864504\n",
      "best k is -0.3447982904867967, best b is 4.533085071187878, and the loss is 3.6971453396864504\n",
      "best k is -0.3447982904867967, best b is 4.533085071187878, and the loss is 3.5892942469982216\n",
      "best k is -0.3254456923076836, best b is 4.463216051215129, and the loss is 3.5892942469982216\n",
      "best k is -0.3254456923076836, best b is 4.463216051215129, and the loss is 3.4850587379923796\n",
      "best k is -0.3064202461337704, best b is 4.394528153785921, and the loss is 3.4850587379923796\n",
      "best k is -0.3064202461337704, best b is 4.394528153785921, and the loss is 3.3843176043896377\n",
      "best k is -0.2877164215223993, best b is 4.32700141224746, and the loss is 3.3843176043896377\n",
      "best k is -0.2877164215223993, best b is 4.32700141224746, and the loss is 3.286953701278685\n",
      "best k is -0.2693287815219982, best b is 4.260616197479434, and the loss is 3.286953701278685\n",
      "best k is -0.2693287815219982, best b is 4.260616197479434, and the loss is 3.192853810896461\n",
      "best k is -0.2512519810916305, best b is 4.19535321218809, and the loss is 3.192853810896461\n",
      "best k is -0.2512519810916305, best b is 4.19535321218809, and the loss is 3.1019085109750306\n",
      "best k is -0.23348076554726396, best b is 4.13119348529677, and the loss is 3.1019085109750306\n",
      "best k is -0.23348076554726396, best b is 4.13119348529677, and the loss is 3.0140120475019856\n",
      "best k is -0.21600996903430478, best b is 4.068118366431273, and the loss is 3.0140120475019856\n",
      "best k is -0.21600996903430478, best b is 4.068118366431273, and the loss is 2.929062211746395\n",
      "best k is -0.19883451302595145, best b is 4.006109520498437, and the loss is 2.929062211746395\n",
      "best k is -0.19883451302595145, best b is 4.006109520498437, and the loss is 2.8469602214073304\n",
      "best k is -0.18194940484693586, best b is 3.9451489223563785, and the loss is 2.8469602214073304\n",
      "best k is -0.18194940484693586, best b is 3.9451489223563785, and the loss is 2.7676106057467473\n",
      "best k is -0.1653497362222199, best b is 3.8852188515748214, and the loss is 2.7676106057467473\n",
      "best k is -0.1653497362222199, best b is 3.8852188515748214, and the loss is 2.6909210945731497\n",
      "best k is -0.14903068185022442, best b is 3.8263018872840053, and the loss is 2.6909210945731497\n",
      "best k is -0.14903068185022442, best b is 3.8263018872840053, and the loss is 2.6168025109469553\n",
      "best k is -0.13298749800017917, best b is 3.768380903110672, and the loss is 2.6168025109469553\n",
      "best k is -0.13298749800017917, best b is 3.768380903110672, and the loss is 2.545168667482785\n",
      "best k is -0.11721552113318368, best b is 3.7114390621996587, and the loss is 2.545168667482785\n",
      "best k is -0.11721552113318368, best b is 3.7114390621996587, and the loss is 2.475936266128104\n",
      "best k is -0.10171016654657923, best b is 3.655459812319648, and the loss is 2.475936266128104\n",
      "best k is -0.10171016654657923, best b is 3.655459812319648, and the loss is 2.4090248013016615\n",
      "best k is -0.08646692704123646, best b is 3.600426881051657, and the loss is 2.4090248013016615\n",
      "best k is -0.08646692704123646, best b is 3.600426881051657, and the loss is 2.3443564662791117\n",
      "best k is -0.0714813716113735, best b is 3.5463242710588623, and the loss is 2.3443564662791117\n",
      "best k is -0.0714813716113735, best b is 3.5463242710588623, and the loss is 2.2818560627169373\n",
      "best k is -0.05674914415652137, best b is 3.4931362554363883, and the loss is 2.2818560627169373\n",
      "best k is -0.05674914415652137, best b is 3.4931362554363883, and the loss is 2.2214509132094946\n",
      "best k is -0.042265962215264394, best b is 3.440847373139706, and the loss is 2.2214509132094946\n",
      "best k is -0.042265962215264394, best b is 3.440847373139706, and the loss is 2.163070776777466\n",
      "best k is -0.028027615720385347, best b is 3.3894424244903147, and the loss is 2.163070776777466\n",
      "best k is -0.028027615720385347, best b is 3.3894424244903147, and the loss is 2.106647767189478\n",
      "best k is -0.014029965775055903, best b is 3.3389064667573987, and the loss is 2.106647767189478\n",
      "best k is -0.014029965775055903, best b is 3.3389064667573987, and the loss is 2.0521162740218886\n",
      "best k is -0.0002689434497140323, best b is 3.2892248098141756, and the loss is 2.0521162740218886\n",
      "best k is -0.0002689434497140323, best b is 3.2892248098141756, and the loss is 1.9994128863649494\n",
      "best k is 0.013259451400718764, best b is 3.240383011867672, and the loss is 1.9994128863649494\n",
      "best k is 0.013259451400718764, best b is 3.240383011867672, and the loss is 1.9484763190866397\n",
      "best k is 0.02655915129962647, best b is 3.1923668752606895, and the loss is 1.9484763190866397\n",
      "best k is 0.02655915129962647, best b is 3.1923668752606895, and the loss is 1.8992473415684157\n",
      "best k is 0.039634022291830495, best b is 3.1451624423447324, and the loss is 1.8992473415684157\n",
      "best k is 0.039634022291830495, best b is 3.1451624423447324, and the loss is 1.8516687088300088\n",
      "best k is 0.05248786506739721, best b is 3.09875599142271, and the loss is 1.8516687088300088\n",
      "best k is 0.05248786506739721, best b is 3.09875599142271, and the loss is 1.805685094963191\n",
      "best k is 0.06512441606644734, best b is 3.05313403276022, and the loss is 1.805685094963191\n",
      "best k is 0.06512441606644734, best b is 3.05313403276022, and the loss is 1.7612430287970995\n",
      "best k is 0.07754734856528929, best b is 3.0082833046642636, and the loss is 1.7612430287970995\n",
      "best k is 0.07754734856528929, best b is 3.0082833046642636, and the loss is 1.7182908317202916\n",
      "best k is 0.08976027374419199, best b is 2.9641907696282503, and the loss is 1.7182908317202916\n",
      "best k is 0.08976027374419199, best b is 2.9641907696282503, and the loss is 1.6767785575872658\n",
      "best k is 0.10176674173710569, best b is 2.9208436105421676, and the loss is 1.6767785575872658\n",
      "best k is 0.10176674173710569, best b is 2.9208436105421676, and the loss is 1.6366579346395276\n",
      "best k is 0.11357024266363915, best b is 2.8782292269668193, and the loss is 1.6366579346395276\n",
      "best k is 0.11357024266363915, best b is 2.8782292269668193, and the loss is 1.597882309373701\n",
      "best k is 0.12517420764359033, best b is 2.8363352314710455, and the loss is 1.597882309373701\n",
      "best k is 0.12517420764359033, best b is 2.8363352314710455, and the loss is 1.5604065922913803\n",
      "best k is 0.13658200979432733, best b is 2.7951494460308637, and the loss is 1.5604065922913803\n",
      "best k is 0.13658200979432733, best b is 2.7951494460308637, and the loss is 1.5241872054676722\n",
      "best k is 0.14779696521130817, best b is 2.754659898489479, and the loss is 1.5241872054676722\n",
      "best k is 0.14779696521130817, best b is 2.754659898489479, and the loss is 1.4891820318774314\n",
      "best k is 0.15882233393202547, best b is 2.7148548190771384, and the loss is 1.4891820318774314\n",
      "best k is 0.15882233393202547, best b is 2.7148548190771384, and the loss is 1.4553503664202787\n",
      "best k is 0.16966132088365593, best b is 2.675722636989817, and the loss is 1.4553503664202787\n",
      "best k is 0.16966132088365593, best b is 2.675722636989817, and the loss is 1.4226528685874495\n",
      "best k is 0.18031707681468934, best b is 2.6372519770257385, and the loss is 1.4226528685874495\n",
      "best k is 0.18031707681468934, best b is 2.6372519770257385, and the loss is 1.391051516715433\n",
      "best k is 0.1907926992108095, best b is 2.599431656278758, and the loss is 1.391051516715433\n",
      "best k is 0.1907926992108095, best b is 2.599431656278758, and the loss is 1.3605095637731992\n",
      "best k is 0.20109123319529162, best b is 2.5622506808876393, and the loss is 1.3605095637731992\n",
      "best k is 0.20109123319529162, best b is 2.5622506808876393, and the loss is 1.3309914946316195\n",
      "best k is 0.21121567241417907, best b is 2.525698242840288, and the loss is 1.3309914946316195\n",
      "best k is 0.21121567241417907, best b is 2.525698242840288, and the loss is 1.302462984765368\n",
      "best k is 0.22116895990649565, best b is 2.4897637168320057, and the loss is 1.302462984765368\n",
      "best k is 0.22116895990649565, best b is 2.4897637168320057, and the loss is 1.2748908603393034\n",
      "best k is 0.23095398895974872, best b is 2.4544366571768563, and the loss is 1.2748908603393034\n",
      "best k is 0.23095398895974872, best b is 2.4544366571768563, and the loss is 1.2482430596329057\n",
      "best k is 0.24057360395096822, best b is 2.4197067947712463, and the loss is 1.2482430596329057\n",
      "best k is 0.24057360395096822, best b is 2.4197067947712463, and the loss is 1.2224885957579121\n",
      "best k is 0.25003060117352927, best b is 2.3855640341088313, and the loss is 1.2224885957579121\n",
      "best k is 0.25003060117352927, best b is 2.3855640341088313, and the loss is 1.1975975206258063\n",
      "best k is 0.2593277296499977, best b is 2.3519984503458895, and the loss is 1.1975975206258063\n",
      "best k is 0.2593277296499977, best b is 2.3519984503458895, and the loss is 1.1735408901232498\n",
      "best k is 0.2684676919312334, best b is 2.3190002864163013, and the loss is 1.1735408901232498\n",
      "best k is 0.2684676919312334, best b is 2.3190002864163013, and the loss is 1.1502907304549737\n",
      "best k is 0.2774531448819862, best b is 2.286559950195301, and the loss is 1.1502907304549737\n",
      "best k is 0.2774531448819862, best b is 2.286559950195301, and the loss is 1.1278200056149807\n",
      "best k is 0.28628670045321103, best b is 2.2546680117111753, and the loss is 1.1278200056149807\n",
      "best k is 0.28628670045321103, best b is 2.2546680117111753, and the loss is 1.1061025859482396\n",
      "best k is 0.29497092644132633, best b is 2.2233152004040946, and the loss is 1.1061025859482396\n",
      "best k is 0.29497092644132633, best b is 2.2233152004040946, and the loss is 1.0851132177663143\n",
      "best k is 0.303508347234639, best b is 2.192492402431287, and the loss is 1.0851132177663143\n",
      "best k is 0.303508347234639, best b is 2.192492402431287, and the loss is 1.0648274939815912\n",
      "best k is 0.31190144454715, best b is 2.1621906580177668, and the loss is 1.0648274939815912\n",
      "best k is 0.31190144454715, best b is 2.1621906580177668, and the loss is 1.0452218257259627\n",
      "best k is 0.3201526581399549, best b is 2.1324011588518452, and the loss is 1.0452218257259627\n",
      "best k is 0.3201526581399549, best b is 2.1324011588518452, and the loss is 1.026273414920959\n",
      "best k is 0.3282643865304509, best b is 2.103115245524674, and the loss is 1.026273414920959\n",
      "best k is 0.3282643865304509, best b is 2.103115245524674, and the loss is 1.007960227767438\n",
      "best k is 0.3362389876895527, best b is 2.0743244050130714, and the loss is 1.007960227767438\n",
      "best k is 0.3362389876895527, best b is 2.0743244050130714, and the loss is 0.9902609691239987\n",
      "best k is 0.34407877972712336, best b is 2.0460202682048982, and the loss is 0.9902609691239987\n",
      "best k is 0.34407877972712336, best b is 2.0460202682048982, and the loss is 0.9731550577443336\n",
      "best k is 0.3517860415658182, best b is 2.0181946074662713, and the loss is 0.9731550577443336\n",
      "best k is 0.3517860415658182, best b is 2.0181946074662713, and the loss is 0.956622602344716\n",
      "best k is 0.3593630136035368, best b is 1.9908393342498987, and the loss is 0.956622602344716\n",
      "best k is 0.3593630136035368, best b is 1.9908393342498987, and the loss is 0.9406443784737994\n",
      "best k is 0.36681189836467676, best b is 1.9639464967438478, and the loss is 0.9406443784737994\n",
      "best k is 0.36681189836467676, best b is 1.9639464967438478, and the loss is 0.925201806157831\n",
      "best k is 0.37413486114037803, best b is 1.93750827756006, and the loss is 0.925201806157831\n",
      "best k is 0.37413486114037803, best b is 1.93750827756006, and the loss is 0.9102769282952796\n",
      "best k is 0.3813340306179442, best b is 1.9115169914619405, and the loss is 0.9102769282952796\n",
      "best k is 0.3813340306179442, best b is 1.9115169914619405, and the loss is 0.8958523897757662\n",
      "best k is 0.38841149949962345, best b is 1.8859650831303632, and the loss is 0.8958523897757662\n",
      "best k is 0.38841149949962345, best b is 1.8859650831303632, and the loss is 0.8819114172989988\n",
      "best k is 0.39536932511092876, best b is 1.8608451249674398, and the loss is 0.8819114172989988\n",
      "best k is 0.39536932511092876, best b is 1.8608451249674398, and the loss is 0.8684377998702644\n",
      "best k is 0.4022095299986752, best b is 1.8361498149374171, and the loss is 0.8684377998702644\n",
      "best k is 0.4022095299986752, best b is 1.8361498149374171, and the loss is 0.8554158699497764\n",
      "best k is 0.4089341025189074, best b is 1.8118719744440728, and the loss is 0.8554158699497764\n",
      "best k is 0.4089341025189074, best b is 1.8118719744440728, and the loss is 0.8428304852339796\n",
      "best k is 0.4155449974148875, best b is 1.7880045462439933, and the loss is 0.8428304852339796\n",
      "best k is 0.4155449974148875, best b is 1.7880045462439933, and the loss is 0.8306670110476073\n",
      "best k is 0.4220441363853133, best b is 1.7645405923951278, and the loss is 0.8306670110476073\n",
      "best k is 0.4220441363853133, best b is 1.7645405923951278, and the loss is 0.8189113033260265\n",
      "best k is 0.42843340864293034, best b is 1.7414732922400211, and the loss is 0.8189113033260265\n",
      "best k is 0.42843340864293034, best b is 1.7414732922400211, and the loss is 0.8075496921680838\n",
      "best k is 0.43471467146370063, best b is 1.7187959404231399, and the loss is 0.8075496921680838\n",
      "best k is 0.43471467146370063, best b is 1.7187959404231399, and the loss is 0.7965689659403206\n",
      "best k is 0.44088975072668796, best b is 1.6965019449417158, and the loss is 0.7965689659403206\n",
      "best k is 0.44088975072668796, best b is 1.6965019449417158, and the loss is 0.7859563559140744\n",
      "best k is 0.44696044144481645, best b is 1.6745848252295379, and the loss is 0.7859563559140744\n",
      "best k is 0.44696044144481645, best b is 1.6745848252295379, and the loss is 0.7756995214176081\n",
      "best k is 0.45292850828665704, best b is 1.6530382102731391, and the loss is 0.7756995214176081\n",
      "best k is 0.45292850828665704, best b is 1.6530382102731391, and the loss is 0.7657865354859924\n",
      "best k is 0.4587956860893926, best b is 1.6318558367598281, and the loss is 0.7657865354859924\n",
      "best k is 0.4587956860893926, best b is 1.6318558367598281, and the loss is 0.7562058709920615\n",
      "best k is 0.46456368036311235, best b is 1.6110315472570276, and the loss is 0.7562058709920615\n",
      "best k is 0.46456368036311235, best b is 1.6110315472570276, and the loss is 0.7469463872423154\n",
      "best k is 0.47023416778658045, best b is 1.5905592884223911, and the loss is 0.7469463872423154\n",
      "best k is 0.47023416778658045, best b is 1.5905592884223911, and the loss is 0.7379973170221727\n",
      "best k is 0.47580879669462456, best b is 1.570433109244178, and the loss is 0.7379973170221727\n",
      "best k is 0.47580879669462456, best b is 1.570433109244178, and the loss is 0.729348254075522\n",
      "best k is 0.48128918755728417, best b is 1.5506471593113726, and the loss is 0.729348254075522\n",
      "best k is 0.48128918755728417, best b is 1.5506471593113726, and the loss is 0.7209891410040032\n",
      "best k is 0.48667693345085977, best b is 1.53119568711305, and the loss is 0.7209891410040032\n",
      "best k is 0.48667693345085977, best b is 1.53119568711305, and the loss is 0.7129102575719564\n",
      "best k is 0.49197360052099903, best b is 1.512073038366487, and the loss is 0.7129102575719564\n",
      "best k is 0.49197360052099903, best b is 1.512073038366487, and the loss is 0.7051022094034268\n",
      "best k is 0.4971807284379539, best b is 1.4932736543735388, and the loss is 0.7051022094034268\n",
      "best k is 0.4971807284379539, best b is 1.4932736543735388, and the loss is 0.6975559170580956\n",
      "best k is 0.502299830844143, best b is 1.4747920704047988, and the loss is 0.6975559170580956\n",
      "best k is 0.502299830844143, best b is 1.4747920704047988, and the loss is 0.6902626054734243\n",
      "best k is 0.507332395794146, best b is 1.456622914111076, and the loss is 0.6902626054734243\n",
      "best k is 0.507332395794146, best b is 1.456622914111076, and the loss is 0.6832137937607422\n",
      "best k is 0.5122798861872626, best b is 1.4387609039617246, and the loss is 0.6832137937607422\n",
      "best k is 0.5122798861872626, best b is 1.4387609039617246, and the loss is 0.676401285343403\n",
      "best k is 0.5171437401927564, best b is 1.4212008477093734, and the loss is 0.676401285343403\n",
      "best k is 0.5171437401927564, best b is 1.4212008477093734, and the loss is 0.6698171584255551\n",
      "best k is 0.5219253716679123, best b is 1.403937640880609, and the loss is 0.6698171584255551\n",
      "best k is 0.5219253716679123, best b is 1.403937640880609, and the loss is 0.6634537567804287\n",
      "best k is 0.526626170569026, best b is 1.3869662652921744, and the loss is 0.6634537567804287\n",
      "best k is 0.526626170569026, best b is 1.3869662652921744, and the loss is 0.657303680847441\n",
      "best k is 0.5312475033554451, best b is 1.370281787592249, and the loss is 0.657303680847441\n",
      "best k is 0.5312475033554451, best b is 1.370281787592249, and the loss is 0.6513597791277551\n",
      "best k is 0.5357907133867809, best b is 1.3538793578263906, and the loss is 0.6513597791277551\n",
      "best k is 0.5357907133867809, best b is 1.3538793578263906, and the loss is 0.6456151398682979\n",
      "best k is 0.5402571213134048, best b is 1.3377542080277172, and the loss is 0.6456151398682979\n",
      "best k is 0.5402571213134048, best b is 1.3377542080277172, and the loss is 0.6400630830245586\n",
      "best k is 0.5446480254603444, best b is 1.321901650830924, and the loss is 0.6400630830245586\n",
      "best k is 0.5446480254603444, best b is 1.321901650830924, and the loss is 0.6346971524928255\n",
      "best k is 0.5489647022046883, best b is 1.3063170781097282, and the loss is 0.6346971524928255\n",
      "best k is 0.5489647022046883, best b is 1.3063170781097282, and the loss is 0.6295111086028298\n",
      "best k is 0.5532084063466127, best b is 1.2909959596373488, and the loss is 0.6295111086028298\n",
      "best k is 0.5532084063466127, best b is 1.2909959596373488, and the loss is 0.6244989208620622\n",
      "best k is 0.5573803714741341, best b is 1.2759338417696302, and the loss is 0.6244989208620622\n",
      "best k is 0.5573803714741341, best b is 1.2759338417696302, and the loss is 0.6196547609433305\n",
      "best k is 0.5614818103216975, best b is 1.261126346150427, and the loss is 0.6196547609433305\n",
      "best k is 0.5614818103216975, best b is 1.261126346150427, and the loss is 0.6149729959073998\n",
      "best k is 0.5655139151227021, best b is 1.246569168438875, and the loss is 0.6149729959073998\n",
      "best k is 0.5655139151227021, best b is 1.246569168438875, and the loss is 0.6104481816528359\n",
      "best k is 0.5694778579560673, best b is 1.232258077058177, and the loss is 0.6104481816528359\n",
      "best k is 0.5694778579560673, best b is 1.232258077058177, and the loss is 0.6060750565854364\n",
      "best k is 0.5733747910869402, best b is 1.2181889119655391, and the loss is 0.6060750565854364\n",
      "best k is 0.5733747910869402, best b is 1.2181889119655391, and the loss is 0.6018485354998844\n",
      "best k is 0.5772058473016443, best b is 1.2043575834429032, and the loss is 0.6018485354998844\n",
      "best k is 0.5772058473016443, best b is 1.2043575834429032, and the loss is 0.5977637036665162\n",
      "best k is 0.5809721402369646, best b is 1.1907600709081196, and the loss is 0.5977637036665162\n",
      "best k is 0.5809721402369646, best b is 1.1907600709081196, and the loss is 0.5938158111163199\n",
      "best k is 0.5846747647038677, best b is 1.1773924217462184, and the loss is 0.5938158111163199\n",
      "best k is 0.5846747647038677, best b is 1.1773924217462184, and the loss is 0.5900002671175248\n",
      "best k is 0.5883147970057476, best b is 1.1642507501604362, and the loss is 0.5900002671175248\n",
      "best k is 0.5883147970057476, best b is 1.1642507501604362, and the loss is 0.5863126348373562\n",
      "best k is 0.5918932952512943, best b is 1.1513312360426684, and the loss is 0.5863126348373562\n",
      "best k is 0.5918932952512943, best b is 1.1513312360426684, and the loss is 0.5827486261827481\n",
      "best k is 0.59541129966207, best b is 1.1386301238630132, and the loss is 0.5827486261827481\n",
      "best k is 0.59541129966207, best b is 1.1386301238630132, and the loss is 0.5793040968140182\n",
      "best k is 0.598869832874889, best b is 1.126143721578091, and the loss is 0.5793040968140182\n",
      "best k is 0.598869832874889, best b is 1.126143721578091, and the loss is 0.5759750413256968\n",
      "best k is 0.6022699002390838, best b is 1.1138683995578151, and the loss is 0.5759750413256968\n",
      "best k is 0.6022699002390838, best b is 1.1138683995578151, and the loss is 0.5727575885889226\n",
      "best k is 0.6056124901087471, best b is 1.1018005895303085, and the loss is 0.5727575885889226\n",
      "best k is 0.6056124901087471, best b is 1.1018005895303085, and the loss is 0.5696479972499702\n",
      "best k is 0.6088985741300328, best b is 1.0899367835446534, and the loss is 0.5696479972499702\n",
      "best k is 0.6088985741300328, best b is 1.0899367835446534, and the loss is 0.566642651379689\n",
      "best k is 0.6121291075236007, best b is 1.0782735329511781, and the loss is 0.566642651379689\n",
      "best k is 0.6121291075236007, best b is 1.0782735329511781, and the loss is 0.5637380562687898\n",
      "best k is 0.6153050293622865, best b is 1.0668074473989801, and the loss is 0.5637380562687898\n",
      "best k is 0.6153050293622865, best b is 1.0668074473989801, and the loss is 0.5609308343640881\n",
      "best k is 0.6184272628440773, best b is 1.0555351938503963, and the loss is 0.5609308343640881\n",
      "best k is 0.6184272628440773, best b is 1.0555351938503963, and the loss is 0.5582177213409795\n",
      "best k is 0.6214967155604734, best b is 1.0444534956121334, and the loss is 0.5582177213409795\n",
      "best k is 0.6214967155604734, best b is 1.0444534956121334, and the loss is 0.5555955623075828\n",
      "best k is 0.6245142797603126, best b is 1.033559131382778, and the loss is 0.5555955623075828\n",
      "best k is 0.6245142797603126, best b is 1.033559131382778, and the loss is 0.5530613081361351\n",
      "best k is 0.6274808326091353, best b is 1.0228489343164064, and the loss is 0.5530613081361351\n",
      "best k is 0.6274808326091353, best b is 1.0228489343164064, and the loss is 0.5506120119173696\n",
      "best k is 0.6303972364441646, best b is 1.0123197911020252, and the loss is 0.5506120119173696\n",
      "best k is 0.6303972364441646, best b is 1.0123197911020252, and the loss is 0.5482448255337586\n",
      "best k is 0.633264339024976, best b is 1.0019686410585733, and the loss is 0.5482448255337586\n",
      "best k is 0.633264339024976, best b is 1.0019686410585733, and the loss is 0.5459569963476358\n",
      "best k is 0.6360829737799304, best b is 0.9917924752452232, and the loss is 0.5459569963476358\n",
      "best k is 0.6360829737799304, best b is 0.9917924752452232, and the loss is 0.5437458640003421\n",
      "best k is 0.63885396004844, best b is 0.9817883355867217, and the loss is 0.5437458640003421\n",
      "best k is 0.63885396004844, best b is 0.9817883355867217, and the loss is 0.5416088573186787\n",
      "best k is 0.6415781033191394, best b is 0.9719533140135176, and the loss is 0.5416088573186787\n",
      "best k is 0.6415781033191394, best b is 0.9719533140135176, and the loss is 0.5395434913250674\n",
      "best k is 0.6442561954640308, best b is 0.962284551616424, and the loss is 0.5395434913250674\n",
      "best k is 0.6442561954640308, best b is 0.962284551616424, and the loss is 0.5375473643479425\n",
      "best k is 0.6468890149686697, best b is 0.9527792378155724, and the loss is 0.5375473643479425\n",
      "best k is 0.6468890149686697, best b is 0.9527792378155724, and the loss is 0.5356181552290102\n",
      "best k is 0.6494773271584614, best b is 0.9434346095434143, and the loss is 0.5356181552290102\n",
      "best k is 0.6494773271584614, best b is 0.9434346095434143, and the loss is 0.5337536206241369\n",
      "best k is 0.6520218844211295, best b is 0.9342479504415344, and the loss is 0.5337536206241369\n",
      "best k is 0.6520218844211295, best b is 0.9342479504415344, and the loss is 0.5319515923947167\n",
      "best k is 0.6545234264254267, best b is 0.9252165900710421, and the loss is 0.5319515923947167\n",
      "best k is 0.6545234264254267, best b is 0.9252165900710421, and the loss is 0.5302099750864948\n",
      "best k is 0.6569826803361447, best b is 0.9163379031363098, and the loss is 0.5302099750864948\n",
      "best k is 0.6569826803361447, best b is 0.9163379031363098, and the loss is 0.528526743492906\n",
      "best k is 0.6594003610254925, best b is 0.9076093087218354, and the loss is 0.528526743492906\n",
      "best k is 0.6594003610254925, best b is 0.9076093087218354, and the loss is 0.5268999403001038\n",
      "best k is 0.6617771712809001, best b is 0.8990282695420041, and the loss is 0.5268999403001038\n",
      "best k is 0.6617771712809001, best b is 0.8990282695420041, and the loss is 0.5253276738109337\n",
      "best k is 0.6641138020093087, best b is 0.8905922912035337, and the loss is 0.5253276738109337\n",
      "best k is 0.6641138020093087, best b is 0.8905922912035337, and the loss is 0.5238081157452101\n",
      "best k is 0.6664109324380091, best b is 0.8822989214803877, and the loss is 0.5238081157452101\n",
      "best k is 0.6664109324380091, best b is 0.8822989214803877, and the loss is 0.522339499113736\n",
      "best k is 0.6686692303120828, best b is 0.8741457496009462, and the loss is 0.522339499113736\n",
      "best k is 0.6686692303120828, best b is 0.8741457496009462, and the loss is 0.5209201161635914\n",
      "best k is 0.670889352088508, best b is 0.8661304055472268, and the loss is 0.5209201161635914\n",
      "best k is 0.670889352088508, best b is 0.8661304055472268, and the loss is 0.519548316392306\n",
      "best k is 0.6730719431269812, best b is 0.8582505593659517, and the loss is 0.519548316392306\n",
      "best k is 0.6730719431269812, best b is 0.8582505593659517, and the loss is 0.5182225046286058\n",
      "best k is 0.6752176378775163, best b is 0.8505039204912622, and the loss is 0.5182225046286058\n",
      "best k is 0.6752176378775163, best b is 0.8505039204912622, and the loss is 0.5169411391774951\n",
      "best k is 0.6773270600648696, best b is 0.842888237078881, and the loss is 0.5169411391774951\n",
      "best k is 0.6773270600648696, best b is 0.842888237078881, and the loss is 0.5157027300275281\n",
      "best k is 0.6794008228698488, best b is 0.8354012953515321, and the loss is 0.5157027300275281\n",
      "best k is 0.6794008228698488, best b is 0.8354012953515321, and the loss is 0.5145058371181773\n",
      "best k is 0.6814395291075555, best b is 0.8280409189554242, and the loss is 0.5145058371181773\n",
      "best k is 0.6814395291075555, best b is 0.8280409189554242, and the loss is 0.5133490686652855\n",
      "best k is 0.6834437714026171, best b is 0.8208049683276152, and the loss is 0.5133490686652855\n",
      "best k is 0.6834437714026171, best b is 0.8208049683276152, and the loss is 0.5122310795426566\n",
      "best k is 0.6854141323614538, best b is 0.8136913400740685, and the loss is 0.5122310795426566\n",
      "best k is 0.6854141323614538, best b is 0.8136913400740685, and the loss is 0.5111505697179008\n",
      "best k is 0.687351184741634, best b is 0.8066979663582255, and the loss is 0.5111505697179008\n",
      "best k is 0.687351184741634, best b is 0.8066979663582255, and the loss is 0.5101062827407193\n",
      "best k is 0.689255491618369, best b is 0.7998228142999128, and the loss is 0.5101062827407193\n",
      "best k is 0.689255491618369, best b is 0.7998228142999128, and the loss is 0.5090970042818592\n",
      "best k is 0.6911276065481893, best b is 0.7930638853844107, and the loss is 0.5090970042818592\n",
      "best k is 0.6911276065481893, best b is 0.7930638853844107, and the loss is 0.5081215607210597\n",
      "best k is 0.6929680737298578, best b is 0.7864192148815129, and the loss is 0.5081215607210597\n",
      "best k is 0.6929680737298578, best b is 0.7864192148815129, and the loss is 0.5071788177823278\n",
      "best k is 0.6947774281625604, best b is 0.7798868712744043, and the loss is 0.5071788177823278\n",
      "best k is 0.6947774281625604, best b is 0.7798868712744043, and the loss is 0.5062676792149654\n",
      "best k is 0.6965561958014227, best b is 0.7734649556981957, and the loss is 0.5062676792149654\n",
      "best k is 0.6965561958014227, best b is 0.7734649556981957, and the loss is 0.5053870855188178\n",
      "best k is 0.6983048937103989, best b is 0.7671516013879494, and the loss is 0.5053870855188178\n",
      "best k is 0.6983048937103989, best b is 0.7671516013879494, and the loss is 0.5045360127122531\n",
      "best k is 0.7000240302125753, best b is 0.7609449731360347, and the loss is 0.5045360127122531\n",
      "best k is 0.7000240302125753, best b is 0.7609449731360347, and the loss is 0.5037134711414433\n",
      "best k is 0.7017141050379321, best b is 0.7548432667586586, and the loss is 0.5037134711414433\n",
      "best k is 0.7017141050379321, best b is 0.7548432667586586, and the loss is 0.5029185043295663\n",
      "best k is 0.7033756094686091, best b is 0.7488447085714132, and the loss is 0.5029185043295663\n",
      "best k is 0.7033756094686091, best b is 0.7488447085714132, and the loss is 0.5021501878645835\n",
      "best k is 0.7050090264817152, best b is 0.7429475548736891, and the loss is 0.5021501878645835\n",
      "best k is 0.7050090264817152, best b is 0.7429475548736891, and the loss is 0.5014076283243059\n",
      "best k is 0.7066148308897218, best b is 0.7371500914418057, and the loss is 0.5014076283243059\n",
      "best k is 0.7066148308897218, best b is 0.7371500914418057, and the loss is 0.5006899622374934\n",
      "best k is 0.708193489478486, best b is 0.7314506330307086, and the loss is 0.5006899622374934\n",
      "best k is 0.708193489478486, best b is 0.7314506330307086, and the loss is 0.4999963550797858\n",
      "best k is 0.7097454611429389, best b is 0.7258475228840919, and the loss is 0.4999963550797858\n",
      "best k is 0.7097454611429389, best b is 0.7258475228840919, and the loss is 0.4993260003032908\n",
      "best k is 0.7112711970204785, best b is 0.7203391322528011, and the loss is 0.4993260003032908\n",
      "best k is 0.7112711970204785, best b is 0.7203391322528011, and the loss is 0.4986781183987058\n",
      "best k is 0.7127711406221119, best b is 0.7149238599213774, and the loss is 0.4986781183987058\n",
      "best k is 0.7127711406221119, best b is 0.7149238599213774, and the loss is 0.49805195598887925\n",
      "best k is 0.7142457279613756, best b is 0.7096001317426062, and the loss is 0.49805195598887925\n",
      "best k is 0.7142457279613756, best b is 0.7096001317426062, and the loss is 0.49744678495276107\n",
      "best k is 0.7156953876810805, best b is 0.7043664001799329, and the loss is 0.49744678495276107\n",
      "best k is 0.7156953876810805, best b is 0.7043664001799329, and the loss is 0.49686190157872157\n",
      "best k is 0.717120541177912, best b is 0.6992211438576155, and the loss is 0.49686190157872157\n",
      "best k is 0.717120541177912, best b is 0.6992211438576155, and the loss is 0.4962966257462519\n",
      "best k is 0.7185216027249242, best b is 0.6941628671184803, and the loss is 0.4962966257462519\n",
      "best k is 0.7185216027249242, best b is 0.6941628671184803, and the loss is 0.49575030013510135\n",
      "best k is 0.7198989795919635, best b is 0.689190099589155, and the loss is 0.49575030013510135\n",
      "best k is 0.7198989795919635, best b is 0.689190099589155, and the loss is 0.4952222894609227\n",
      "best k is 0.7212530721640572, best b is 0.6843013957526505, and the loss is 0.4952222894609227\n",
      "best k is 0.7212530721640572, best b is 0.6843013957526505, and the loss is 0.49471197973654535\n",
      "best k is 0.7225842740577991, best b is 0.6794953345281682, and the loss is 0.49471197973654535\n",
      "best k is 0.7225842740577991, best b is 0.6794953345281682, and the loss is 0.4942187775580117\n",
      "best k is 0.7238929722357695, best b is 0.6747705188580116, and the loss is 0.4942187775580117\n",
      "best k is 0.7238929722357695, best b is 0.6747705188580116, and the loss is 0.49374210941454766\n",
      "best k is 0.7251795471190196, best b is 0.6701255753014796, and the loss is 0.49374210941454766\n",
      "best k is 0.7251795471190196, best b is 0.6701255753014796, and the loss is 0.49328142102166783\n",
      "best k is 0.7264443726976542, best b is 0.6655591536356258, and the loss is 0.49328142102166783\n",
      "best k is 0.7264443726976542, best b is 0.6655591536356258, and the loss is 0.4928361766766368\n",
      "best k is 0.7276878166395468, best b is 0.661069926462767, and the loss is 0.4928361766766368\n",
      "best k is 0.7276878166395468, best b is 0.661069926462767, and the loss is 0.4924058586355346\n",
      "best k is 0.7289102403972152, best b is 0.6566565888246262, and the loss is 0.4924058586355346\n",
      "best k is 0.7289102403972152, best b is 0.6566565888246262, and the loss is 0.49198996651121163\n",
      "best k is 0.7301119993128906, best b is 0.652317857822999, and the loss is 0.49198996651121163\n",
      "best k is 0.7301119993128906, best b is 0.652317857822999, and the loss is 0.49158801669142027\n",
      "best k is 0.7312934427218113, best b is 0.6480524722468319, and the loss is 0.49158801669142027\n",
      "best k is 0.7312934427218113, best b is 0.6480524722468319, and the loss is 0.4911995417764571\n",
      "best k is 0.7324549140537693, best b is 0.6438591922056054, and the loss is 0.4911995417764571\n",
      "best k is 0.7324549140537693, best b is 0.6438591922056054, and the loss is 0.490824090035655\n",
      "best k is 0.7335967509329415, best b is 0.6397367987689141, and the loss is 0.490824090035655\n",
      "best k is 0.7335967509329415, best b is 0.6397367987689141, and the loss is 0.4904612248820979\n",
      "best k is 0.7347192852760316, best b is 0.6356840936121402, and the loss is 0.4904612248820979\n",
      "best k is 0.7347192852760316, best b is 0.6356840936121402, and the loss is 0.4901105243649428\n",
      "best k is 0.7358228433887548, best b is 0.6316998986681167, and the loss is 0.4901105243649428\n",
      "best k is 0.7358228433887548, best b is 0.6316998986681167, and the loss is 0.4897715806787623\n",
      "best k is 0.7369077460606894, best b is 0.6277830557846786, and the loss is 0.4897715806787623\n",
      "best k is 0.7369077460606894, best b is 0.6277830557846786, and the loss is 0.48944399968933866\n",
      "best k is 0.7379743086585275, best b is 0.623932426388004, and the loss is 0.48944399968933866\n",
      "best k is 0.7379743086585275, best b is 0.623932426388004, and the loss is 0.48912740047534653\n",
      "best k is 0.7390228412177461, best b is 0.6201468911516453, and the loss is 0.48912740047534653\n",
      "best k is 0.7390228412177461, best b is 0.6201468911516453, and the loss is 0.48882141488541225\n",
      "best k is 0.7400536485327318, best b is 0.616425349671157, and the loss is 0.48882141488541225\n",
      "best k is 0.7400536485327318, best b is 0.616425349671157, and the loss is 0.4885256871100105\n",
      "best k is 0.7410670302453797, best b is 0.6127667201442217, and the loss is 0.4885256871100105\n",
      "best k is 0.7410670302453797, best b is 0.6127667201442217, and the loss is 0.48823987326772345\n",
      "best k is 0.7420632809321955, best b is 0.6091699390561857, and the loss is 0.48823987326772345\n",
      "best k is 0.7420632809321955, best b is 0.6091699390561857, and the loss is 0.48796364100535905\n",
      "best k is 0.7430426901899247, best b is 0.6056339608709085, and the loss is 0.48796364100535905\n",
      "best k is 0.7430426901899247, best b is 0.6056339608709085, and the loss is 0.48769666911148485\n",
      "best k is 0.7440055427197351, best b is 0.6021577577268402, and the loss is 0.48769666911148485\n",
      "best k is 0.7440055427197351, best b is 0.6021577577268402, and the loss is 0.4874386471429112\n",
      "best k is 0.7449521184099744, best b is 0.5987403191382357, and the loss is 0.4874386471429112\n",
      "best k is 0.7449521184099744, best b is 0.5987403191382357, and the loss is 0.48718927506369913\n",
      "best k is 0.7458826924175319, best b is 0.5953806517014197, and the loss is 0.48718927506369913\n",
      "best k is 0.7458826924175319, best b is 0.5953806517014197, and the loss is 0.4869482628962684\n",
      "best k is 0.7467975352478209, best b is 0.5920777788060182, and the loss is 0.4869482628962684\n",
      "best k is 0.7467975352478209, best b is 0.5920777788060182, and the loss is 0.48671533038420467\n",
      "best k is 0.7476969128334124, best b is 0.5888307403510701, and the loss is 0.48671533038420467\n",
      "best k is 0.7476969128334124, best b is 0.5888307403510701, and the loss is 0.4864902066663658\n",
      "best k is 0.7485810866113377, best b is 0.5856385924659393, and the loss is 0.4864902066663658\n",
      "best k is 0.7485810866113377, best b is 0.5856385924659393, and the loss is 0.4862726299619175\n",
      "best k is 0.7494503135990844, best b is 0.5825004072359441, and the loss is 0.4862726299619175\n",
      "best k is 0.7494503135990844, best b is 0.5825004072359441, and the loss is 0.4860623472659274\n",
      "best k is 0.7503048464693084, best b is 0.5794152724326244, and the loss is 0.4860623472659274\n",
      "best k is 0.7503048464693084, best b is 0.5794152724326244, and the loss is 0.48585911405516147\n",
      "best k is 0.7511449336232818, best b is 0.5763822912485694, and the loss is 0.48585911405516147\n",
      "best k is 0.7511449336232818, best b is 0.5763822912485694, and the loss is 0.4856626940037453\n",
      "best k is 0.751970819263101, best b is 0.573400582036728, and the loss is 0.4856626940037453\n",
      "best k is 0.751970819263101, best b is 0.573400582036728, and the loss is 0.48547285870835805\n",
      "best k is 0.7527827434626715, best b is 0.5704692780541248, and the loss is 0.48547285870835805\n",
      "best k is 0.7527827434626715, best b is 0.5704692780541248, and the loss is 0.4852893874226364\n",
      "best k is 0.7535809422374954, best b is 0.5675875272099109, and the loss is 0.4852893874226364\n",
      "best k is 0.7535809422374954, best b is 0.5675875272099109, and the loss is 0.48511206680048574\n",
      "best k is 0.7543656476132772, best b is 0.5647544918176712, and the loss is 0.48511206680048574\n",
      "best k is 0.7543656476132772, best b is 0.5647544918176712, and the loss is 0.48494069064799233\n",
      "best k is 0.755137087693371, best b is 0.5619693483519209, and the loss is 0.48494069064799233\n",
      "best k is 0.755137087693371, best b is 0.5619693483519209, and the loss is 0.48477505968365636\n",
      "best k is 0.7558954867250866, best b is 0.5592312872087175, and the loss is 0.48477505968365636\n",
      "best k is 0.7558954867250866, best b is 0.5592312872087175, and the loss is 0.48461498130666153\n",
      "best k is 0.7566410651648762, best b is 0.5565395124703199, and the loss is 0.48461498130666153\n",
      "best k is 0.7566410651648762, best b is 0.5565395124703199, and the loss is 0.48446026937291126\n",
      "best k is 0.7573740397424165, best b is 0.553893241673825, and the loss is 0.48446026937291126\n",
      "best k is 0.7573740397424165, best b is 0.553893241673825, and the loss is 0.4843107439785753\n",
      "best k is 0.7580946235236109, best b is 0.5512917055837177, and the loss is 0.4843107439785753\n",
      "best k is 0.7580946235236109, best b is 0.5512917055837177, and the loss is 0.4841662312508931\n",
      "best k is 0.7588030259725236, best b is 0.5487341479682626, and the loss is 0.4841662312508931\n",
      "best k is 0.7588030259725236, best b is 0.5487341479682626, and the loss is 0.48402656314598713\n",
      "best k is 0.7594994530122688, best b is 0.5462198253796793, and the loss is 0.48402656314598713\n",
      "best k is 0.7594994530122688, best b is 0.5462198253796793, and the loss is 0.4838915772534581\n",
      "best k is 0.7601841070848694, best b is 0.5437480069380307, and the loss is 0.4838915772534581\n",
      "best k is 0.7601841070848694, best b is 0.5437480069380307, and the loss is 0.48376111660752796\n",
      "best k is 0.7608571872101039, best b is 0.5413179741187668, and the loss is 0.48376111660752796\n",
      "best k is 0.7608571872101039, best b is 0.5413179741187668, and the loss is 0.4836350295045163\n",
      "best k is 0.7615188890433596, best b is 0.5389290205438589, and the loss is 0.4836350295045163\n",
      "best k is 0.7615188890433596, best b is 0.5389290205438589, and the loss is 0.48351316932643246\n",
      "best k is 0.7621694049325063, best b is 0.5365804517764652, and the loss is 0.48351316932643246\n",
      "best k is 0.7621694049325063, best b is 0.5365804517764652, and the loss is 0.48339539437048595\n",
      "best k is 0.7628089239738098, best b is 0.5342715851190668, and the loss is 0.48339539437048595\n",
      "best k is 0.7628089239738098, best b is 0.5342715851190668, and the loss is 0.4832815676843095\n",
      "best k is 0.763437632066899, best b is 0.5320017494150172, and the loss is 0.4832815676843095\n",
      "best k is 0.763437632066899, best b is 0.5320017494150172, and the loss is 0.48317155690670577\n",
      "best k is 0.764055711968805, best b is 0.5297702848534458, and the loss is 0.48317155690670577\n",
      "best k is 0.764055711968805, best b is 0.5297702848534458, and the loss is 0.4830652341137343\n",
      "best k is 0.7646633433470857, best b is 0.5275765427774597, and the loss is 0.4830652341137343\n",
      "best k is 0.7646633433470857, best b is 0.5275765427774597, and the loss is 0.482962475669957\n",
      "best k is 0.7652607028320535, best b is 0.5254198854955879, and the loss is 0.482962475669957\n",
      "best k is 0.7652607028320535, best b is 0.5254198854955879, and the loss is 0.48286316208467184\n",
      "best k is 0.7658479640681183, best b is 0.5232996860964131, and the loss is 0.48286316208467184\n",
      "best k is 0.7658479640681183, best b is 0.5232996860964131, and the loss is 0.48276717787296547\n",
      "best k is 0.7664252977642643, best b is 0.5212153282663363, and the loss is 0.48276717787296547\n",
      "best k is 0.7664252977642643, best b is 0.5212153282663363, and the loss is 0.4826744114214219\n",
      "best k is 0.7669928717436727, best b is 0.5191662061104234, and the loss is 0.4826744114214219\n",
      "best k is 0.7669928717436727, best b is 0.5191662061104234, and the loss is 0.48258475485833763\n",
      "best k is 0.7675508509925058, best b is 0.5171517239762793, and the loss is 0.48258475485833763\n",
      "best k is 0.7675508509925058, best b is 0.5171517239762793, and the loss is 0.48249810392828385\n",
      "best k is 0.7680993977078657, best b is 0.5151712962808996, and the loss is 0.48249810392828385\n",
      "best k is 0.7680993977078657, best b is 0.5151712962808996, and the loss is 0.4824143578708736\n",
      "best k is 0.7686386713449436, best b is 0.51322434734045, and the loss is 0.4824143578708736\n",
      "best k is 0.7686386713449436, best b is 0.51322434734045, and the loss is 0.4823334193035974\n",
      "best k is 0.7691688286633707, best b is 0.5113103112029219, and the loss is 0.4823334193035974\n",
      "best k is 0.7691688286633707, best b is 0.5113103112029219, and the loss is 0.4822551941085811\n",
      "best k is 0.7696900237727864, best b is 0.5094286314836185, and the loss is 0.4822551941085811\n",
      "best k is 0.7696900237727864, best b is 0.5094286314836185, and the loss is 0.4821795913231444\n",
      "best k is 0.7702024081776357, best b is 0.5075787612034207, and the loss is 0.4821795913231444\n",
      "best k is 0.7702024081776357, best b is 0.5075787612034207, and the loss is 0.48210652303402624\n",
      "best k is 0.7707061308212102, best b is 0.505760162629788, and the loss is 0.48210652303402624\n",
      "best k is 0.7707061308212102, best b is 0.505760162629788, and the loss is 0.4820359042751563\n",
      "best k is 0.7712013381289425, best b is 0.503972307120446, and the loss is 0.4820359042751563\n",
      "best k is 0.7712013381289425, best b is 0.503972307120446, and the loss is 0.48196765292885396\n",
      "best k is 0.7716881740509719, best b is 0.5022146749697187, and the loss is 0.48196765292885396\n",
      "best k is 0.7716881740509719, best b is 0.5022146749697187, and the loss is 0.481901689630339\n",
      "best k is 0.7721667801039872, best b is 0.5004867552574552, and the loss is 0.481901689630339\n",
      "best k is 0.7721667801039872, best b is 0.5004867552574552, and the loss is 0.48183793767544436\n",
      "best k is 0.7726372954123648, best b is 0.49878804570051355, and the loss is 0.48183793767544436\n",
      "best k is 0.7726372954123648, best b is 0.49878804570051355, and the loss is 0.48177632293142114\n",
      "best k is 0.7730998567486095, best b is 0.4971180525067528, and the loss is 0.48177632293142114\n",
      "best k is 0.7730998567486095, best b is 0.4971180525067528, and the loss is 0.4817167737507365\n",
      "best k is 0.7735545985731132, best b is 0.49547629023149464, and the loss is 0.4817167737507365\n",
      "best k is 0.7735545985731132, best b is 0.49547629023149464, and the loss is 0.48165922088775864\n",
      "best k is 0.7740016530732403, best b is 0.4938622816364112, and the loss is 0.48165922088775864\n",
      "best k is 0.7740016530732403, best b is 0.4938622816364112, and the loss is 0.481603597418235\n",
      "best k is 0.7744411502017526, best b is 0.492275557550798, and the loss is 0.481603597418235\n",
      "best k is 0.7744411502017526, best b is 0.492275557550798, and the loss is 0.48154983866147144\n",
      "best k is 0.7748732177145853, best b is 0.4907156567351924, and the loss is 0.48154983866147144\n",
      "best k is 0.7748732177145853, best b is 0.4907156567351924, and the loss is 0.4814978821051204\n",
      "best k is 0.7752979812079838, best b is 0.4891821257472976, and the loss is 0.4814978821051204\n",
      "best k is 0.7752979812079838, best b is 0.4891821257472976, and the loss is 0.4814476673324887\n",
      "best k is 0.7757155641550124, best b is 0.4876745188101727, and the loss is 0.4814476673324887\n",
      "best k is 0.7757155641550124, best b is 0.4876745188101727, and the loss is 0.481399135952283\n",
      "best k is 0.7761260879414469, best b is 0.4861923976826517, and the loss is 0.481399135952283\n",
      "best k is 0.7761260879414469, best b is 0.4861923976826517, and the loss is 0.4813522315307104\n",
      "best k is 0.7765296719010598, best b is 0.4847353315319525, and the loss is 0.4813522315307104\n",
      "best k is 0.7765296719010598, best b is 0.4847353315319525, and the loss is 0.4813068995258565\n",
      "best k is 0.7769264333503083, best b is 0.4833028968084393, and the loss is 0.4813068995258565\n",
      "best k is 0.7769264333503083, best b is 0.4833028968084393, and the loss is 0.48126308722426164\n",
      "best k is 0.7773164876224374, best b is 0.4818946771225029, and the loss is 0.48126308722426164\n",
      "best k is 0.7773164876224374, best b is 0.4818946771225029, and the loss is 0.4812207436796238\n",
      "best k is 0.7776999481010054, best b is 0.48051026312352135, and the loss is 0.4812207436796238\n",
      "best k is 0.7776999481010054, best b is 0.48051026312352135, and the loss is 0.4811798196535576\n",
      "best k is 0.7780769262528431, best b is 0.4791492523808676, and the loss is 0.4811798196535576\n",
      "best k is 0.7780769262528431, best b is 0.4791492523808676, and the loss is 0.48114026755833805\n",
      "best k is 0.7784475316604553, best b is 0.4778112492669279, and the loss is 0.48114026755833805\n",
      "best k is 0.7784475316604553, best b is 0.4778112492669279, and the loss is 0.48110204140156265\n",
      "best k is 0.7788118720538761, best b is 0.47649586484209855, and the loss is 0.48110204140156265\n",
      "best k is 0.7788118720538761, best b is 0.47649586484209855, and the loss is 0.48106509673267234\n",
      "best k is 0.7791700533419829, best b is 0.4752027167417259, and the loss is 0.48106509673267234\n",
      "best k is 0.7791700533419829, best b is 0.4752027167417259, and the loss is 0.4810293905912615\n",
      "best k is 0.7795221796432839, best b is 0.4739314290649584, and the loss is 0.4810293905912615\n",
      "best k is 0.7795221796432839, best b is 0.4739314290649584, and the loss is 0.4809948814571232\n",
      "best k is 0.7798683533161841, best b is 0.4726816322654774, and the loss is 0.4809948814571232\n",
      "best k is 0.7798683533161841, best b is 0.4726816322654774, and the loss is 0.48096152920196655\n",
      "best k is 0.7802086749887385, best b is 0.4714529630440744, and the loss is 0.48096152920196655\n",
      "best k is 0.7802086749887385, best b is 0.4714529630440744, and the loss is 0.48092929504275556\n",
      "best k is 0.7805432435879038, best b is 0.4702450642430454, and the loss is 0.48092929504275556\n",
      "best k is 0.7805432435879038, best b is 0.4702450642430454, and the loss is 0.48089814149661175\n",
      "best k is 0.780872156368296, best b is 0.46905758474236975, and the loss is 0.48089814149661175\n",
      "best k is 0.780872156368296, best b is 0.46905758474236975, and the loss is 0.4808680323372266\n",
      "best k is 0.7811955089404595, best b is 0.467890179357644, and the loss is 0.4808680323372266\n",
      "best k is 0.7811955089404595, best b is 0.467890179357644, and the loss is 0.4808389325527364\n",
      "best k is 0.7815133952986608, best b is 0.4667425087397417, and the loss is 0.4808389325527364\n",
      "best k is 0.7815133952986608, best b is 0.4667425087397417, and the loss is 0.4808108083050101\n",
      "best k is 0.7818259078482114, best b is 0.4656142392761693, and the loss is 0.4808108083050101\n",
      "best k is 0.7818259078482114, best b is 0.4656142392761693, and the loss is 0.4807836268903015\n",
      "best k is 0.782133137432328, best b is 0.46450504299408896, and the loss is 0.4807836268903015\n",
      "best k is 0.782133137432328, best b is 0.46450504299408896, and the loss is 0.48075735670121916\n",
      "best k is 0.7824351733585405, best b is 0.46341459746498165, and the loss is 0.48075735670121916\n",
      "best k is 0.7824351733585405, best b is 0.46341459746498165, and the loss is 0.48073196718997363\n",
      "best k is 0.7827321034246515, best b is 0.46234258571092135, and the loss is 0.48073196718997363\n",
      "best k is 0.7827321034246515, best b is 0.46234258571092135, and the loss is 0.4807074288328538\n",
      "best k is 0.7830240139442585, best b is 0.4612886961124338, and the loss is 0.4807074288328538\n",
      "best k is 0.7830240139442585, best b is 0.4612886961124338, and the loss is 0.48068371309589875\n",
      "best k is 0.7833109897718441, best b is 0.46025262231791286, and the loss is 0.48068371309589875\n",
      "best k is 0.7833109897718441, best b is 0.46025262231791286, and the loss is 0.48066079240171433\n",
      "best k is 0.7835931143274417, best b is 0.45923406315456833, and the loss is 0.48066079240171433\n",
      "best k is 0.7835931143274417, best b is 0.45923406315456833, and the loss is 0.4806386400974073\n",
      "best k is 0.7838704696208854, best b is 0.458232722540879, and the loss is 0.4806386400974073\n",
      "best k is 0.7838704696208854, best b is 0.458232722540879, and the loss is 0.4806172304235914\n",
      "best k is 0.7841431362756477, best b is 0.45724830940052547, and the loss is 0.4806172304235914\n",
      "best k is 0.7841431362756477, best b is 0.45724830940052547, and the loss is 0.4805965384844355\n",
      "best k is 0.7844111935522775, best b is 0.4562805375777786, and the loss is 0.4805965384844355\n",
      "best k is 0.7844111935522775, best b is 0.4562805375777786, and the loss is 0.4805765402187112\n",
      "best k is 0.7846747193714386, best b is 0.4553291257543175, and the loss is 0.4805765402187112\n",
      "best k is 0.7846747193714386, best b is 0.4553291257543175, and the loss is 0.48055721237181553\n",
      "best k is 0.7849337903365609, best b is 0.45439379736745417, and the loss is 0.48055721237181553\n",
      "best k is 0.7849337903365609, best b is 0.45439379736745417, and the loss is 0.4805385324687294\n",
      "best k is 0.7851884817561077, best b is 0.4534742805297405, and the loss is 0.4805385324687294\n",
      "best k is 0.7851884817561077, best b is 0.4534742805297405, and the loss is 0.4805204787878825\n",
      "best k is 0.785438867665467, best b is 0.45257030794993414, and the loss is 0.4805204787878825\n",
      "best k is 0.785438867665467, best b is 0.45257030794993414, and the loss is 0.4805030303358956\n",
      "best k is 0.785685020848473, best b is 0.4516816168553006, and the loss is 0.4805030303358956\n",
      "best k is 0.785685020848473, best b is 0.4516816168553006, and the loss is 0.48048616682316814\n",
      "best k is 0.7859270128585626, best b is 0.4508079489152287, and the loss is 0.48048616682316814\n",
      "best k is 0.7859270128585626, best b is 0.4508079489152287, and the loss is 0.4804698686402854\n",
      "best k is 0.7861649140395751, best b is 0.44994905016613707, and the loss is 0.4804698686402854\n",
      "best k is 0.7861649140395751, best b is 0.44994905016613707, and the loss is 0.4804541168352149\n",
      "best k is 0.7863987935462013, best b is 0.44910467093765083, and the loss is 0.4804541168352149\n",
      "best k is 0.7863987935462013, best b is 0.44910467093765083, and the loss is 0.48043889309126947\n",
      "best k is 0.7866287193640846, best b is 0.44827456578002534, and the loss is 0.48043889309126947\n",
      "best k is 0.7866287193640846, best b is 0.44827456578002534, and the loss is 0.48042417970580836\n",
      "best k is 0.786854758329584, best b is 0.4474584933927974, and the loss is 0.48042417970580836\n",
      "best k is 0.786854758329584, best b is 0.4474584933927974, and the loss is 0.48040995956965116\n",
      "best k is 0.7870769761492024, best b is 0.4466562165546425, and the loss is 0.48040995956965116\n",
      "best k is 0.7870769761492024, best b is 0.4466562165546425, and the loss is 0.48039621614718336\n",
      "best k is 0.787295437418687, best b is 0.44586750205441755, and the loss is 0.48039621614718336\n",
      "best k is 0.787295437418687, best b is 0.44586750205441755, and the loss is 0.48038293345712635\n",
      "best k is 0.7875102056418061, best b is 0.4450921206233697, and the loss is 0.48038293345712635\n",
      "best k is 0.7875102056418061, best b is 0.4450921206233697, and the loss is 0.48037009605395786\n",
      "best k is 0.7877213432488085, best b is 0.4443298468684909, and the loss is 0.48037009605395786\n",
      "best k is 0.7877213432488085, best b is 0.4443298468684909, and the loss is 0.4803576890099469\n",
      "best k is 0.7879289116145718, best b is 0.4435804592069993, and the loss is 0.4803576890099469\n",
      "best k is 0.7879289116145718, best b is 0.4435804592069993, and the loss is 0.48034569789779863\n",
      "best k is 0.7881329710764431, best b is 0.4428437398019278, and the loss is 0.48034569789779863\n",
      "best k is 0.7881329710764431, best b is 0.4428437398019278, and the loss is 0.4803341087738763\n",
      "best k is 0.7883335809517773, best b is 0.4421194744988021, and the loss is 0.4803341087738763\n",
      "best k is 0.7883335809517773, best b is 0.4421194744988021, and the loss is 0.4803229081619877\n",
      "best k is 0.7885307995551817, best b is 0.4414074527633887, and the loss is 0.4803229081619877\n",
      "best k is 0.7885307995551817, best b is 0.4414074527633887, and the loss is 0.480312083037714\n",
      "best k is 0.7887246842154652, best b is 0.44070746762049534, and the loss is 0.480312083037714\n",
      "best k is 0.7887246842154652, best b is 0.44070746762049534, and the loss is 0.48030162081326583\n",
      "best k is 0.788915291292305, best b is 0.44001931559380625, and the loss is 0.48030162081326583\n",
      "best k is 0.788915291292305, best b is 0.44001931559380625, and the loss is 0.48029150932284337\n",
      "best k is 0.7891026761926276, best b is 0.43934279664673415, and the loss is 0.48029150932284337\n",
      "best k is 0.7891026761926276, best b is 0.43934279664673415, and the loss is 0.4802817368084937\n",
      "best k is 0.789286893386717, best b is 0.4386777141242724, and the loss is 0.4802817368084937\n",
      "best k is 0.789286893386717, best b is 0.4386777141242724, and the loss is 0.4802722919064335\n",
      "best k is 0.7894679964240465, best b is 0.4380238746958301, and the loss is 0.4802722919064335\n",
      "best k is 0.7894679964240465, best b is 0.4380238746958301, and the loss is 0.48026316363383864\n",
      "best k is 0.7896460379488464, best b is 0.4373810882990331, and the loss is 0.48026316363383864\n",
      "best k is 0.7896460379488464, best b is 0.4373810882990331, and the loss is 0.4802543413760702\n",
      "best k is 0.7898210697154054, best b is 0.4367491680844759, and the loss is 0.4802543413760702\n",
      "best k is 0.7898210697154054, best b is 0.4367491680844759, and the loss is 0.4802458148743338\n",
      "best k is 0.7899931426031167, best b is 0.4361279303614067, and the loss is 0.4802458148743338\n",
      "best k is 0.7899931426031167, best b is 0.4361279303614067, and the loss is 0.4802375742137488\n",
      "best k is 0.7901623066312663, best b is 0.435517194544331, and the loss is 0.4802375742137488\n",
      "best k is 0.7901623066312663, best b is 0.435517194544331, and the loss is 0.4802296098118207\n",
      "best k is 0.7903286109735741, best b is 0.434916783100518, and the loss is 0.4802296098118207\n",
      "best k is 0.7903286109735741, best b is 0.434916783100518, and the loss is 0.4802219124072953\n",
      "best k is 0.7904921039724873, best b is 0.43432652149839396, and the loss is 0.4802219124072953\n",
      "best k is 0.7904921039724873, best b is 0.43432652149839396, and the loss is 0.48021447304939224\n",
      "best k is 0.7906528331532331, best b is 0.4337462381568084, and the loss is 0.48021447304939224\n",
      "best k is 0.7906528331532331, best b is 0.4337462381568084, and the loss is 0.4802072830873955\n",
      "best k is 0.7908108452376341, best b is 0.43317576439515765, and the loss is 0.4802072830873955\n",
      "best k is 0.7908108452376341, best b is 0.43317576439515765, and the loss is 0.4802003341605956\n",
      "best k is 0.7909661861576893, best b is 0.43261493438435167, and the loss is 0.4802003341605956\n",
      "best k is 0.7909661861576893, best b is 0.43261493438435167, and the loss is 0.48019361818856393\n",
      "best k is 0.7911189010689256, best b is 0.4320635850986097, and the loss is 0.48019361818856393\n",
      "best k is 0.7911189010689256, best b is 0.4320635850986097, and the loss is 0.4801871273617606\n",
      "best k is 0.7912690343635245, best b is 0.43152155626807104, and the loss is 0.4801871273617606\n",
      "best k is 0.7912690343635245, best b is 0.43152155626807104, and the loss is 0.4801808541324514\n",
      "best k is 0.7914166296832262, best b is 0.4309886903322066, and the loss is 0.4801808541324514\n",
      "best k is 0.7914166296832262, best b is 0.4309886903322066, and the loss is 0.48017479120593015\n",
      "best k is 0.7915617299320155, best b is 0.43046483239401806, and the loss is 0.48017479120593015\n",
      "best k is 0.7915617299320155, best b is 0.43046483239401806, and the loss is 0.48016893153203866\n",
      "best k is 0.7917043772885931, best b is 0.42994983017501165, and the loss is 0.48016893153203866\n",
      "best k is 0.7917043772885931, best b is 0.42994983017501165, and the loss is 0.4801632682969664\n",
      "best k is 0.7918446132186372, best b is 0.42944353397093254, and the loss is 0.4801632682969664\n",
      "best k is 0.7918446132186372, best b is 0.42944353397093254, and the loss is 0.48015779491532856\n",
      "best k is 0.7919824784868565, best b is 0.42894579660824816, and the loss is 0.48015779491532856\n",
      "best k is 0.7919824784868565, best b is 0.42894579660824816, and the loss is 0.480152505022507\n",
      "best k is 0.7921180131688399, best b is 0.4284564734013664, and the loss is 0.480152505022507\n",
      "best k is 0.7921180131688399, best b is 0.4284564734013664, and the loss is 0.4801473924672502\n",
      "best k is 0.7922512566627061, best b is 0.42797542211057776, and the loss is 0.4801473924672502\n",
      "best k is 0.7922512566627061, best b is 0.42797542211057776, and the loss is 0.4801424513045209\n",
      "best k is 0.7923822477005561, best b is 0.4275025029007082, and the loss is 0.4801424513045209\n",
      "best k is 0.7923822477005561, best b is 0.4275025029007082, and the loss is 0.4801376757885822\n",
      "best k is 0.792511024359732, best b is 0.42703757830047057, and the loss is 0.4801376757885822\n",
      "best k is 0.792511024359732, best b is 0.42703757830047057, and the loss is 0.48013306036631526\n",
      "best k is 0.7926376240738856, best b is 0.4265805131625039, and the loss is 0.48013306036631526\n",
      "best k is 0.7926376240738856, best b is 0.4265805131625039, and the loss is 0.4801285996707657\n",
      "best k is 0.7927620836438602, best b is 0.42613117462408784, and the loss is 0.4801285996707657\n",
      "best k is 0.7927620836438602, best b is 0.42613117462408784, and the loss is 0.48012428851489686\n",
      "best k is 0.7928844392483876, best b is 0.425689432068521, and the loss is 0.48012428851489686\n",
      "best k is 0.7928844392483876, best b is 0.425689432068521, and the loss is 0.480120121885564\n",
      "best k is 0.793004726454605, best b is 0.4252551570871526, and the loss is 0.480120121885564\n",
      "best k is 0.793004726454605, best b is 0.4252551570871526, and the loss is 0.4801160949376815\n",
      "best k is 0.7931229802283937, best b is 0.42482822344205584, and the loss is 0.4801160949376815\n",
      "best k is 0.7931229802283937, best b is 0.42482822344205584, and the loss is 0.480112202988589\n",
      "best k is 0.7932392349445438, best b is 0.42440850702933214, and the loss is 0.480112202988589\n",
      "best k is 0.7932392349445438, best b is 0.42440850702933214, and the loss is 0.4801084415126084\n",
      "best k is 0.793353524396746, best b is 0.42399588584303577, and the loss is 0.4801084415126084\n",
      "best k is 0.793353524396746, best b is 0.42399588584303577, and the loss is 0.4801048061357777\n",
      "best k is 0.7934658818074146, best b is 0.4235902399397084, and the loss is 0.4801048061357777\n",
      "best k is 0.7934658818074146, best b is 0.4235902399397084, and the loss is 0.4801012926307696\n",
      "best k is 0.7935763398373461, best b is 0.42319145140351316, and the loss is 0.4801012926307696\n",
      "best k is 0.7935763398373461, best b is 0.42319145140351316, and the loss is 0.48009789691197025\n",
      "best k is 0.7936849305952114, best b is 0.422799404311958, and the loss is 0.48009789691197025\n",
      "best k is 0.7936849305952114, best b is 0.422799404311958, and the loss is 0.48009461503073336\n",
      "best k is 0.7937916856468915, best b is 0.42241398470219876, and the loss is 0.48009461503073336\n",
      "best k is 0.7937916856468915, best b is 0.42241398470219876, and the loss is 0.4800914431707855\n",
      "best k is 0.7938966360246512, best b is 0.42203508053791144, and the loss is 0.4800914431707855\n",
      "best k is 0.7938966360246512, best b is 0.42203508053791144, and the loss is 0.48008837764378975\n",
      "best k is 0.7939998122361615, best b is 0.42166258167672493, and the loss is 0.48008837764378975\n",
      "best k is 0.7939998122361615, best b is 0.42166258167672493, and the loss is 0.4800854148850565\n",
      "best k is 0.7941012442733664, best b is 0.421296379838204, and the loss is 0.4800854148850565\n",
      "best k is 0.7941012442733664, best b is 0.421296379838204, and the loss is 0.4800825514493973\n",
      "best k is 0.7942009616212021, best b is 0.4209363685723737, and the loss is 0.4800825514493973\n",
      "best k is 0.7942009616212021, best b is 0.4209363685723737, and the loss is 0.4800797840071215\n",
      "best k is 0.7942989932661676, best b is 0.4205824432287757, and the loss is 0.4800797840071215\n",
      "best k is 0.7942989932661676, best b is 0.4205824432287757, and the loss is 0.4800771093401612\n",
      "best k is 0.7943953677047506, best b is 0.42023450092604786, and the loss is 0.4800771093401612\n",
      "best k is 0.7943953677047506, best b is 0.42023450092604786, and the loss is 0.4800745243383303\n",
      "best k is 0.7944901129517106, best b is 0.4198924405220179, and the loss is 0.4800745243383303\n",
      "best k is 0.7944901129517106, best b is 0.4198924405220179, and the loss is 0.4800720259957091\n",
      "best k is 0.7945832565482236, best b is 0.41955616258430295, and the loss is 0.4800720259957091\n",
      "best k is 0.7945832565482236, best b is 0.41955616258430295, and the loss is 0.48006961140714727\n",
      "best k is 0.7946748255698868, best b is 0.4192255693614056, and the loss is 0.48006961140714727\n",
      "best k is 0.7946748255698868, best b is 0.4192255693614056, and the loss is 0.4800672777648862\n",
      "best k is 0.7947648466345896, best b is 0.418900564754299, and the loss is 0.4800672777648862\n",
      "best k is 0.7947648466345896, best b is 0.418900564754299, and the loss is 0.4800650223552946\n",
      "best k is 0.7948533459102513, best b is 0.4185810542884922, and the loss is 0.4800650223552946\n",
      "best k is 0.7948533459102513, best b is 0.4185810542884922, and the loss is 0.48006284255571224\n",
      "best k is 0.7949403491224273, best b is 0.4182669450865676, and the loss is 0.48006284255571224\n",
      "best k is 0.7949403491224273, best b is 0.4182669450865676, and the loss is 0.4800607358314009\n",
      "best k is 0.795025881561787, best b is 0.4179581458411826, and the loss is 0.4800607358314009\n",
      "best k is 0.795025881561787, best b is 0.4179581458411826, and the loss is 0.48005869973259585\n",
      "best k is 0.7951099680914665, best b is 0.41765456678852825, and the loss is 0.48005869973259585\n",
      "best k is 0.7951099680914665, best b is 0.41765456678852825, and the loss is 0.4800567318916585\n",
      "best k is 0.7951926331542949, best b is 0.41735611968223546, and the loss is 0.4800567318916585\n",
      "best k is 0.7951926331542949, best b is 0.41735611968223546, and the loss is 0.4800548300203224\n",
      "best k is 0.7952739007798999, best b is 0.41706271776772347, and the loss is 0.4800548300203224\n",
      "best k is 0.7952739007798999, best b is 0.41706271776772347, and the loss is 0.48005299190703254\n",
      "best k is 0.795353794591693, best b is 0.41677427575698117, and the loss is 0.48005299190703254\n",
      "best k is 0.795353794591693, best b is 0.41677427575698117, and the loss is 0.48005121541437407\n",
      "best k is 0.7954323378137363, best b is 0.41649070980377517, and the loss is 0.48005121541437407\n",
      "best k is 0.7954323378137363, best b is 0.41649070980377517, and the loss is 0.4800494984765863\n",
      "best k is 0.7955095532774937, best b is 0.41621193747927676, and the loss is 0.4800494984765863\n",
      "best k is 0.7955095532774937, best b is 0.41621193747927676, and the loss is 0.48004783909716053\n",
      "best k is 0.7955854634284676, best b is 0.415937877748101, and the loss is 0.48004783909716053\n",
      "best k is 0.7955854634284676, best b is 0.415937877748101, and the loss is 0.4800462353465192\n",
      "best k is 0.795660090332723, best b is 0.41566845094475063, and the loss is 0.4800462353465192\n",
      "best k is 0.795660090332723, best b is 0.41566845094475063, and the loss is 0.48004468535977096\n",
      "best k is 0.7957334556833024, best b is 0.41540357875045864, and the loss is 0.48004468535977096\n",
      "best k is 0.7957334556833024, best b is 0.41540357875045864, and the loss is 0.4800431873345438\n",
      "best k is 0.7958055808065321, best b is 0.415143184170422, and the loss is 0.4800431873345438\n",
      "best k is 0.7958055808065321, best b is 0.415143184170422, and the loss is 0.4800417395288873\n",
      "best k is 0.7958764866682202, best b is 0.4148871915114202, and the loss is 0.4800417395288873\n",
      "best k is 0.7958764866682202, best b is 0.4148871915114202, and the loss is 0.48004034025924813\n",
      "best k is 0.7959461938797519, best b is 0.4146355263598121, and the loss is 0.48004034025924813\n",
      "best k is 0.7959461938797519, best b is 0.4146355263598121, and the loss is 0.480038987898512\n",
      "best k is 0.7960147227040811, best b is 0.41438811555990535, and the loss is 0.480038987898512\n",
      "best k is 0.7960147227040811, best b is 0.41438811555990535, and the loss is 0.48003768087411275\n",
      "best k is 0.7960820930616204, best b is 0.4141448871926905, and the loss is 0.48003768087411275\n",
      "best k is 0.7960820930616204, best b is 0.4141448871926905, and the loss is 0.48003641766620075\n",
      "best k is 0.7961483245360308, best b is 0.4139057705549353, and the loss is 0.48003641766620075\n",
      "best k is 0.7961483245360308, best b is 0.4139057705549353, and the loss is 0.4800351968058797\n",
      "best k is 0.7962134363799164, best b is 0.41367069613863255, and the loss is 0.4800351968058797\n",
      "best k is 0.7962134363799164, best b is 0.41367069613863255, and the loss is 0.48003401687349456\n",
      "best k is 0.7962774475204185, best b is 0.41343959561079435, and the loss is 0.48003401687349456\n",
      "best k is 0.7962774475204185, best b is 0.41343959561079435, and the loss is 0.480032876496984\n",
      "best k is 0.7963403765647198, best b is 0.41321240179358937, and the loss is 0.480032876496984\n",
      "best k is 0.7963403765647198, best b is 0.41321240179358937, and the loss is 0.4800317743502827\n",
      "best k is 0.7964022418054512, best b is 0.4129890486448145, and the loss is 0.4800317743502827\n",
      "best k is 0.7964022418054512, best b is 0.4129890486448145, and the loss is 0.4800307091517803\n",
      "best k is 0.7964630612260105, best b is 0.41276947123869767, and the loss is 0.4800307091517803\n",
      "best k is 0.7964630612260105, best b is 0.41276947123869767, and the loss is 0.4800296796628315\n",
      "best k is 0.7965228525057896, best b is 0.41255360574702477, and the loss is 0.4800296796628315\n",
      "best k is 0.7965228525057896, best b is 0.41255360574702477, and the loss is 0.48002868468631393\n",
      "best k is 0.7965816330253136, best b is 0.4123413894205854, and the loss is 0.48002868468631393\n",
      "best k is 0.7965816330253136, best b is 0.4123413894205854, and the loss is 0.480027723065238\n",
      "best k is 0.7966394198712929, best b is 0.4121327605709328, and the loss is 0.480027723065238\n",
      "best k is 0.7966394198712929, best b is 0.4121327605709328, and the loss is 0.4800267936814012\n",
      "best k is 0.7966962298415908, best b is 0.4119276585524516, and the loss is 0.4800267936814012\n",
      "best k is 0.7966962298415908, best b is 0.4119276585524516, and the loss is 0.4800258954540878\n",
      "best k is 0.7967520794501054, best b is 0.4117260237447292, and the loss is 0.4800258954540878\n",
      "best k is 0.7967520794501054, best b is 0.4117260237447292, and the loss is 0.48002502733881014\n",
      "best k is 0.7968069849315706, best b is 0.41152779753522467, and the loss is 0.48002502733881014\n",
      "best k is 0.7968069849315706, best b is 0.41152779753522467, and the loss is 0.4800241883260979\n",
      "best k is 0.7968609622462756, best b is 0.411332922302231, and the loss is 0.4800241883260979\n",
      "best k is 0.7968609622462756, best b is 0.411332922302231, and the loss is 0.48002337744032025\n",
      "best k is 0.7969140270847032, best b is 0.41114134139812525, and the loss is 0.48002337744032025\n",
      "best k is 0.7969140270847032, best b is 0.41114134139812525, and the loss is 0.4800225937385547\n",
      "best k is 0.7969661948720921, best b is 0.4109529991329018, and the loss is 0.4800225937385547\n",
      "best k is 0.7969661948720921, best b is 0.4109529991329018, and the loss is 0.4800218363094884\n",
      "best k is 0.7970174807729202, best b is 0.410767840757984, and the loss is 0.4800218363094884\n",
      "best k is 0.7970174807729202, best b is 0.410767840757984, and the loss is 0.48002110427235917\n",
      "best k is 0.7970678996953128, best b is 0.41058581245030956, and the loss is 0.48002110427235917\n",
      "best k is 0.7970678996953128, best b is 0.41058581245030956, and the loss is 0.48002039677593167\n",
      "best k is 0.7971174662953758, best b is 0.41040686129668474, and the loss is 0.48002039677593167\n",
      "best k is 0.7971174662953758, best b is 0.41040686129668474, and the loss is 0.48001971299750734\n",
      "best k is 0.7971661949814569, best b is 0.4102309352784035, and the loss is 0.48001971299750734\n",
      "best k is 0.7971661949814569, best b is 0.4102309352784035, and the loss is 0.48001905214196705\n",
      "best k is 0.7972140999183333, best b is 0.4100579832561261, and the loss is 0.48001905214196705\n",
      "best k is 0.7972140999183333, best b is 0.4100579832561261, and the loss is 0.4800184134408479\n",
      "best k is 0.7972611950313289, best b is 0.4098879549550135, and the loss is 0.4800184134408479\n",
      "best k is 0.7972611950313289, best b is 0.4098879549550135, and the loss is 0.48001779615144846\n",
      "best k is 0.797307494010363, best b is 0.4097208009501135, and the loss is 0.48001779615144846\n",
      "best k is 0.797307494010363, best b is 0.4097208009501135, and the loss is 0.48001719955596467\n",
      "best k is 0.7973530103139297, best b is 0.40955647265199324, and the loss is 0.48001719955596467\n",
      "best k is 0.7973530103139297, best b is 0.40955647265199324, and the loss is 0.4800166229606579\n",
      "best k is 0.7973977571730091, best b is 0.409394922292615, and the loss is 0.4800166229606579\n",
      "best k is 0.7973977571730091, best b is 0.409394922292615, and the loss is 0.4800160656950446\n",
      "best k is 0.7974417475949146, best b is 0.40923610291145074, and the loss is 0.4800160656950446\n",
      "best k is 0.7974417475949146, best b is 0.40923610291145074, and the loss is 0.4800155271111194\n",
      "best k is 0.7974849943670733, best b is 0.4090799683418313, and the loss is 0.4800155271111194\n",
      "best k is 0.7974849943670733, best b is 0.4090799683418313, and the loss is 0.48001500658259993\n",
      "best k is 0.7975275100607433, best b is 0.4089264731975262, and the loss is 0.48001500658259993\n",
      "best k is 0.7975275100607433, best b is 0.4089264731975262, and the loss is 0.4800145035041993\n",
      "best k is 0.7975693070346678, best b is 0.4087755728595506, and the loss is 0.4800145035041993\n",
      "best k is 0.7975693070346678, best b is 0.4087755728595506, and the loss is 0.4800140172909234\n",
      "best k is 0.797610397438668, best b is 0.4086272234631952, and the loss is 0.4800140172909234\n",
      "best k is 0.797610397438668, best b is 0.4086272234631952, and the loss is 0.48001354737738716\n",
      "best k is 0.7976507932171746, best b is 0.4084813818852753, and the loss is 0.48001354737738716\n",
      "best k is 0.7976507932171746, best b is 0.4084813818852753, and the loss is 0.4800130932171613\n",
      "best k is 0.7976905061126999, best b is 0.40833800573159534, and the loss is 0.4800130932171613\n",
      "best k is 0.7976905061126999, best b is 0.40833800573159534, and the loss is 0.48001265428213385\n",
      "best k is 0.7977295476692514, best b is 0.40819705332462586, and the loss is 0.48001265428213385\n",
      "best k is 0.7977295476692514, best b is 0.40819705332462586, and the loss is 0.48001223006189786\n",
      "best k is 0.7977679292356871, best b is 0.40805848369138786, and the loss is 0.48001223006189786\n",
      "best k is 0.7977679292356871, best b is 0.40805848369138786, and the loss is 0.48001182006315685\n",
      "best k is 0.7978056619690149, best b is 0.40792225655154296, and the loss is 0.48001182006315685\n",
      "best k is 0.7978056619690149, best b is 0.40792225655154296, and the loss is 0.48001142380915107\n",
      "best k is 0.7978427568376356, best b is 0.4077883323056842, and the loss is 0.48001142380915107\n",
      "best k is 0.7978427568376356, best b is 0.4077883323056842, and the loss is 0.4800110408391052\n",
      "best k is 0.7978792246245312, best b is 0.40765667202382505, and the loss is 0.4800110408391052\n",
      "best k is 0.7978792246245312, best b is 0.40765667202382505, and the loss is 0.48001067070768855\n",
      "best k is 0.7979150759303993, best b is 0.4075272374340832, and the loss is 0.48001067070768855\n",
      "best k is 0.7979150759303993, best b is 0.4075272374340832, and the loss is 0.4800103129845013\n",
      "best k is 0.7979503211767351, best b is 0.4073999909115551, and the loss is 0.4800103129845013\n",
      "best k is 0.7979503211767351, best b is 0.4073999909115551, and the loss is 0.48000996725357237\n",
      "best k is 0.7979849706088599, best b is 0.40727489546737905, and the loss is 0.48000996725357237\n",
      "best k is 0.7979849706088599, best b is 0.40727489546737905, and the loss is 0.4800096331128748\n",
      "best k is 0.7980190342989003, best b is 0.4071519147379832, and the loss is 0.4800096331128748\n",
      "best k is 0.7980190342989003, best b is 0.4071519147379832, and the loss is 0.48000931017385895\n",
      "best k is 0.798052522148715, best b is 0.40703101297451477, and the loss is 0.48000931017385895\n",
      "best k is 0.798052522148715, best b is 0.40703101297451477, and the loss is 0.4800089980610015\n",
      "best k is 0.798085443892774, best b is 0.40691215503244876, and the loss is 0.4800089980610015\n",
      "best k is 0.798085443892774, best b is 0.40691215503244876, and the loss is 0.4800086964113687\n",
      "best k is 0.798117809100988, best b is 0.4067953063613717, and the loss is 0.4800086964113687\n",
      "best k is 0.798117809100988, best b is 0.4067953063613717, and the loss is 0.48000840487419205\n",
      "best k is 0.7981496271814896, best b is 0.4066804329949381, and the loss is 0.48000840487419205\n",
      "best k is 0.7981496271814896, best b is 0.4066804329949381, and the loss is 0.480008123110464\n",
      "best k is 0.7981809073833696, best b is 0.4065675015409974, and the loss is 0.480008123110464\n",
      "best k is 0.7981809073833696, best b is 0.4065675015409974, and the loss is 0.48000785079254005\n",
      "best k is 0.7982116587993638, best b is 0.4064564791718868, and the loss is 0.48000785079254005\n",
      "best k is 0.7982116587993638, best b is 0.4064564791718868, and the loss is 0.4800075876037611\n",
      "best k is 0.7982418903684976, best b is 0.40634733361488895, and the loss is 0.4800075876037611\n",
      "best k is 0.7982418903684976, best b is 0.40634733361488895, and the loss is 0.4800073332380827\n",
      "best k is 0.7982716108786836, best b is 0.4062400331428508, and the loss is 0.4800073332380827\n",
      "best k is 0.7982716108786836, best b is 0.4062400331428508, and the loss is 0.480007087399721\n",
      "best k is 0.7983008289692765, best b is 0.4061345465649606, and the loss is 0.480007087399721\n",
      "best k is 0.7983008289692765, best b is 0.4061345465649606, and the loss is 0.480006849802807\n",
      "best k is 0.7983295531335842, best b is 0.4060308432176816, and the loss is 0.480006849802807\n",
      "best k is 0.7983295531335842, best b is 0.4060308432176816, and the loss is 0.4800066201710562\n",
      "best k is 0.7983577917213371, best b is 0.40592889295583817, and the loss is 0.4800066201710562\n",
      "best k is 0.7983577917213371, best b is 0.40592889295583817, and the loss is 0.48000639823744506\n",
      "best k is 0.7983855529411149, best b is 0.4058286661438532, and the loss is 0.48000639823744506\n",
      "best k is 0.7983855529411149, best b is 0.4058286661438532, and the loss is 0.4800061837439028\n",
      "best k is 0.7984128448627326, best b is 0.40573013364713345, and the loss is 0.4800061837439028\n",
      "best k is 0.7984128448627326, best b is 0.40573013364713345, and the loss is 0.4800059764410096\n",
      "best k is 0.7984396754195867, best b is 0.40563326682360035, and the loss is 0.4800059764410096\n",
      "best k is 0.7984396754195867, best b is 0.40563326682360035, and the loss is 0.4800057760877074\n",
      "best k is 0.7984660524109612, best b is 0.40553803751536427, and the loss is 0.4800057760877074\n",
      "best k is 0.7984660524109612, best b is 0.40553803751536427, and the loss is 0.4800055824510185\n",
      "best k is 0.7984919835042946, best b is 0.4054444180405395, and the loss is 0.4800055824510185\n",
      "best k is 0.7984919835042946, best b is 0.4054444180405395, and the loss is 0.4800053953057769\n",
      "best k is 0.7985174762374087, best b is 0.4053523811851972, and the loss is 0.4800053953057769\n",
      "best k is 0.7985174762374087, best b is 0.4053523811851972, and the loss is 0.480005214434364\n",
      "best k is 0.7985425380207, best b is 0.4052619001954549, and the loss is 0.480005214434364\n",
      "best k is 0.7985425380207, best b is 0.4052619001954549, and the loss is 0.48000503962645685\n",
      "best k is 0.7985671761392936, best b is 0.4051729487696994, and the loss is 0.48000503962645685\n",
      "best k is 0.7985671761392936, best b is 0.4051729487696994, and the loss is 0.4800048706787832\n",
      "best k is 0.7985913977551607, best b is 0.4050855010509414, and the loss is 0.4800048706787832\n",
      "best k is 0.7985913977551607, best b is 0.4050855010509414, and the loss is 0.48000470739488577\n",
      "best k is 0.7986152099092014, best b is 0.404999531619299, and the loss is 0.48000470739488577\n",
      "best k is 0.7986152099092014, best b is 0.404999531619299, and the loss is 0.48000454958489336\n",
      "best k is 0.7986386195232902, best b is 0.4049150154846087, and the loss is 0.48000454958489336\n",
      "best k is 0.7986386195232902, best b is 0.4049150154846087, and the loss is 0.4800043970652991\n",
      "best k is 0.7986616334022884, best b is 0.40483192807916074, and the loss is 0.4800043970652991\n",
      "best k is 0.7986616334022884, best b is 0.40483192807916074, and the loss is 0.4800042496587486\n",
      "best k is 0.798684258236023, best b is 0.40475024525055814, and the loss is 0.4800042496587486\n",
      "best k is 0.798684258236023, best b is 0.40475024525055814, and the loss is 0.4800041071938329\n",
      "best k is 0.7987065006012303, best b is 0.40466994325469546, and the loss is 0.4800041071938329\n",
      "best k is 0.7987065006012303, best b is 0.40466994325469546, and the loss is 0.4800039695048893\n",
      "best k is 0.7987283669634683, best b is 0.40459099874885684, and the loss is 0.4800039695048893\n",
      "best k is 0.7987283669634683, best b is 0.40459099874885684, and the loss is 0.4800038364318091\n",
      "best k is 0.7987498636789961, best b is 0.40451338878493065, and the loss is 0.4800038364318091\n",
      "best k is 0.7987498636789961, best b is 0.40451338878493065, and the loss is 0.4800037078198506\n",
      "best k is 0.7987709969966211, best b is 0.4044370908027387, and the loss is 0.4800037078198506\n",
      "best k is 0.7987709969966211, best b is 0.4044370908027387, and the loss is 0.48000358351946043\n",
      "best k is 0.7987917730595163, best b is 0.4043620826234785, and the loss is 0.48000358351946043\n",
      "best k is 0.7987917730595163, best b is 0.4043620826234785, and the loss is 0.4800034633860972\n",
      "best k is 0.7988121979070049, best b is 0.40428834244327577, and the loss is 0.4800034633860972\n",
      "best k is 0.7988121979070049, best b is 0.40428834244327577, and the loss is 0.48000334728006644\n",
      "best k is 0.7988322774763168, best b is 0.40421584882684675, and the loss is 0.48000334728006644\n",
      "best k is 0.7988322774763168, best b is 0.40421584882684675, and the loss is 0.4800032350663571\n",
      "best k is 0.7988520176043143, best b is 0.404144580701267, and the loss is 0.4800032350663571\n",
      "best k is 0.7988520176043143, best b is 0.404144580701267, and the loss is 0.4800031266144827\n",
      "best k is 0.7988714240291884, best b is 0.404074517349846, and the loss is 0.4800031266144827\n",
      "best k is 0.7988714240291884, best b is 0.404074517349846, and the loss is 0.480003021798333\n",
      "best k is 0.7988905023921273, best b is 0.40400563840610487, and the loss is 0.480003021798333\n",
      "best k is 0.7988905023921273, best b is 0.40400563840610487, and the loss is 0.48000292049602383\n",
      "best k is 0.7989092582389559, best b is 0.4039379238478562, and the loss is 0.48000292049602383\n",
      "best k is 0.7989092582389559, best b is 0.4039379238478562, and the loss is 0.4800028225897576\n",
      "best k is 0.7989276970217476, best b is 0.40387135399138385, and the loss is 0.4800028225897576\n",
      "best k is 0.7989276970217476, best b is 0.40387135399138385, and the loss is 0.4800027279656865\n",
      "best k is 0.7989458241004102, best b is 0.4038059094857212, and the loss is 0.4800027279656865\n",
      "best k is 0.7989458241004102, best b is 0.4038059094857212, and the loss is 0.4800026365137784\n",
      "best k is 0.7989636447442426, best b is 0.40374157130702604, and the loss is 0.4800026365137784\n",
      "best k is 0.7989636447442426, best b is 0.40374157130702604, and the loss is 0.4800025481276903\n",
      "best k is 0.7989811641334679, best b is 0.4036783207530506, and the loss is 0.4800025481276903\n",
      "best k is 0.7989811641334679, best b is 0.4036783207530506, and the loss is 0.4800024627046434\n",
      "best k is 0.798998387360738, best b is 0.4036161394377052, and the loss is 0.4800024627046434\n",
      "best k is 0.798998387360738, best b is 0.4036161394377052, and the loss is 0.48000238014530633\n",
      "best k is 0.7990153194326146, best b is 0.40355500928571325, and the loss is 0.48000238014530633\n",
      "best k is 0.7990153194326146, best b is 0.40355500928571325, and the loss is 0.48000230035367614\n",
      "best k is 0.7990319652710245, best b is 0.40349491252735753, and the loss is 0.48000230035367614\n",
      "best k is 0.7990319652710245, best b is 0.40349491252735753, and the loss is 0.4800022232369682\n",
      "best k is 0.7990483297146903, best b is 0.40343583169331443, and the loss is 0.4800022232369682\n",
      "best k is 0.7990483297146903, best b is 0.40343583169331443, and the loss is 0.48000214870550917\n",
      "best k is 0.7990644175205366, best b is 0.4033777496095759, and the loss is 0.48000214870550917\n",
      "best k is 0.7990644175205366, best b is 0.4033777496095759, and the loss is 0.48000207667263106\n",
      "best k is 0.7990802333650735, best b is 0.4033206493924573, and the loss is 0.48000207667263106\n",
      "best k is 0.7990802333650735, best b is 0.4033206493924573, and the loss is 0.48000200705457274\n",
      "best k is 0.7990957818457554, best b is 0.40326451444368955, and the loss is 0.48000200705457274\n",
      "best k is 0.7990957818457554, best b is 0.40326451444368955, and the loss is 0.4800019397703793\n",
      "best k is 0.7991110674823176, best b is 0.403209328445594, and the loss is 0.4800019397703793\n",
      "best k is 0.7991110674823176, best b is 0.403209328445594, and the loss is 0.4800018747418113\n",
      "best k is 0.79912609471809, best b is 0.4031550753563393, and the loss is 0.4800018747418113\n",
      "best k is 0.79912609471809, best b is 0.4031550753563393, and the loss is 0.4800018118932511\n",
      "best k is 0.7991408679212892, best b is 0.4031017394052784, and the loss is 0.4800018118932511\n",
      "best k is 0.7991408679212892, best b is 0.4031017394052784, and the loss is 0.4800017511516164\n",
      "best k is 0.7991553913862876, best b is 0.4030493050883638, and the loss is 0.4800017511516164\n",
      "best k is 0.7991553913862876, best b is 0.4030493050883638, and the loss is 0.4800016924462748\n",
      "best k is 0.7991696693348621, best b is 0.4029977571636411, and the loss is 0.4800016924462748\n",
      "best k is 0.7991696693348621, best b is 0.4029977571636411, and the loss is 0.48000163570896276\n",
      "best k is 0.7991837059174215, best b is 0.4029470806468184, and the loss is 0.48000163570896276\n",
      "best k is 0.7991837059174215, best b is 0.4029470806468184, and the loss is 0.48000158087370315\n",
      "best k is 0.7991975052142123, best b is 0.4028972608069101, and the loss is 0.48000158087370315\n",
      "best k is 0.7991975052142123, best b is 0.4028972608069101, and the loss is 0.4800015278767331\n",
      "best k is 0.7992110712365057, best b is 0.40284828316195537, and the loss is 0.4800015278767331\n",
      "best k is 0.7992110712365057, best b is 0.40284828316195537, and the loss is 0.480001476656425\n",
      "best k is 0.7992244079277628, best b is 0.40280013347480814, and the loss is 0.480001476656425\n",
      "best k is 0.7992244079277628, best b is 0.40280013347480814, and the loss is 0.480001427153219\n",
      "best k is 0.7992375191647814, best b is 0.40275279774899847, and the loss is 0.480001427153219\n",
      "best k is 0.7992375191647814, best b is 0.40275279774899847, and the loss is 0.4800013793095507\n",
      "best k is 0.7992504087588224, best b is 0.4027062622246642, and the loss is 0.4800013793095507\n",
      "best k is 0.7992504087588224, best b is 0.4027062622246642, and the loss is 0.48000133306978665\n",
      "best k is 0.7992630804567185, best b is 0.40266051337455105, and the loss is 0.48000133306978665\n",
      "best k is 0.7992630804567185, best b is 0.40266051337455105, and the loss is 0.48000128838015693\n",
      "best k is 0.7992755379419629, best b is 0.4026155379000804, and the loss is 0.48000128838015693\n",
      "best k is 0.7992755379419629, best b is 0.4026155379000804, and the loss is 0.48000124518869614\n",
      "best k is 0.7992877848357797, best b is 0.4025713227274835, and the loss is 0.48000124518869614\n",
      "best k is 0.7992877848357797, best b is 0.4025713227274835, and the loss is 0.4800012034451787\n",
      "best k is 0.799299824698177, best b is 0.4025278550040013, and the loss is 0.4800012034451787\n",
      "best k is 0.799299824698177, best b is 0.4025278550040013, and the loss is 0.4800011631010651\n",
      "best k is 0.7993116610289819, best b is 0.40248512209414805, and the loss is 0.4800011631010651\n",
      "best k is 0.7993116610289819, best b is 0.40248512209414805, and the loss is 0.48000112410944124\n",
      "best k is 0.7993232972688574, best b is 0.4024431115760387, and the loss is 0.48000112410944124\n",
      "best k is 0.7993232972688574, best b is 0.4024431115760387, and the loss is 0.4800010864249666\n",
      "best k is 0.7993347368003026, best b is 0.4024018112377776, and the loss is 0.4800010864249666\n",
      "best k is 0.7993347368003026, best b is 0.4024018112377776, and the loss is 0.4800010500038211\n",
      "best k is 0.7993459829486365, best b is 0.40236120907390904, and the loss is 0.4800010500038211\n",
      "best k is 0.7993459829486365, best b is 0.40236120907390904, and the loss is 0.48000101480365176\n",
      "best k is 0.7993570389829636, best b is 0.40232129328192723, and the loss is 0.48000101480365176\n",
      "best k is 0.7993570389829636, best b is 0.40232129328192723, and the loss is 0.48000098078352826\n",
      "best k is 0.7993679081171254, best b is 0.4022820522588454, and the loss is 0.48000098078352826\n",
      "best k is 0.7993679081171254, best b is 0.4022820522588454, and the loss is 0.4800009479038896\n",
      "best k is 0.7993785935106338, best b is 0.4022434745978232, and the loss is 0.4800009479038896\n",
      "best k is 0.7993785935106338, best b is 0.4022434745978232, and the loss is 0.48000091612650325\n",
      "best k is 0.7993890982695896, best b is 0.40220554908485073, and the loss is 0.48000091612650325\n",
      "best k is 0.7993890982695896, best b is 0.40220554908485073, and the loss is 0.48000088541441766\n",
      "best k is 0.7993994254475858, best b is 0.4021682646954888, and the loss is 0.48000088541441766\n",
      "best k is 0.7993994254475858, best b is 0.4021682646954888, and the loss is 0.48000085573191864\n",
      "best k is 0.7994095780465947, best b is 0.40213161059166413, and the loss is 0.48000085573191864\n",
      "best k is 0.7994095780465947, best b is 0.40213161059166413, and the loss is 0.48000082704449154\n",
      "best k is 0.7994195590178412, best b is 0.4020955761185193, and the loss is 0.48000082704449154\n",
      "best k is 0.7994195590178412, best b is 0.4020955761185193, and the loss is 0.4800007993187775\n",
      "best k is 0.7994293712626601, best b is 0.402060150801315, and the loss is 0.4800007993187775\n",
      "best k is 0.7994293712626601, best b is 0.402060150801315, and the loss is 0.480000772522536\n",
      "best k is 0.7994390176333395, best b is 0.40202532434238547, and the loss is 0.480000772522536\n",
      "best k is 0.7994390176333395, best b is 0.40202532434238547, and the loss is 0.48000074662460757\n",
      "best k is 0.7994485009339505, best b is 0.4019910866181451, and the loss is 0.48000074662460757\n",
      "best k is 0.7994485009339505, best b is 0.4019910866181451, and the loss is 0.4800007215948776\n",
      "best k is 0.7994578239211614, best b is 0.40195742767614545, and the loss is 0.4800007215948776\n",
      "best k is 0.7994578239211614, best b is 0.40195742767614545, and the loss is 0.48000069740424023\n",
      "best k is 0.7994669893050402, best b is 0.4019243377321825, and the loss is 0.48000069740424023\n",
      "best k is 0.7994669893050402, best b is 0.4019243377321825, and the loss is 0.4800006740245659\n",
      "best k is 0.7994759997498413, best b is 0.4018918071674522, and the loss is 0.4800006740245659\n",
      "best k is 0.7994759997498413, best b is 0.4018918071674522, and the loss is 0.48000065142866866\n",
      "best k is 0.7994848578747803, best b is 0.40185982652575464, and the loss is 0.48000065142866866\n",
      "best k is 0.7994848578747803, best b is 0.40185982652575464, and the loss is 0.4800006295902728\n",
      "best k is 0.7994935662547956, best b is 0.4018283865107451, and the loss is 0.4800006295902728\n",
      "best k is 0.7994935662547956, best b is 0.4018283865107451, and the loss is 0.48000060848398396\n",
      "best k is 0.7995021274212969, best b is 0.4017974779832319, and the loss is 0.48000060848398396\n",
      "best k is 0.7995021274212969, best b is 0.4017974779832319, and the loss is 0.4800005880852588\n",
      "best k is 0.7995105438629007, best b is 0.4017670919585196, and the loss is 0.4800005880852588\n",
      "best k is 0.7995105438629007, best b is 0.4017670919585196, and the loss is 0.48000056837037786\n",
      "best k is 0.7995188180261541, best b is 0.40173721960379744, and the loss is 0.48000056837037786\n",
      "best k is 0.7995188180261541, best b is 0.40173721960379744, and the loss is 0.48000054931641556\n",
      "best k is 0.7995269523162454, best b is 0.4017078522355715, and the loss is 0.48000054931641556\n",
      "best k is 0.7995269523162454, best b is 0.4017078522355715, and the loss is 0.4800005309012151\n",
      "best k is 0.799534949097704, best b is 0.4016789813171407, and the loss is 0.4800005309012151\n",
      "best k is 0.799534949097704, best b is 0.4016789813171407, and the loss is 0.48000051310336345\n",
      "best k is 0.7995428106950874, best b is 0.40165059845611545, and the loss is 0.48000051310336345\n",
      "best k is 0.7995428106950874, best b is 0.40165059845611545, and the loss is 0.4800004959021639\n",
      "best k is 0.7995505393936566, best b is 0.4016226954019777, and the loss is 0.4800004959021639\n",
      "best k is 0.7995505393936566, best b is 0.4016226954019777, and the loss is 0.4800004792776148\n",
      "best k is 0.7995581374400411, best b is 0.40159526404368295, and the loss is 0.4800004792776148\n",
      "best k is 0.7995581374400411, best b is 0.40159526404368295, and the loss is 0.480000463210385\n",
      "best k is 0.799565607042891, best b is 0.4015682964073023, and the loss is 0.480000463210385\n",
      "best k is 0.799565607042891, best b is 0.4015682964073023, and the loss is 0.48000044768179045\n",
      "best k is 0.7995729503735203, best b is 0.40154178465370477, and the loss is 0.48000044768179045\n",
      "best k is 0.7995729503735203, best b is 0.40154178465370477, and the loss is 0.48000043267377357\n",
      "best k is 0.7995801695665365, best b is 0.4015157210762782, and the loss is 0.48000043267377357\n",
      "best k is 0.7995801695665365, best b is 0.4015157210762782, and the loss is 0.48000041816888367\n",
      "best k is 0.7995872667204629, best b is 0.4014900980986894, and the loss is 0.48000041816888367\n",
      "best k is 0.7995872667204629, best b is 0.4014900980986894, and the loss is 0.4800004041502535\n",
      "best k is 0.7995942438983469, best b is 0.4014649082726816, and the loss is 0.4800004041502535\n",
      "best k is 0.7995942438983469, best b is 0.4014649082726816, and the loss is 0.4800003906015819\n",
      "best k is 0.7996011031283609, best b is 0.4014401442759094, and the loss is 0.4800003906015819\n",
      "best k is 0.7996011031283609, best b is 0.4014401442759094, and the loss is 0.48000037750711355\n",
      "best k is 0.7996078464043911, best b is 0.4014157989098102, and the loss is 0.48000037750711355\n",
      "best k is 0.7996078464043911, best b is 0.4014157989098102, and the loss is 0.4800003648516224\n",
      "best k is 0.7996144756866178, best b is 0.40139186509751185, and the loss is 0.4800003648516224\n",
      "best k is 0.7996144756866178, best b is 0.40139186509751185, and the loss is 0.48000035262039165\n",
      "best k is 0.7996209929020847, best b is 0.40136833588177534, and the loss is 0.48000035262039165\n",
      "best k is 0.7996209929020847, best b is 0.40136833588177534, and the loss is 0.48000034079919934\n",
      "best k is 0.7996273999452589, best b is 0.4013452044229724, and the loss is 0.48000034079919934\n",
      "best k is 0.7996273999452589, best b is 0.4013452044229724, and the loss is 0.4800003293742983\n",
      "best k is 0.7996336986785824, best b is 0.4013224639970975, and the loss is 0.4800003293742983\n",
      "best k is 0.7996336986785824, best b is 0.4013224639970975, and the loss is 0.4800003183324043\n",
      "best k is 0.7996398909330126, best b is 0.40130010799381305, and the loss is 0.4800003183324043\n"
     ]
    }
   ],
   "source": [
    "min_loss = float(\"inf\") # 正无穷或负无穷，使用float(\"inf\")或float(\"-inf\")来表示\n",
    "best_k,best_b = None,None  # 空值\n",
    "current_k, current_b = 10, 10\n",
    "try_times = 1000\n",
    "learn_rate = 0.1\n",
    "for i in range(try_times):\n",
    "    \n",
    "    y_hat = [function(x_i,current_k,current_b) for x_i in list(x)]\n",
    "    current_loss = loss(y,y_hat)\n",
    "    if current_loss < min_loss:\n",
    "        min_loss = current_loss\n",
    "        best_k,best_b = current_k, current_b\n",
    "    else:\n",
    "        k_gradient = gradient_k(x,y,y_hat)\n",
    "        b_gradient = gradient_b(y,y_hat)\n",
    "        current_k = current_k - learn_rate * k_gradient\n",
    "        current_b = current_b - learn_rate * b_gradient\n",
    "        best_k,best_b = current_k, current_b\n",
    "    print(\"best k is {}, best b is {}, and the loss is {}\".format(best_k, best_b, current_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAXmElEQVR4nO3df3RU5Z3H8fdXjGtKPOacRiNGK9rV0C6okaxVY2tCbWEL2tTSHjz+WO1W2m4P1kpR0eJPRJRFPWq3HKu0dv0RFZFSjpbahugp1dZA1KCR9Rc0pBbb0gDBHDbAs3/cSUjiJHOTzJ37zMzndQ6nQ+Yy99PH5JObZ2a+MeccIiLirwPiDiAiIoNTUYuIeE5FLSLiORW1iIjnVNQiIp47MIoHLSkpcWPHjh3x4+zatYvRo0ePPFAaKVN4PuZSpvB8zJXLmdatW/c359xhSe90zqX9z8SJE106rFmzJi2Pk07KFJ6PuZQpPB9z5XImoNEN0Kna+hAR8ZyKWkTEcypqERHPqahFRDynohYR8ZyKWkTEcypqERHPhXrDi5ltAnYCe4E9zrnKKEOJiMh+Q7mirnHOnaySFhGBFU1tVC2sp7ltO1UL61nR1BbZuSJ5C7mISC5b0dTG3OXNdHbthaOhrb2TucubAaitKEv7+cJeUTvg12a2zsxmpj2FiEgWWbR6Y1DSvXR27WXR6o2RnM9ciF/FZWZHOuf+bGaHA88Bs5xzL/Q7ZiYwE6C0tHRiXV3diMN1dHRQVFQ04sdJJ2UKz8dcyhSej7l8ydTctr3ndmkhbO3cf9+EskOH9Zg1NTXrBtpaDlXUff6B2Y1Ah3PuvwY6prKy0jU2Ng7pcZNpaGigurp6xI+TTsoUno+5lCk8H3P5kqlqYT1t7UE7z56wh8XNwS5yWXEha6+ZNKzHNLMBizrl1oeZjTazQ7pvA18ENgwriYhIDpgzuZzCglF9PlZYMIo5k8sjOV+YJxNLgafNrPv4R51zv4okjYhIFuh+wjDYk95JWXEhcyaXR/JEIoQoaufcu8BJkZxdRCRL1VaUUVtRRkNDA7MuqI70XHpnooiI51TUIiKeU1GLiHhORS0i4jkVtYiI51TUIiKeU1GLiHhORS0i4jkVtYiI51TUIiKeU1GLiHhORS0i4jkVtYiI51TUIiKeU1GLiHhORS0i4jkVtYiI51TUIiKeU1GLiHhORS0i4jkVtYiI51TUIiKeU1GLiAzXrl0c+uqrkZ9GRS0iMlS7d8O998InP8mJ11wD7e2Rnk5FLSISVlcX/OQncPzxcPnlMG4cr91xBxQXR3paFbWISCp798LDD8OnPgUzZ8KRR8JvfgNr1rB9woTIT6+iFhEZyL598NRTcOKJcNFFUFQEv/wlvPgifP7zYJaRGCpqEZH+nINnnoHKSpg+PSjsJ56A9eth2rSMFXQ3FbWISG9r1sCZZ8LUqcGThA89BBs2wNe+BgfEU5kqahER2L+dMWkSbN4MS5bAxo1w8cUwalSs0VTUIpLfmpqC7YwzzoDmZrjrLnj7bfjWt6CgIO50gIpaRPLVG28E2xmnnAJr18KCBfDuu3DFFXDwwXGn6+PAuAOIiGTUO+/AjTfCI4/A6NEwbx5ceWXkr4UeCRW1iOSH1la45RZYuhQOOgh+8AO46iooKYk7WUoqahHJbVu3BtsaS5YEL7v7znfg2mthzJi4k4WmohaR3LRtG9xxRzCTY/duuOSSYJvjmGPiTjZkKmoRyS07dgSv3LjzTti5E84/P9iTPv74uJMNm4paRHLDrl3wox/B7bcHV9PnnQc33QTjx8edbMT08jwRyW69Ro5y9dXwmc9AY2MwoyMHShp0RS0i2aqrC372s+CVHK2tcNZZsGxZ8PbvHBP6itrMRplZk5mtijKQiMigBhk5moslDUPb+vge0BJVEBGRQe3bR8nzz8c+cjQOoYrazI4CpgIPRBtHRKSfXiNHx994Y+wjR+NgzrnUB5ktA24DDgF+4JybluSYmcBMgNLS0ol1dXUjDtfR0UFRUdGIHyedlCk8H3MpU3g+5CpuauLYBx/k0Ndfp3PMGN6cMYPtU6fGPs2ut3StU01NzTrnXGXSO51zg/4BpgH/nbhdDaxK9W8mTpzo0mHNmjVpeZx0UqbwfMylTOHFmuv3v3du0iTnwLmyMueWLHFu924v1ypdmYBGN0Cnhtn6qALONbNNQB0wycweHvG3DxGR/gYbOXrQQXGni03KonbOzXXOHeWcGwvMAOqdcxdGnkxE8kcWjRyNg15HLSLxycKRo3EYUlE75xqAhkiSiEj+aG2F+fODkaMFBVk1cjQOuqIWkczZuhVuuw1+/OPgZXff/nbWjRyNg4paRKKXQyNH46CiFpHo5ODI0TioqEUk/XJ45GgcNOZURNInD0aOxkFX1CIycnk0cjQOuqIWkeHLw5GjcVBRi8jQ7dsXbGf0Hjm6cmVejByNg4paRMLrNXKU6dODK+rHHw9Gjp5zjgo6IipqEQmneztj6lRob4eHHoING+DrX4cDVCVR0uqKyOC6tzMmTYLNm2HJEnjzTbj4YjhQr0fIBBW1iCRV9NZbGjnqCX07FJG+Wlrg+uupXLYsmGK3YAHMmhU8YSixUFGLSOCdd4J3Dz7yCHzsY2y66CLG3nOPRo56QFsfIvmutTXYzhg3LniTyuzZ8N57bPrGN1TSntAVtUi+0sjRrKGiFsk3GjmadVTUIvlCI0ezlopaJNdp5GjW05OJIrlKI0dzhq6oRXKNRo7mHF1Ri+SK/iNHx4yB557TyNEcoKIWyXYDjRx96SU4+2xNtMsBKmqRbKWRo3lDRS2SjTRyNK/ov6hINuneztDI0byiohbJBk1NwcjR00+H117TyNE8o2/BIj5LjBxFI0fzmopaxEf9Ro4ybx5ceaWm2eUpFbWIT1pbYf58WLoUCgqCkaNXXQUlJXEnkxipqEV8oJGjMggVtUictm3juPvvh1/8QiNHZUAqapE49Bo5erRGjkoKKmqRTEoycrRx2jT+9dJL404mHtPrqEUyof/I0VNP7Rk5uuvYY+NOJ57TFbVIlDRyVNJAV9QiUdDIUUkjFbVIOmnkqERARS2SDho5KhFKWdRmdrCZ/dHMXjWz183spkwEk9RWNLVRtbCe5rbtVC2sZ0VTW9yR8lOOjRzV55V/wnwW7QYmOedOAk4GppjZadHGklRWNLUxd3kzbe2dALS1dzJ3ebO+qDIpB0eO6vPKTymL2gU6En8tSPxxkaaSlBat3khn194+H+vs2sui1RtjSpRHcnjkqD6v/GTOpe5cMxsFrAP+GfiRc+7qJMfMBGYClJaWTqyrqxtxuI6ODoo8G+foS6bmtu09t0sLYWvn/vsmlB0aQ6KP8mWtehtJpo9t3szYn/6Uw59/nq6iIlpnzKDtvPPYW1gYW6Z08/3zyqe16pauTDU1Neucc5XJ7gtV1D0HmxUDTwOznHMbBjqusrLSNTY2Djlofw0NDVRXV4/4cdLJl0xVC+t7fjydPWEPi5uDH7XLigtZe82kOKP18GWtehtWpv4jR7///bSOHPVpnXz/vPJprbqlK5OZDVjUQ3qmwznXDjQAU0acSkZkzuRyCgtG9flYYcEo5kwujylRDmptDbYzxo0L3qQyeza89x7cfHPOzoXW55WfUj7jYWaHAV3OuXYzKwTOBm6PPJkMqraiDCCxd7iTsuJC5kwu7/m4jEAejxzV55Wfwjw1PQZ4KLFPfQDwhHNuVbSxJIzaijJqK8poaGhg1gXVccfJftu2wR13BDM58njkqD6v/JOyqJ1zrwEVGcgiEo9eI0fRyFHxUHa+2FMkHfqPHP3KV4L95/Hj404m0kf2vW1KZKSSjRx9+WVYvlwlLV7SFbXkj64uxqxaFbxzUCNHJYvoilpy3969wWugP/1pyhcv1shRyToqasldzgXbGSedBBdeCKNH03zrrRo5KllHRS25p/fI0a9+Ffbs6Rk5+vczzlBBS9ZRUUtu6T1y9B//yPqRoyKgopZckYMjR0W6qaglu+XwyFGRbrrUkOzU0gLXXx+8vK64GBYsgFmzgt9RKJJjVNSSXfqPHJ03L60jR0V8pKKW7NDaCvPnw9KlUFAQjBy96iooKYk7mUjkVNTitzweOSrSTUUtftLIUZEeKmrxy44dcPfdsHixRo6KJKioxQ8ffgj33dd35OhNN8GECXEnE4mdXkct8eoeOXrccR8dOaqSFgF0RS1x6eoK3t59880aOSqSgq6oJbN6jRzlsss0clQkBBW1ZEaSkaOsXKmRoyIhqKglWoOMHOWcc1TQIiGoqCUyxU1NGjkqkgZ6MlHS76WX4Ic/5OTf/hbKyoKRo5deqml2IsOkyxpJn1de6TNy9O3vflcjR0XSQEUtI9fSEmxnVFTA2rXByNF332XL9Olw8MFxpxPJetr6kOF7993g7d0aOSoSKRW1DN2WLXDLLRo5KpIhKmoJr3vk6JIlsG9fsPd83XUaOSoSMRW1pLZtGyxaBPfco5GjIjFQUcvAko0cveEGOOGEuJOJ5BUVtXyURo6KeEUvz5P9NHJUxEu6ohaNHBXxnK6o85lGjopkBRV1PtLIUZGsoqLOJ87Bs89q5KhIllFR54uGBvjsZ+FLX9LIUZEso6/QXNe9nVFTA5s2Be8qfPNNuPhiOFDPJYtkAxV1rnrllWA7IzFylLvu0shRkSyVsqjN7GgzW2NmLWb2upl9LxPBZJh6jxz93e96Ro5yxRV5PXJ0RVMbVQvraW7bTtXCelY0tcUdSSS0MD/77gFmO+fWm9khwDoze84590bE2WQoNHJ0QCua2pi7vJnOrr1wNLS1dzJ3eTMAtRVlMacTSS3lFbVz7n3n3PrE7Z1AC6DPbl9s2cIJixdDeTk8+WRQzu+9F7x5RSUNwKLVG4OS7qWzay+LVm+MKZHI0JhzLvzBZmOBF4Dxzrkd/e6bCcwEKC0tnVhXVzficB0dHRQVFY34cdLJl0wF27ZxzKOPcuTKleAc70+bxuYLL+T/Pv7xuKP18GWtmtu299wuLYStnfvvm1B2aAyJ+vJlnfrzMVcuZ6qpqVnnnKtMdl/oojazIuB54Fbn3PLBjq2srHSNjY1DDtpfQ0MD1dXVI36cdIo9U5KRoy+efTanz5gRX6YBxL5WCVUL62lrD9p59oQ9LG4OdvzKigtZe82kOKMB/qxTfz7myuVMZjZgUYd61YeZFQBPAY+kKmmJyI4dwXbGsccGU+1qa+GNN+CBB9h9xBFxp/PanMnlFBaM6vOxwoJRzJlcHlMikaFJ+WSimRnwINDinLsz+kjSh0aOjlj3E4bBnvROyooLmTO5XE8kStYI86qPKuAioNnMXkl87Frn3DPRxRJ274b774dbbw1+BdaUKcHvKaxM+pORpFBbUUZtRRkNDQ3MuqA67jgiQ5KyqJ1zvwM0BCJTurrg5z8Ptjn+9CeNHBURvTPRG71Hjn7zm3DEERo5KiKAijp+GjkqIimoqOOikaMiEpKKOg4aOSoiQ6BWyCSNHBWRYVBRZ4JGjorICKioo6SRoyKSBvp5OwoaOSoiaaSiTqctW4J3Dy5dGuw5X3klXH01lJTEnUxEspiKOh22boXbbgueHNy3L9h7vu46GDMm7mQikgNU1CORZOQo8+bBMcfEnUxEcoiKejh27IC774bFi2HnTjj/fLjhBjjhhLiTiUgOUlEPxYcfcvRjjwXvJNTIURHJEL08L4zdu+Hee+G44/jk/ffDqafCyy8HMzpU0iISMRX1YLq64IEH4Pjj4fLLYdw4mu65Z/+MDhGRDFBRJ9N75OhllwWv3kiMHN2uK2gRyTAVdW8aOSoiHlJRg0aOiojXVNQaOSoinsvfJtLIURHJEvlX1MlGjr71lkaOioi38ufSsaUlePfgk08GU+wWLIBZs6CoKO5kIiKDyv2i1shREclyuVvUGjkqIjki94paI0dFJMfkTlFr5KiI5KjsL2qNHBWRHJe9Rf3hh3DffXD77Ro5KiI5LfteR91r5ChXX62RoyKS87LnirqrK3h79803Q2srnHUWLFsGZ54ZdzIRkUj5f0U9yMhRlbSI5AN/i9o5Sl54QSNHRSTv+bn18Ze/wNSpjF+/HsrLg5Gj06drmp2I5CU/i/rww+ETn6DlC1/gU/Pna5qdiOQ1Py9RDzgAnn6arVOmqKRFJO/5WdQiItJDRS0i4jkVtYiI51TUIiKeS1nUZrbUzD4wsw2ZCCQiIn2FuaL+GTAl4hx9rGhqo2phPc1t26laWM+KprZMnl5ExCspi9o59wKwLQNZgKCk5y5vpq29E4C29k7mLm9WWYtI3vJuj3rR6o10du3t87HOrr0sWr0xpkQiIvEy51zqg8zGAqucc+MHOWYmMBOgtLR0Yl1d3bACNbdt77ldWghbO/ffN6Hs0GE9Zjp1dHRQ5NlvLvcxE/iZS5nC8zFXLmeqqalZ55yrTHZf2oq6t8rKStfY2DiUjD2qFtb3bHvMnrCHxc3BOxPLigtZe82kYT1mOjU0NFBdXR13jD58zAR+5lKm8HzMlcuZzGzAovZu62PO5HIKC0b1+VhhwSjmTC6PKZGISLzCvDzvMeBFoNzMtpjZf0QZqLaijNvOm0BZcSEQXEnfdt4EaivKojytiIi3Uk48cs6dn4kgvdVWlFFbUUZDQwOzLqjO9OlFRLzi3daHiIj0paIWEfGcilpExHMqahERz6moRUQ8p6IWEfGcilpExHOh3kI+5Ac1+yuwOQ0PVQL8LQ2Pk07KFJ6PuZQpPB9z5XKmY5xzhyW7I5KiThczaxzove9xUabwfMylTOH5mCtfM2nrQ0TEcypqERHP+V7U98cdIAllCs/HXMoUno+58jKT13vUIiLi/xW1iEjeU1GLiHgu9qI2s6Vm9oGZbRjgfjOze8zsbTN7zcxO8SBTtZltN7NXEn+uz0Cmo81sjZm1mNnrZva9JMdkdK1CZopjrQ42sz+a2auJXDclOeafzOzxxFr9IfHr5uLOdImZ/bXXWn0zyky9zjvKzJrMbFWS+zK6TiEzxbVOm8ysOXHOj/yuwUi//pxzsf4BPgecAmwY4P4vAc8CBpwG/MGDTNUEv0Myk+s0BjglcfsQ4H+BT8e5ViEzxbFWBhQlbhcAfwBO63fMfwJLErdnAI97kOkS4L5MrlXivFcCjyb775TpdQqZKa512gSUDHJ/ZF9/sV9RO+deALYNcsiXgZ+7wEtAsZmNiTlTxjnn3nfOrU/c3gm0AP1/P1lG1ypkpoxL/P/vSPy1IPGn/7PmXwYeStxeBnzezCzmTBlnZkcBU4EHBjgko+sUMpOvIvv6i72oQygDWnv9fQselAFweuLH2GfN7F8yeeLEj58VBFdlvcW2VoNkghjWKvGj8yvAB8BzzrkB18o5twfYDnw85kwAX0382LzMzI6OMk/C3cBVwL4B7s/4OoXIBJlfJwi+sf7azNaZ2cwk90f29ZcNRZ3su3fcVyLrCd6XfxJwL7AiUyc2syLgKeAK59yO/ncn+SeRr1WKTLGslXNur3PuZOAo4FQzG9/vkIyvVYhMvwTGOudOBH7D/ivZSJjZNOAD59y6wQ5L8rHI1ilkpoyuUy9VzrlTgH8Dvmtmn+t3f2RrlQ1FvQXo/R3zKODPMWUBwDm3o/vHWOfcM0CBmZVEfV4zKyAoxEecc8uTHJLxtUqVKa616nX+dqABmNLvrp61MrMDgUPJ0HbXQJmcc393zu1O/PUnwMSIo1QB55rZJqAOmGRmD/c7JtPrlDJTDOvUfd4/J/73A+Bp4NR+h0T29ZcNRb0SuDjxjOppwHbn3PtxBjKzI7r36czsVIJ1/HvE5zTgQaDFOXfnAIdldK3CZIpprQ4zs+LE7ULgbODNfoetBP49cXs6UO8SzwjFlanffua5BHv+kXHOzXXOHeWcG0vwRGG9c+7CfodldJ3CZMr0OiXOOdrMDum+DXwR6P+qsMi+/g5Mx4OMhJk9RvDKgBIz2wLcQPBEC865JcAzBM+mvg18CFzqQabpwHfMbA/QCcyI8pM3oQq4CGhO7HMCXAt8oleuTK9VmExxrNUY4CEzG0XwjeEJ59wqM7sZaHTOrST4BvM/ZvY2wRXiDA8yXW5m5wJ7EpkuiThTUjGvU5hMcaxTKfB04prjQOBR59yvzOzbEP3Xn95CLiLiuWzY+hARyWsqahERz6moRUQ8p6IWEfGcilpExHMqahERz6moRUQ89/870Nge0fs43QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# best k is 0.80, best b is 0.40, and the loss is 0.48\n",
    "k,b = 0.80,0.40\n",
    "y_hat = k*x + b\n",
    "plt.plot(x,y_hat,color='red') # 绘制 x 与 y，color='red'设置颜色\n",
    "plt.scatter(x,y)\n",
    "plt.grid() # 显示网格"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 3 线性回归(Linear Regression) \n",
    " ## 3.1 Extension-Multiple Variables\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200413143328240.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_left\" alt=\"图片替换文本\" width=\"500\" height=\"500\" align=\"bottom\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200413143817359.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_left\" alt=\"图片替换文本\" width=\"700\" height=\"600\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 过拟合(Overfitting) 和 欠拟合(Underfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/20200412172241826.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "## 4.1 why？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Bias and Variance\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200413123053887.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"500\" height=\"500\" align=\"bottom\" />\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200413131243813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"500\" height=\"500\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 How to detect Overfitting?\n",
    "> ### 4.3.1 数据集划分\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200412182252554.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"600\" height=\"400\" align=\"bottom\" />\n",
    "\n",
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/20200413132215734.jpeg#pic_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> ### 4.3.2 正则化(Regularlization)\n",
    "\n",
    " >> $ R(k) $ 刻画模型的复杂程度\n",
    "  $$ Cost(k,b) +  \\lambda R(k) $$\n",
    "\n",
    "\n",
    " >> L1 正则化： \n",
    "  $$ R(k) = \\left \\|  k \\right \\|_1 = \\sum_i |k_i|$$\n",
    "\n",
    " >> L2 正则化： \n",
    "$$ R(k) = \\left \\|  k \\right \\|_2^2 = \\sum_i |k_i^2|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 如何评价？\n",
    "\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200413103828575.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"700\" height=\"600\" align=\"bottom\" />\n",
    "\n",
    "$$ Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}  $$   \n",
    "\n",
    "$$ Precision = \\frac{TP}{(TP+FP)}  $$\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    " \n",
    "$$ F1_{score} = \\frac{2\\times Recall\\times Precision}{Recall + Precision} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Classes\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200413152008202.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"600\" height=\"600\" align=\"bottom\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5 Logistic Function and Sigmoid Function\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200413152402793.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"600\" height=\"600\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hypothesis(X, theta):\n",
    "    z = X.dot(theta)\n",
    "    g = 1/(1+np.e**-z)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 逻辑回归(Logistic Regression)\n",
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/2020041315514144.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 代价函数和梯度\n",
    "def costFunction(theta, X, y):\n",
    "    h = hypothesis(X,theta)\n",
    "    J = 1.0/m*(-y.T.dot(log(h)) - (1-y).T.dot(log(1-h)))\n",
    "    grad = 1.0/m*X.T.dot(h-y)\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 信用卡反欺诈检测实战----判断客户交易记录是否正常    \n",
    "\n",
    "> 银行提供了一份个人交易记录，考虑到数据比较私密，不清楚大部分数据是什么，仅知道Amount列表示贷款的金额，Class列表示分类结果，若Class为0代表该条交易记录正常，若Class为1代表交易异常。    \n",
    "  数据已经做了处理，分出了训练集和测试集，且平衡了两个类别下的数据量。   \n",
    "  该实战的目的是希望学生掌握读取数据、逻辑回归建模、模型参数调优、模型评价。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6870</th>\n",
       "      <td>-1.863756</td>\n",
       "      <td>3.442644</td>\n",
       "      <td>-4.468260</td>\n",
       "      <td>2.805336</td>\n",
       "      <td>-2.118412</td>\n",
       "      <td>-2.332285</td>\n",
       "      <td>-4.261237</td>\n",
       "      <td>1.701682</td>\n",
       "      <td>-1.439396</td>\n",
       "      <td>-6.999907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360924</td>\n",
       "      <td>0.667927</td>\n",
       "      <td>-0.516242</td>\n",
       "      <td>-0.012218</td>\n",
       "      <td>0.070614</td>\n",
       "      <td>0.058504</td>\n",
       "      <td>0.304883</td>\n",
       "      <td>0.418012</td>\n",
       "      <td>0.208858</td>\n",
       "      <td>-0.349231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152015</th>\n",
       "      <td>-0.425454</td>\n",
       "      <td>1.226245</td>\n",
       "      <td>-1.314904</td>\n",
       "      <td>0.775567</td>\n",
       "      <td>2.264370</td>\n",
       "      <td>-0.768401</td>\n",
       "      <td>1.371214</td>\n",
       "      <td>-0.326371</td>\n",
       "      <td>0.816325</td>\n",
       "      <td>-1.715394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.229286</td>\n",
       "      <td>-0.210073</td>\n",
       "      <td>-0.280529</td>\n",
       "      <td>-0.351881</td>\n",
       "      <td>-0.098883</td>\n",
       "      <td>0.411762</td>\n",
       "      <td>-0.588556</td>\n",
       "      <td>-0.046577</td>\n",
       "      <td>0.089299</td>\n",
       "      <td>-0.133335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261556</th>\n",
       "      <td>2.030930</td>\n",
       "      <td>-0.866155</td>\n",
       "      <td>-0.228221</td>\n",
       "      <td>-0.428373</td>\n",
       "      <td>-1.155582</td>\n",
       "      <td>-0.474754</td>\n",
       "      <td>-0.873382</td>\n",
       "      <td>-0.039503</td>\n",
       "      <td>-0.415414</td>\n",
       "      <td>0.946528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.500968</td>\n",
       "      <td>-0.546578</td>\n",
       "      <td>-1.040354</td>\n",
       "      <td>0.497881</td>\n",
       "      <td>0.044952</td>\n",
       "      <td>-0.780473</td>\n",
       "      <td>0.204253</td>\n",
       "      <td>-0.015302</td>\n",
       "      <td>-0.043013</td>\n",
       "      <td>-0.245321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214775</th>\n",
       "      <td>-0.395582</td>\n",
       "      <td>-0.751792</td>\n",
       "      <td>-1.984666</td>\n",
       "      <td>-0.203459</td>\n",
       "      <td>1.903967</td>\n",
       "      <td>-1.430289</td>\n",
       "      <td>-0.076548</td>\n",
       "      <td>-0.992260</td>\n",
       "      <td>0.756307</td>\n",
       "      <td>0.217630</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.027716</td>\n",
       "      <td>1.377515</td>\n",
       "      <td>2.151787</td>\n",
       "      <td>0.189225</td>\n",
       "      <td>0.772943</td>\n",
       "      <td>-0.872443</td>\n",
       "      <td>-0.200612</td>\n",
       "      <td>0.356856</td>\n",
       "      <td>0.032113</td>\n",
       "      <td>-0.350471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149145</th>\n",
       "      <td>-2.405580</td>\n",
       "      <td>3.738235</td>\n",
       "      <td>-2.317843</td>\n",
       "      <td>1.367442</td>\n",
       "      <td>0.394001</td>\n",
       "      <td>1.919938</td>\n",
       "      <td>-3.106942</td>\n",
       "      <td>-10.764403</td>\n",
       "      <td>3.353525</td>\n",
       "      <td>0.369936</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.140874</td>\n",
       "      <td>10.005998</td>\n",
       "      <td>-2.454964</td>\n",
       "      <td>1.684957</td>\n",
       "      <td>0.118263</td>\n",
       "      <td>-1.531380</td>\n",
       "      <td>-0.695308</td>\n",
       "      <td>-0.152502</td>\n",
       "      <td>-0.138866</td>\n",
       "      <td>-0.325283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "6870   -1.863756  3.442644 -4.468260  2.805336 -2.118412 -2.332285 -4.261237   \n",
       "152015 -0.425454  1.226245 -1.314904  0.775567  2.264370 -0.768401  1.371214   \n",
       "261556  2.030930 -0.866155 -0.228221 -0.428373 -1.155582 -0.474754 -0.873382   \n",
       "214775 -0.395582 -0.751792 -1.984666 -0.203459  1.903967 -1.430289 -0.076548   \n",
       "149145 -2.405580  3.738235 -2.317843  1.367442  0.394001  1.919938 -3.106942   \n",
       "\n",
       "               V8        V9       V10     ...           V20        V21  \\\n",
       "6870     1.701682 -1.439396 -6.999907     ...      0.360924   0.667927   \n",
       "152015  -0.326371  0.816325 -1.715394     ...     -0.229286  -0.210073   \n",
       "261556  -0.039503 -0.415414  0.946528     ...     -0.500968  -0.546578   \n",
       "214775  -0.992260  0.756307  0.217630     ...     -1.027716   1.377515   \n",
       "149145 -10.764403  3.353525  0.369936     ...     -2.140874  10.005998   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "6870   -0.516242 -0.012218  0.070614  0.058504  0.304883  0.418012  0.208858   \n",
       "152015 -0.280529 -0.351881 -0.098883  0.411762 -0.588556 -0.046577  0.089299   \n",
       "261556 -1.040354  0.497881  0.044952 -0.780473  0.204253 -0.015302 -0.043013   \n",
       "214775  2.151787  0.189225  0.772943 -0.872443 -0.200612  0.356856  0.032113   \n",
       "149145 -2.454964  1.684957  0.118263 -1.531380 -0.695308 -0.152502 -0.138866   \n",
       "\n",
       "        normAmount  \n",
       "6870     -0.349231  \n",
       "152015   -0.133335  \n",
       "261556   -0.245321  \n",
       "214775   -0.350471  \n",
       "149145   -0.325283  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train = pd.read_csv('X_train.csv', index_col=0)\n",
    "y_train = pd.read_csv('y_train.csv', index_col=0)\n",
    "\n",
    "X_test = pd.read_csv('X_test.csv', index_col=0)\n",
    "y_test = pd.read_csv('y_test.csv', index_col=0)\n",
    "\n",
    "X_train.head() # 前五行数据展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > 原始数据为个人交易记录，该数据集总共有29列，其中数据特征有30列，Time列暂时不考虑，Amount列表示贷款的金额."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6870</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152015</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261556</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214775</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149145</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Class\n",
       "6870        1\n",
       "152015      0\n",
       "261556      0\n",
       "214775      1\n",
       "149145      1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Class为0代表该条交易记录正常，Class为1代表交易异常。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score # Recall = TP/(TP+FN)\n",
    "from sklearn.model_selection import KFold, cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KFold in module sklearn.model_selection._split:\n",
      "\n",
      "class KFold(_BaseKFold)\n",
      " |  KFold(n_splits=3, shuffle=False, random_state=None)\n",
      " |  \n",
      " |  K-Folds cross-validator\n",
      " |  \n",
      " |  Provides train/test indices to split data in train/test sets. Split\n",
      " |  dataset into k consecutive folds (without shuffling by default).\n",
      " |  \n",
      " |  Each fold is then used once as a validation while the k - 1 remaining\n",
      " |  folds form the training set.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <cross_validation>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=3\n",
      " |      Number of folds. Must be at least 2.\n",
      " |  \n",
      " |  shuffle : boolean, optional\n",
      " |      Whether to shuffle the data before splitting into batches.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default=None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`. Used when ``shuffle`` == True.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.model_selection import KFold\n",
      " |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      " |  >>> y = np.array([1, 2, 3, 4])\n",
      " |  >>> kf = KFold(n_splits=2)\n",
      " |  >>> kf.get_n_splits(X)\n",
      " |  2\n",
      " |  >>> print(kf)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |  KFold(n_splits=2, random_state=None, shuffle=False)\n",
      " |  >>> for train_index, test_index in kf.split(X):\n",
      " |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      " |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      " |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      " |  TRAIN: [2 3] TEST: [0 1]\n",
      " |  TRAIN: [0 1] TEST: [2 3]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The first ``n_samples % n_splits`` folds have size\n",
      " |  ``n_samples // n_splits + 1``, other folds have size\n",
      " |  ``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  StratifiedKFold\n",
      " |      Takes group information into account to avoid building folds with\n",
      " |      imbalanced class distributions (for binary or multiclass\n",
      " |      classification tasks).\n",
      " |  \n",
      " |  GroupKFold: K-fold iterator variant with non-overlapping groups.\n",
      " |  \n",
      " |  RepeatedKFold: Repeats K-Fold n times.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KFold\n",
      " |      _BaseKFold\n",
      " |      abc.NewBase\n",
      " |      BaseCrossValidator\n",
      " |      abc.NewBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_splits=3, shuffle=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseKFold:\n",
      " |  \n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |  \n",
      " |  split(self, X, y=None, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target variable for supervised learning problems.\n",
      " |      \n",
      " |      groups : array-like, with shape (n_samples,), optional\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |      \n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Randomized CV splitters may return different results for each call of\n",
      " |      split. You can make the results identical by setting ``random_state``\n",
      " |      to an integer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseCrossValidator:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from abc.NewBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(KFold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Kfold_get_best_c(x_train_data,y_train_data):\n",
    "\n",
    "    fold = KFold(5,shuffle=False)  # k-fold 表示K折的交叉验证\n",
    "    # 这里会得到两个索引集合: 训练集 = indices[0], 验证集 = indices[1]\n",
    "    \n",
    "    c_param_range = [0.01,0.1,1,10,100]  # 定义不同的正则化惩罚力度\n",
    "    \n",
    "    results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\n",
    "    results_table['C_parameter'] = c_param_range\n",
    "\n",
    "    j = 0\n",
    "    # 循环遍历不同的参数\n",
    "    for c_param in c_param_range:\n",
    "        print('-------------------------------------------')\n",
    "        print('正则化惩罚力度: ', c_param)\n",
    "        print('-------------------------------------------')\n",
    "        print('')\n",
    "\n",
    "        recall_accs = []\n",
    "        \n",
    "        \n",
    "        for iteration, indices in enumerate(fold.split(x_train_data)):\n",
    "\n",
    "            # 逻辑回归，并且给定参数,L1正则\n",
    "            lr = LogisticRegression(C = c_param, penalty = 'l1',solver='liblinear')\n",
    "\n",
    "            # 训练模型，注意索引不要给错了，训练的时候一定传入的是训练集，所以X和Y的索引都是0\n",
    "            lr.fit(x_train_data.iloc[indices[0],:],y_train_data.iloc[indices[0],:].values.ravel())\n",
    "   \n",
    "            # 建立好模型后，预测模型结果，这里用的就是验证集，索引为1\n",
    "            y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values)\n",
    "\n",
    "            # 有了预测结果之后就可以来进行评估了，这里recall_score需要传入预测值和真实值。\n",
    "            recall_acc = recall_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample)\n",
    "            # 一会还要算平均，先把每一步的结果都先保存起来。\n",
    "            recall_accs.append(recall_acc)\n",
    "            print('Iteration ', iteration,': 召回率 = ', recall_acc)\n",
    "\n",
    "        \n",
    "        # 当执行完所有的交叉验证后，计算平均结果\n",
    "        results_table.loc[j,'Mean recall score'] = np.mean(recall_accs)\n",
    "        j += 1\n",
    "        print('')\n",
    "        print('平均召回率 ', np.mean(recall_accs))\n",
    "        print('')\n",
    "        \n",
    "    #找到最好的参数，哪一个Recall高，自然就是最好的了。\n",
    "    best_c = results_table.loc[results_table['Mean recall score'].astype('float32').idxmax()]['C_parameter']\n",
    "    \n",
    "    # 打印最好的结果\n",
    "    print('*********************************************************************************')\n",
    "    print('效果最好的模型所选参数 = ', best_c)\n",
    "    print('*********************************************************************************')\n",
    "    \n",
    "    return best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "正则化惩罚力度:  0.01\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  0 : 召回率 =  0.9315068493150684\n",
      "Iteration  1 : 召回率 =  0.9178082191780822\n",
      "Iteration  2 : 召回率 =  1.0\n",
      "Iteration  3 : 召回率 =  0.9594594594594594\n",
      "Iteration  4 : 召回率 =  0.9696969696969697\n",
      "\n",
      "平均召回率  0.955694299529916\n",
      "\n",
      "-------------------------------------------\n",
      "正则化惩罚力度:  0.1\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  0 : 召回率 =  0.8356164383561644\n",
      "Iteration  1 : 召回率 =  0.863013698630137\n",
      "Iteration  2 : 召回率 =  0.9152542372881356\n",
      "Iteration  3 : 召回率 =  0.9459459459459459\n",
      "Iteration  4 : 召回率 =  0.8787878787878788\n",
      "\n",
      "平均召回率  0.8877236398016523\n",
      "\n",
      "-------------------------------------------\n",
      "正则化惩罚力度:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  0 : 召回率 =  0.863013698630137\n",
      "Iteration  1 : 召回率 =  0.863013698630137\n",
      "Iteration  2 : 召回率 =  0.9491525423728814\n",
      "Iteration  3 : 召回率 =  0.9459459459459459\n",
      "Iteration  4 : 召回率 =  0.8939393939393939\n",
      "\n",
      "平均召回率  0.9030130559036991\n",
      "\n",
      "-------------------------------------------\n",
      "正则化惩罚力度:  10\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  0 : 召回率 =  0.863013698630137\n",
      "Iteration  1 : 召回率 =  0.863013698630137\n",
      "Iteration  2 : 召回率 =  0.9491525423728814\n",
      "Iteration  3 : 召回率 =  0.9459459459459459\n",
      "Iteration  4 : 召回率 =  0.8939393939393939\n",
      "\n",
      "平均召回率  0.9030130559036991\n",
      "\n",
      "-------------------------------------------\n",
      "正则化惩罚力度:  100\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  0 : 召回率 =  0.863013698630137\n",
      "Iteration  1 : 召回率 =  0.863013698630137\n",
      "Iteration  2 : 召回率 =  0.9491525423728814\n",
      "Iteration  3 : 召回率 =  0.9459459459459459\n",
      "Iteration  4 : 召回率 =  0.8939393939393939\n",
      "\n",
      "平均召回率  0.9030130559036991\n",
      "\n",
      "*********************************************************************************\n",
      "效果最好的模型所选参数 =  0.01\n",
      "*********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# 交叉验证得到不同参数结果\n",
    "best_c = Kfold_get_best_c(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "召回率： 0.9319727891156463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soph7e/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(C = 0.01, penalty = 'l1', solver='liblinear')\n",
    "LR.fit(X_train, y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "LR_recall = recall_score(y_test,y_pred)\n",
    "print('召回率：',LR_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "召回率： 0.9115646258503401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soph7e/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# 没有加L1正则化\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "recall = recall_score(y_test,y_pred)\n",
    "print('召回率：',recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 神经网络(Neural Network)\n",
    "## 7.1  Why Need NN?\n",
    ">  Non Linear\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200414104406949.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"600\" height=\"600\" align=\"bottom\" />\n",
    "\n",
    ">  Mimic Brain\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200414105454130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"600\" height=\"600\" align=\"bottom\" />\n",
    "\n",
    "## 7.2 Logistic Unit And Neural Network\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200414112153267.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"600\" height=\"600\" align=\"bottom\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 激活函数\n",
    "<img src=\"https://lh6.googleusercontent.com/H8qFLpDwU1VvMRnaEqNVOJvRSjTePk3Jk1TNEOsvWaRs-rwn3GOIWe_RTyoBy36Lh-dU3-k8yfYdPp5B0DJ9ljMJjqkxvsRn7xxgUCdEXRQAx1vSIsgSvBtUOuEf2yJ106s-67yk\" alt=\"图片替换文本\" width=\"800\" height=\"800\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Back Propagation \n",
    "<img src=\"https://img-blog.csdnimg.cn/20200415162406262.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"600\" height=\"600\" align=\"bottom\" />\n",
    "\n",
    "<img src=\"https://img-blog.csdnimg.cn/20200415162638161.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA3NTE5NzQ=,size_16,color_FFFFFF,t_70#pic_center\" alt=\"图片替换文本\" width=\"600\" height=\"500\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, inputs=[]):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "\n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)\n",
    "            # set 'self' node as inbound_nodes's outbound_nodes\n",
    "\n",
    "        self.value = None\n",
    "\n",
    "        self.gradients = {}\n",
    "        # keys are the inputs to this node, and their\n",
    "        # values are the partials of this node with \n",
    "        # respect to that input.\n",
    "        # \\partial{node}{input_i}\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        '''\n",
    "        Forward propagation. \n",
    "        Compute the output value vased on 'inbound_nodes' and store the \n",
    "        result in self.value\n",
    "        '''\n",
    "\n",
    "        raise NotImplemented\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        raise NotImplemented\n",
    "        \n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        An Input node has no inbound nodes.\n",
    "        So no need to pass anything to the Node instantiator.\n",
    "        '''\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self, value=None):\n",
    "        '''\n",
    "        Only input node is the node where the value may be passed\n",
    "        as an argument to forward().\n",
    "        All other node implementations should get the value of the \n",
    "        previous node from self.inbound_nodes\n",
    "        \n",
    "        Example: \n",
    "        val0: self.inbound_nodes[0].value\n",
    "        '''\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "            ## It's is input node, when need to forward, this node initiate self's value.\n",
    "\n",
    "        # Input subclass just holds a value, such as a data feature or a model parameter(weight/bias)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.gradients = {self:0}\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost * 1\n",
    "            \n",
    "        \n",
    "        # input N --> N1, N2\n",
    "        # \\partial L / \\partial N \n",
    "        # ==> \\partial L / \\partial N1 * \\ partial N1 / \\partial N\n",
    "\n",
    "\n",
    "class Add(Node):\n",
    "    def __init__(self, *nodes):\n",
    "        Node.__init__(self, nodes)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        self.value = sum(map(lambda n: n.value, self.inputs))\n",
    "        ## when execute forward, this node caculate value as defined.\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(self, [nodes, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        inputs = self.inputs[0].value\n",
    "        weights = self.inputs[1].value\n",
    "        bias = self.inputs[2].value\n",
    "\n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "        \n",
    "    def backward(self):\n",
    "\n",
    "        # initial a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            # Get the partial of the cost w.r.t this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "\n",
    "            self.gradients[self.inputs[0]] = np.dot(grad_cost, self.inputs[1].value.T)\n",
    "            self.gradients[self.inputs[1]] = np.dot(self.inputs[0].value.T, grad_cost)\n",
    "            self.gradients[self.inputs[2]] = np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "        # WX + B / W ==> X\n",
    "        # WX + B / X ==> W\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1 + np.exp(-1 * x))\n",
    "\n",
    "    def forward(self):\n",
    "        self.x = self.inputs[0].value\n",
    "        self.value = self._sigmoid(self.x)\n",
    "\n",
    "    def backward(self):\n",
    "        self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x))\n",
    "        \n",
    "        # y = 1 / (1 + e^-x)\n",
    "        # y' = 1 / (1 + e^-x) (1 - 1 / (1 + e^-x))\n",
    "        \n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]  # Get the partial of the cost with respect to this node.\n",
    "\n",
    "            self.gradients[self.inputs[0]] = grad_cost * self.partial\n",
    "            # use * to keep all the dimension same!.\n",
    "\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        y = self.inputs[0].value.reshape(-1, 1)\n",
    "        a = self.inputs[1].value.reshape(-1, 1)\n",
    "        assert(y.shape == a.shape)\n",
    "\n",
    "        self.m = self.inputs[0].value.shape[0]\n",
    "        self.diff = y - a\n",
    "\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        self.gradients[self.inputs[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inputs[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def forward_and_backward(outputnode, graph):\n",
    "    # execute all the forward method of sorted_nodes.\n",
    "\n",
    "    ## In practice, it's common to feed in mutiple data example in each forward pass rather than just 1. \n",
    "    ## Because the examples can be processed in parallel. The number of examples is called batch size.\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "        ## each node execute forward, get self.value based on the topological sort result.\n",
    "\n",
    "    for n in  graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "    #return outputnode.value\n",
    "\n",
    "###   v -->  a -->  C\n",
    "##    b --> C\n",
    "##    b --> v -- a --> C\n",
    "##    v --> v ---> a -- > C\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "            ## if n is Input Node, set n'value as \n",
    "            ## feed_dict[n]\n",
    "            ## else, n's value is caculate as its\n",
    "            ## inbounds\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    # there are so many other update / optimization methods\n",
    "    # such as Adam, Mom, \n",
    "    for t in trainables:\n",
    "        t.value += -1 * learning_rate * t.gradients[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 214.815\n",
      "Epoch: 101, Loss: 6.373\n",
      "Epoch: 201, Loss: 4.443\n",
      "Epoch: 301, Loss: 4.467\n",
      "Epoch: 401, Loss: 3.808\n",
      "Epoch: 501, Loss: 3.270\n",
      "Epoch: 601, Loss: 3.724\n",
      "Epoch: 701, Loss: 3.822\n",
      "Epoch: 801, Loss: 3.931\n",
      "Epoch: 901, Loss: 3.355\n",
      "Epoch: 1001, Loss: 3.643\n",
      "Epoch: 1101, Loss: 3.231\n",
      "Epoch: 1201, Loss: 3.881\n",
      "Epoch: 1301, Loss: 3.631\n",
      "Epoch: 1401, Loss: 3.121\n",
      "Epoch: 1501, Loss: 2.988\n",
      "Epoch: 1601, Loss: 3.197\n",
      "Epoch: 1701, Loss: 3.006\n",
      "Epoch: 1801, Loss: 3.620\n",
      "Epoch: 1901, Loss: 3.409\n",
      "Epoch: 2001, Loss: 3.188\n",
      "Epoch: 2101, Loss: 2.898\n",
      "Epoch: 2201, Loss: 3.451\n",
      "Epoch: 2301, Loss: 3.062\n",
      "Epoch: 2401, Loss: 2.999\n",
      "Epoch: 2501, Loss: 3.443\n",
      "Epoch: 2601, Loss: 3.516\n",
      "Epoch: 2701, Loss: 3.151\n",
      "Epoch: 2801, Loss: 3.490\n",
      "Epoch: 2901, Loss: 3.107\n",
      "Epoch: 3001, Loss: 3.427\n",
      "Epoch: 3101, Loss: 3.183\n",
      "Epoch: 3201, Loss: 3.537\n",
      "Epoch: 3301, Loss: 3.160\n",
      "Epoch: 3401, Loss: 3.774\n",
      "Epoch: 3501, Loss: 3.427\n",
      "Epoch: 3601, Loss: 3.505\n",
      "Epoch: 3701, Loss: 2.745\n",
      "Epoch: 3801, Loss: 3.060\n",
      "Epoch: 3901, Loss: 3.636\n",
      "Epoch: 4001, Loss: 3.518\n",
      "Epoch: 4101, Loss: 3.214\n",
      "Epoch: 4201, Loss: 2.917\n",
      "Epoch: 4301, Loss: 3.134\n",
      "Epoch: 4401, Loss: 3.301\n",
      "Epoch: 4501, Loss: 3.073\n",
      "Epoch: 4601, Loss: 3.172\n",
      "Epoch: 4701, Loss: 3.399\n",
      "Epoch: 4801, Loss: 2.818\n",
      "Epoch: 4901, Loss: 3.131\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "losses = []\n",
    "\n",
    "\"\"\"\n",
    "Check out the new network architecture and dataset!\n",
    "Notice that the weights and biases are\n",
    "generated randomly.\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size, etc!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 5000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 16\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        _ = None\n",
    "        forward_and_backward(_, graph) # set output node not important.\n",
    "\n",
    "        # Step 3\n",
    "        rate = 1e-2\n",
    "    \n",
    "        sgd_update(trainables, rate)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a24ea79b0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Optimizer Adam\n",
    "## 8.1 SGD + Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.loli.net/2020/04/16/4lY9i6JHszkQ3tv.png\" alt=\"图片替换文本\" width=\"800\" height=\"800\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.loli.net/2020/04/16/WfuNozZEbkxvJh9.png\" width=\"800\" height=\"800\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](RMSProp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Adam.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试机器学习问题比较好的默认参数设定为：\n",
    "\n",
    "alpha=0.001、beta1=0.9、beta2=0.999 和 epsilon=10E−8。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用框架默认参数设定\n",
    "TensorFlow：learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08.\n",
    "\n",
    "Keras：lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0.\n",
    "\n",
    "Blocks：learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-08, decay_factor=1.\n",
    "\n",
    "Lasagne：learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08\n",
    "\n",
    "Caffe：learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08\n",
    "\n",
    "MxNet：learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8\n",
    "\n",
    "Torch：learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3 (default, Mar 27 2019, 16:54:48) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autograd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2604ca96d256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autograd'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, loss, weights, lr=0.001, beta1=0.9, beta2=0.999, epislon=1e-8):\n",
    "        self.loss = loss\n",
    "        self.theta = weights\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epislon = epislon\n",
    "        self.get_gradient = grad(loss)\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        self.t = 0\n",
    "    def minimize_raw(self):\n",
    "        self.t += 1\n",
    "        g = self.get_gradient(self.theta)\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * g\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (g * g)\n",
    "        self.m_cat = self.m / (1 - self.beta1 ** self.t)\n",
    "        self.v_cat = self.v / (1 - self.beta2 ** self.t)\n",
    "        self.theta -= self.lr * self.m_cat / (self.v_cat ** 0.5 + self.epislon)\n",
    "\n",
    "    def minimize(self):\n",
    "        self.t += 1\n",
    "        g = self.get_gradient(self.theta)\n",
    "        lr = self.lr * (1 - self.beta2 ** self.t) ** 0.5 / (1 - self.beta1 ** self.t)\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * g\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (g * g)\n",
    "        self.theta -= lr * self.m / (self.v ** 0.5 + self.epislon)\n",
    "        \n",
    "        \n",
    "    def minimize_show(self, epochs=5000):\n",
    "        for _ in range(epochs):\n",
    "            self.t += 1\n",
    "            g = self.get_gradient(self.theta)\n",
    "            lr = self.lr * (1 - self.beta2 ** self.t) ** 0.5 / (1 - self.beta1 ** self.t)\n",
    "            self.m = self.beta1 * self.m + (1 - self.beta1) * g\n",
    "            self.v = self.beta2 * self.v + (1 - self.beta2) * (g * g)\n",
    "            self.theta -= lr * self.m / (self.v ** 0.5 + self.epislon)\n",
    "            print(\"step{: 4d} g:{} lr:{} m:{} v:{} theta:{}\".format(self.t, g, lr, self.m, self.v, self.theta))\n",
    "            \n",
    "final_loss = self.loss(self.theta)\n",
    "print(\"final loss:{} weights:{}\".format(final_loss, self.theta))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5*(np.tanh(x) + 1)\n",
    "\n",
    "def logistic_predictions(weights, inputs):\n",
    "    # Outputs probability of a label being true according to logistic model.\n",
    "    return sigmoid(np.dot(inputs, weights))\n",
    "\n",
    "def training_loss(weights):\n",
    "    # Training loss is the negative log-likelihood of the training labels.\n",
    "    preds = logistic_predictions(weights, inputs)\n",
    "    label_probabilities = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -np.sum(np.log(label_probabilities))\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = np.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "targets = np.array([True, True, False, True])\n",
    "weights = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "def sgd(epochs=1000):\n",
    "    training_gradient_fun = grad(training_loss)\n",
    "    \n",
    "    # Optimize weights using gradient descent.\n",
    "    weights = np.array([0.0, 0.0, 0.0])\n",
    "    print(\"Initial loss:{}\".format(training_loss(weights)))\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        weights -= training_gradient_fun(weights) * 0.01\n",
    "\n",
    "    print(\"Trained loss:{}\".format(training_loss(weights)))\n",
    "    print(\"weights:{}\".format(weights))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    adam = Adam(training_loss, weights, lr=0.01)\n",
    "    print(\"start to optimize:\")\n",
    "    # adam.minimize_show(epochs=EPOCHS)\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        adam.minimize_raw()\n",
    "    print(\"weights:{} loss:{}\".format(adam.theta, adam.loss(adam.theta)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 深度学习框架\n",
    "## 9.1 Tensorflow\n",
    "请学生自行安装tensorflow 2.0版本，没有安装的同学，可以在cell中运行【! pip install tensorflow==2.0.0b1】。  \n",
    "同学，你可以自行阅读[tensorflow 2.0 中文文档](http://www.tensorfly.cn/tfdoc/get_started/basic_usage.html)    \n",
    "**更建议同学直接阅读**[英文文档](https://tensorflow.google.cn/tutorials/quickstart/beginner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.0.0b1\n",
      "  Using cached https://files.pythonhosted.org/packages/d3/c4/1f93387c61e75b3977a7ad533762f40d9b4741992bfa2ec8f13ecda0456f/tensorflow-2.0.0b1-cp37-cp37m-macosx_10_11_x86_64.whl\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0b1) (1.17.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0b1) (0.33.4)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0b1) (1.11.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0b1) (1.0.8)\n",
      "Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603 (from tensorflow==2.0.0b1)\n",
      "  Using cached https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl\n",
      "Collecting gast>=0.2.0 (from tensorflow==2.0.0b1)\n",
      "  Using cached https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
      "Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 (from tensorflow==2.0.0b1)\n",
      "  Using cached https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.0.0b1)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow==2.0.0b1)\n",
      "  Using cached https://files.pythonhosted.org/packages/78/05/c7450cec52bb1e3d7c56efd384c0ee647cd2b44946004035b3abe3493407/grpcio-1.28.1-cp37-cp37m-macosx_10_9_x86_64.whl\n",
      "Collecting absl-py>=0.7.0 (from tensorflow==2.0.0b1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0b1) (1.1.0)\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow==2.0.0b1)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow==2.0.0b1)\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.6.1 (from tensorflow==2.0.0b1)\n",
      "  Using cached https://files.pythonhosted.org/packages/4c/25/c057a298635d08d087a20f51ff4287d821814208ebb045d84ea65535b3e3/protobuf-3.11.3-cp37-cp37m-macosx_10_9_x86_64.whl\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.0.0b1) (1.12.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==2.0.0b1) (2.10.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0b1) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0b1) (0.15.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0b1) (41.0.1)\n",
      "Installing collected packages: protobuf, grpcio, absl-py, tb-nightly, gast, tf-estimator-nightly, termcolor, google-pasta, astor, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astor-0.8.1 gast-0.3.3 google-pasta-0.2.0 grpcio-1.28.1 protobuf-3.11.3 tb-nightly-1.14.0a20190603 tensorflow-2.0.0b1 termcolor-1.1.0 tf-estimator-nightly-1.14.0.dev2019060501\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tensorflow==2.0.0b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/site-packages (from keras) (5.1.2)\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Collecting h5py (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/8b/4d01ae9a9d50a0bcc7b0b9aae41785d8d9de6fa9bba04dc20b1582181d2d/h5py-2.10.0-cp37-cp37m-macosx_10_6_intel.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 184kB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/site-packages (from keras) (1.17.1)\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Collecting scipy>=0.14 (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/7a/ae480be23b768910a9327c33517ced4623ba88dc035f9ce0206657c353a9/scipy-1.4.1-cp37-cp37m-macosx_10_6_intel.whl (28.4MB)\n",
      "\u001b[K     |████████████████████████████████| 28.4MB 7.4MB/s \n",
      "\u001b[?25hInstalling collected packages: keras-preprocessing, h5py, keras-applications, scipy, keras\n",
      "Successfully installed h5py-2.10.0 keras-2.3.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 scipy-1.4.1\n",
      "Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1076)'))) - skipping\n"
     ]
    }
   ],
   "source": [
    "! pip3 install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-de672dc32709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, activation='sigmoid', input_dim=13))\n",
    "model.add(Dense(units=30, activation='sigmoid', input_dim=64))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-161d380db306>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(X_, y_, epochs=1000, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
